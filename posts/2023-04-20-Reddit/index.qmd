---
title: "Unraveling Subreddits"
description: "Identifying underlying themes in Reddit titles using non-supervised clustering "
date: 2022-04-20
categories: [python, EDA, SQL, analysis]
image: thumbnail.jpeg
# include-in-header: animation.html
format:
  html:
    code-fold: true 
    # css: custom-styles.scss
jupyter: python3

editor: 
  render-on-save: true

draft: true # setting this to `true` will prevent your post from appearing on your listing page until you're ready!

---

<!-- ![](thumbnail.jpeg) -->

<h1>Executive Summary</h1>
<div class='executive-summary'>

Original creation and submission for this notebook was last June 2019.

[Reddit](https://www.reddit.com/) is an American discussion and aggregation website for user-generated content with more than 500 million monthly visitors. We uncovered the underlying categories or "subreddits" by performing representative-based clustering on a sample of Reddit titles. Results reveal that Reddit is a very American-centric platform and that the two main themes are (a) U.S. politics, and (b) seeking help from other Redditors on a wide range of topics including technology, food, and legal matters. For a small number of clusters ($n=10$), the high-level categories that emerged include the 2016 U.S. Presidential Elections (Donald Trump, Hillary Clinton, and Bernie Sanders), food, legal concerns, and "New year"-related subreddits. Increasing the number of clusters to $n=16$ revealed more specific subreddits such as technical support, recipes, apartment/rental concerns, and reaction compilations.
</div>

<h2>Acknowledgements</h2>

This analysis was done together with my Lab partner, George Esleta, and Cohortmates - Gilbert Chua, Nigel Silva and Oonre Advincula-Go. 

<h2> A. Introduction and the Problem Statement </h2>

[Reddit](https://www.reddit.com/) is a discussion and content aggregation website. As of June 2019, it is ranked as the #5 most visited website in the United States and #15 worldwide. Registered members, also called "Redditors", upload user-generated content (UGC) which are either voted up or voted down by other Redditors. Reddit posts are grouped into user-created boards or communities called "subreddits", with each subreddit having a specific topic such as food, entertainment, and politics.

<h2> B. Methodology</h2>

Reddit titles and authors were scraped from the [Reddit](https://www.reddit.com/) website and stored in a text file. The file is tab-delimited and has two columns: author and title.

The general workflow for clustering the Reddit titles involves the following steps:
<ol>
<li> Pre-requisites: Load Requirement Package
<li> Reading the Reddit data
<li> Initial data cleaning
<li> Exploratory data analysis
<li> Vectorization (bag-of-words representation) using TFIDF Vectorizer
<li> Dimensionality reduction using Latent Semantic Analysis (LSA)
<li> Representative-based clustering using *k*-means
</ol>

<h3>Pre-requisites: Load Requirement Package</h3>

Before anything else, let us first load all important modules for this exercise.
```{python}
# | eval: false
# | code-summary: Loading required modules

# These are standard imports
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from collections import Counter
from IPython.display import display_html
from wordcloud import WordCloud, STOPWORDS

from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import stop_words
from sklearn.manifold import TSNE
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import calinski_harabaz_score, silhouette_score
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from nltk import FreqDist, RegexpTokenizer

from IPython.display import HTML
from scipy.spatial.distance import euclidean
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import euclidean, cityblock

import urllib
import requests
import pprint
from PIL import Image

pp = pprint.PrettyPrinter(indent=4)

HTML('''<script>
code_show=true; 
function code_toggle() {
 if (code_show){
 $('div.input').hide();
 } else {
 $('div.input').show();
 }
 code_show = !code_show
} 
$( document ).ready(code_toggle);
</script>
<form action="javascript:code_toggle()"><input type="submit" value="Click here to toggle on/off the raw code."></form>''')

Additionally, we should create a sqlite3 database for where we will store the data what we will scrape. For that, we shall use the `sqlite3` python module.

```{python}
# | eval: false
# | code-summary: Import the SQLite3 moule then create an empty database called chessgames.
import sqlite3
conn = sqlite3.connect('chessgames.db')
```

Next, since we will be doing some web scraping, which may want to set our proxy and headers. A proxy server can help a scraper avoid IP address blocking, access geographically restricted content, facilitate high-volume scraping, and avoid detection. Headers in web scraping are a part of the HTTP request that provides information about the client making the request. They are important because they can affect the response received from the server. Some websites may block or restrict access to content based on the header information. To avoid being detected as a bot or being blocked by the server, it is important to ensure that the headers used in web scraping are appropriate and mimic those of a real user.

```{python}
# | eval: false
# | code-summary: Edit our Proxy and Heading
# Setting of proxy
os.environ['HTTP_PROXY'] = 'http://13.115.147.132:8080'
os.environ['HTTPS_PROXY'] = 'http://13.115.147.132:8080'

# Setting of header
header = {'''accept: text/html,application/xhtml+xml,application/xml;q=0.9,
            image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3
            accept-Encoding: gzip, deflate
            accept-Language: en-US,en;q=0.9
            cache-Control: max-age=0
            connection: keep-alive
            host: www.chessgames.com
            referer: http://www.chessgames.com/perl/chess.pl?page=16&pid=14125
            &playercomp=either&year=2010&yearcomp=ge
            upgrade-Insecure-Requests: 1
            user-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 
            (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'''}
```

<h3>Step 1: Understand the Data and Conduct Web Scraping Tool</h3>

To analyze the top opening moves of the highest-rated chess players, web data extraction was performed on the pages of [ChessGames.com](ChessGames.com). We focused on two entities, namely the chess players (`players`) and their games (`games`). Data for both entities were saved to an SQLlite database (`chessgames.db`). 

The `players` table was obtained by scraping all web pages under the Chess Players Directory [http://www.chessgames.com/directory/](http://www.chessgames.com/directory/). Each page in this directory corresponds to a letter of the alphabet (e.g.,  [http://www.chessgames.com/directory/A](http://www.chessgames.com/directory/A). Each player has the following information:

<center> Table 1: `player` table fields </center>
| Field       | Description                   | Long Description                        | Data Type |
|-------------|-------------------------------|-----------------------------------------|-----------|
| pid         | Player ID                     | indentification number                  | integer   |
| lname       | Last Name                     | player's last name                      | varchar   |
| fname       | First Name                    | player's first name                     | varchar   |
| rating      | Rating                        | highest rating achieved in the database | integer   |
| start_year  | Start Year                    | player's starting year                  | integer   |
| end_year    | End Year                      | player's ending year                    | integer   |
| game_count  | Number of games               | number of games in database             | integer   |

The `get_player_info` method was used to scrape the player information (seen below).

```{python}
# | eval: false
# | code-summary: Function description for get_player_info()
def get_player_info(url):
    '''
    Scrapes player info from the specified URL.

    Parameter
    ---------
    URL : URL of player page

    Return
    ------
    list of tuples (pid, lname, fname, rating, start_year, end_year, 
    game_count)
    '''
    players_list = []
    resp = requests.get(url, headers=headers)
    time.sleep(1)
    print("\tStatus code: ", resp.status_code)
    resp_soup = BeautifulSoup(resp.text, 'lxml')
    players = resp_soup.select('tr[bgcolor="#FFFFFF"],tr[bgcolor="#FFEEDD"]')
    for player in players:
        data = player.select('td')
        rating = data[0].text.strip()
        name = data[2].text.split(',')
        if len(name) == 2:
            fname = name[1].strip()
        else:
            fname = None
        lname = name[0].strip()
        years = data[3].text.strip()
        game_count = data[4].text.strip()
        start_year = re.match('(\d{4})?-?(\d{4})', years).group(1)
        end_year = re.match('(\d{4})?-?(\d{4})', years).group(2)

        url = str(player.select('a')[-1])
        pid = re.match('.*?pid=(\d+)', url).group(1)

        tup = (pid, lname, fname, rating, start_year, end_year, game_count)
        print('\t', tup)
        players_list.append(tup)
    return players_list
```
For the `get_player_info` function, the `Requests` module is a Python library used for making HTTP requests. We can use basic methods such as the `GET`, `POST`, `PUT`, `DELETE`, and others. The module also provides support for handling cookies, adding custom headers, and handling redirects. We also used the `BeautifulSoup` Python library, which is a typical package in parsing HTML and XML documents. The package parses the pased HTML source code into a parsed tree, which can be easily traversed. Finally, we used the `re` Python package to utilize `Regular Expression` for easier string matching.

<br>
Next, the players data are then inserted to the `players` table using the `insert_players` method:

```{python}
# | eval: false
# | code-summary: Function description for insert_players()
def insert_players(conn):
    '''
    Inserts players into the players table

    Parameter
    ---------
    conn : sqlite connection
    '''
    cur = conn.cursor()
    for char in string.ascii_uppercase:
        url = "http://www.chessgames.com/directory/" + char + ".html"
        print(url)
        players = get_player_info(url)
        cur.executemany('''INSERT INTO players 
                            VALUES (?, ?, ?, ?, ?, ?, ?);''', players)
        conn.commit()
```

Here, we access the enter the created database, then Insert the players and associated metadata into the database.

<br>
Next, this study will focus on the games of the thirty (30) highest-rated players. The ranking was based on the rating provided by the website. To extract the games of these players, we first obtained their player IDs (`pid`) by using the `pandas` method `read_sql`. Here we can pass a `SQL` statement:

```{python}
# | eval: false
# | code-summary: Method to query the database
df_players = pd.read_sql("""SELECT pid, fname || ' ' || lname, rating, 
                                    game_count 
                            FROM players
                            WHERE rating != ''
                            ORDER BY rating DESC""", conn)
df_players.columns = ['Player ID', 'Name', 'Rating', 'Number of Games']
df_players.head(30)
```

|    Player ID |                     Name | Rating | Number of Games |
|-------------:|----------------|-------:|----------------:|
|        52948 |           Magnus CARLSEN |   2882 |           3,016 |
|        15940 |           Garry KASPAROV |   2851 |           2,385 |
|        76172 |         Fabiano CARUANA |   2844 |           1,891 |
|        17316 |           Levon ARONIAN |   2830 |           2,708 |
|        95915 |                 Wesley SO |   2822 |           1,400 |
|        56798 | Maxime VACHIER-LAGRAVE |   2819 |           2,369 |
|        12088 |         Viswanathan ANAND |   2817 |           3,542 |
|        12295 |        Vladimir KRAMNIK |   2817 |           3,026 |
|        12089 |           Veselin TOPALOV |   2816 |           2,278 |
|        50065 |   Shakhriyar MAMEDYAROV |   2814 |           2,254 |
|        10084 |          Hikaru NAKAMURA |   2814 |           2,424 |
|        17279 |      Alexander GRISCHUK |   2797 |           2,586 |
|        52629 |               Ding LIREN |   2797 |             920 |
|       107252 |              Anish GIRI |   2793 |           1,522 |
|        49796 |         Teimour RADJABOV |   2793 |           1,746 |
|        54535 |         Sergey KARJAKIN |   2788 |           2,399 |
|        11719 |  Alexander MOROZEVICH |   2788 |           1,847 |
|        12183 |         Vassily IVANCHUK |   2787 |           3,752 |
|        19233 |      Robert James FISCHER |   2785 |           1,052 |
|        20719 |           Anatoly KARPOV |   2780 |           3,609 |
|        13847 |            Boris GELFAND |   2777 |           3,014 |
|        79968 |            Peter SVIDLER |   2769 |           2,786 |
|        49080 |  Leinier Dominguez PEREZ |   2768 |           1,342 |
|        12109 |         Ruslan PONOMARIOV |   2768 |           1,989 |
|        54683 |      Ian NEPOMNIACHTCHI |   2767 |           1,614 |
|        49246 |     Pentala HARIKRISHNA |   2766 |           1,442 |
|        49456 |            Pavel ELJANOV |   2765 |           1,409 |
|        15874 |               Gata KAMSKY |   2763 |           1,889 |
|        12290 |                Peter LEKO |   2763 |           2,364 |
|       112240 |                Yu YANGYI |   2762 |             991 |
: Table 2: Top 30 chess players based on Rating

The `games` table was then populated by web scraping <small>`http://www.chessgames.com/perl/chessplayer?pid=<pid>`</small> and iterating over the top 30 player IDs. The following fields were extracted for each game:


| Field         | Description        | Data Type |
|---------------|--------------------|-----------|
| gid           | Game ID            | integer   |
| white_pid     | White Player ID    | int       |
| black_pid     | Black Player ID    | int       |
| result        | Result             | varchar   |
| moves         | Number of moves    | integer   |
| year          | Year               | integer   |
| tournament    | Tournament Name    | varchar   |
| eco           | Encyclopaedia of Chess Openings  | varchar   |
| opening_move  | Opening move       | varchar   |
: Table 3: `games` table fields 

The `get_players_games` function was implemented to scrape the game data for a given Player ID `pid` and page number `page_start`. This writes the games data of the player to a CSV file (`<pid>.csv`):

```{python}
# | eval: false
# | code-summary: Function description for the get_players_games()
def get_player_games(pid, page_start):
    """
    Web scrapes the games list for a player and writes it to a CSV 

    Parameters:
    -----------
    pid : player ID
    page_start : starting page

    Returns:
    --------
    None
    """
    url = 'http://www.chessgames.com/perl/chessplayer?pid=' + str(pid)
    resp = requests.get(url, headers=headers)
    print('pid = ', pid, '\turl = ', url, '\tcode = ', resp.status_code)
    time.sleep(np.random.randint(1, 2))
    soup = BeautifulSoup(resp.text, 'lxml')
    div_page_count = soup.select(
        'td[background$="/chessimages/table_stripes.gif"]')
    page_count = int(re.findall('of (\d+)\;', div_page_count[0].text)[0])

    with open(f'{pid}.csv', 'a') as file:
        csv_writer = csv.writer(file, delimiter=',', quotechar='"')
        for page in range(page_start, page_count+1):
            page_url = 'http://www.chessgames.com/perl/chess.pl?page=' + \
                str(page) + '&pid=' + str(pid)
            page_resp = requests.get(page_url, headers=headers)
            print('\tpage = ', page, '\turl = ', page_url,
                  '\tcode = ', page_resp.status_code)
            time.sleep(np.random.randint(1, 2))
            page_soup = BeautifulSoup(page_resp.text, 'lxml')
            games = page_soup.select(
                'tr[bgcolor="#FFFFFF"],tr[bgcolor="#EEDDCC"]')

            for game in games:
                data = game.select('td')
                game_url = data[0].find("a")['href']
                game_id = re.findall('(\d+)', game_url)[0]
                result = data[2].text.strip()
                moves = data[3].text.strip()
                year = data[4].text.strip()
                tournament = data[5].text.strip()
                eco = data[6].select('a')[0].text.strip()
                opening_move = re.findall(
                    '^[A-E0-9][0-9]{2} (.*)', data[6].text.strip())[0]

                game_resp = requests.get(
                    'http://www.chessgames.com' + game_url, headers=headers)
                time.sleep(np.random.randint(1, 2))
                game_soup = BeautifulSoup(game_resp.text, 'lxml')
                players = game_soup.select('center')[0].select('a')
                try:
                    white_id = re.findall('(\d+)', players[0]['href'])[0]
                except:
                    white_id = None
                try:
                    black_id = re.findall('(\d+)', players[1]['href'])[0]
                except:
                    black_id = None
                tup = (game_id, white_id, black_id, result,
                       moves, year, tournament, eco, opening_move)

                try:
                    csv_writer.writerow(tup)
                except:
                    print('\t\tgameID: ', game_id, '\tWrite to CSV failed')
```

All player csv files were then inserted to the `games` table of the `chessgames.db` database using the `read_csvs` function:

```{python}
# | eval: false
# | code-summary: Function description for read_csvs()
def read_csvs(conn):
    """
    Read all player csvs and save them to the games table
    """
    gid_failed = []
    cur = conn.cursor()
    for file_name in glob.glob('./games/*.csv'):
        with open(file_name) as file:
            print(file_name)
            reader = csv.reader(file, delimiter = ',')
            for line in reader:
                try:
                    cur.execute('''INSERT INTO games VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);''', line)
                    print('\t\tgameID: ', line[0], '\tInsert success!')
                except Exception as e:
                    gid_failed.append(line[0])
                    print('\t\tgameID: ', line[0], '\tInsert Failed!\t', e)
    return gid_failed
```
Now, let us examine the games dataframe by calling it with the `read_sql` Pandas module.

```{python}
# | eval: false
# | code-summary: Accessing the database.
df_games = pd.read_sql("""SELECT * FROM games""", conn)
df_games.columns = (['Game ID', 'White Player ID', 'Black Player ID',
                     'Result', 'Number of Moves', 'Year', 'Tournament', 'ECO',
                     'Opening Move'])
```

Below you can see that **Table 4** and **Table 5** shows the summary statistics for the `players` and `games` table. **8,574 players** and **50,087** games were inserted into the chessgames database.


|Statistic | Rating |
|---------|-----------|
|count	|8574.000000|
|mean	|2361.279566|
|std	|184.344627|
|min	|1379.000000|
|25%	|2268.000000|
|50%	|2389.000000|
|75%	|2479.000000|
|max	|2882.000000|

: Table 4: `players` table summary statistics

| Statistic |	Number of Moves |	Year |	
|---------|-----------|---------|
| count	| 50087.000000	| 	50087.000000 |
| mean	| 	42.893485	| 	2005.074950 |
| std	| 17.288031	| 	11.643902 |
| min	| 0.000000	| 	1953.000000 |
| 25%	| 31.000000	| 	2000.000000 |
| 50%	| 41.000000	| 	2008.000000 |
| 75%	| 	52.000000	| 	2014.000000 |
| max	| 	255.000000	| 	2019.000000 |

: Table 5: `games` table summary statistics

<h2>Results</h2>

**Figure 1** below shows the top 10 opening moves used by the thirty highest-ranked chess players. 356 games, or nearly 2.6% of the games won by the top players, were opened using the Sicilian, Najdorf (ECO = B90) move. 

However, looking at the top opening moves per player as seen in **Figure 3**, the Sicilian, Najdorf (ECO = B90) move does not appear as the top move for most of the players. For instance, Magnus Carlsen, the highest-ranked player, has the Ruy Lopez, Berlin Defense as his top opening move. As shown in **Table 7**, Only 7 of the top 30 players have the Sicilian, Najdorf move as their top winning opening move. Also, if we look at the top 5 highest-rated players, 3 of 5 of them have the Ruy Lopez, Berlin Defense move as the top winning move.

```{python}
# | eval: false
# | code-summary: Function to see top 10 opening moves and % won.
df_top_moves = pd.read_sql("""SELECT g.white_pid, p.fname || ' ' || 
                            p.lname, p.rating, g.eco, g.opening_move, count(*) 
                            FROM games g
                          INNER JOIN players p on g.white_pid = p.pid
                          WHERE g.white_pid IN (SELECT pid FROM players 
                                              WHERE rating != ''
                                              ORDER BY rating DESC limit 30)
                          AND g.result = '1-0'
                          GROUP BY g.white_pid, g.eco, g.opening_move
                          ORDER BY p.rating DESC, count(*) DESC""", conn)
df_top_moves.columns = ['PID', 'Name', 'Rating',
                        'ECO', 'Top Opening Move', 'Number of Games Won']
df_top_moves = df_top_moves.groupby(['ECO', 'Top Opening Move'])[
    'Number of Games Won'].sum().to_frame()
df_top_moves = df_top_moves.sort_values('Number of Games Won', ascending=False)
df_top_moves.reset_index(inplace=True)
df_top_moves['% of Games Won'] = (
    df_top_moves['Number of Games Won'] / df_top_moves['Number of Games Won'].sum() * 100)
df_top_moves.head(10)
```


|   ECO | Top Opening Move           |   Number of Games Won |   % of Games Won |
|-------|----------------------------|----------------------|------------------|
| B90   | Sicilian, Najdorf          |                  356 |         2.566321 |
| A07   | King's Indian Attack       |                  250 |         1.802191 |
| B12   | Caro-Kann Defense          |                  227 |         1.63639  |
| C78   | Ruy Lopez                   |                  206 |         1.485006 |
| C42   | Petrov Defense             |                  205 |         1.477797 |
| C11   | French                     |                  204 |         1.470588 |
| E15   | Queen's Indian             |                  203 |         1.463379 |
| A04   | Reti Opening               |                  192 |         1.384083 |
| D37   | Queen's Gambit Declined    |                  186 |         1.34083  |
| C65   | Ruy Lopez, Berlin Defense  |                  176 |         1.268743 |

: Table 6: Top Opening Moves of the 30 highest-rated players

```{python}
# | eval: false
# | code-summary: Code to generate the Top Opening Moves.
import matplotlib.pyplot as plt
fig, ax = plt.subplots(dpi=150)
df_top_moves[10::-1].plot.barh('Top Opening Move',
                               '% of Games Won', ax=ax, color='#BF5209',
                               legend=False)
ax.set_xlabel('% of Games Won')
```
<!-- ![FIGURE 1:Top Opening Moves of the 30 highest-rated chess players](image-outputs/image1-topmoves.png) -->

Now, let us view the top opening moves per top player (top 10 players)

```{python}
# | eval: false
# | code-summary: Code to generate the Top Opening Moves for top players
df_top_moves = pd.read_sql("""SELECT g.white_pid, p.fname || ' ' || 
                            p.lname, p.rating, g.eco, g.opening_move, count(*) 
                            FROM games g
                          INNER JOIN players p on g.white_pid = p.pid
                          WHERE g.white_pid IN (SELECT pid FROM players 
                                              WHERE rating != ''
                                              ORDER BY rating DESC limit 30)
                          AND g.result = '1-0'
                          GROUP BY g.white_pid, g.eco
                          ORDER BY p.rating DESC, count(*) DESC""", conn)
df_top_moves.columns = ['PID', 'Name', 'Rating', 'ECO', 'Top Opening Move', 'Number of Games Won']
df_top_moves = (df_top_moves
                .groupby(['Rating', 'PID', 'Name', 'ECO', 'Top Opening Move'])
                ['Number of Games Won'].sum().to_frame())
df_top_moves = (df_top_moves.reset_index()
                .sort_values(['Rating', 'Number of Games Won'],
                             ascending=[False, False])
                .set_index(['PID', 'Rating', 'Name', 'ECO', 'Top Opening Move']))
df_top_moves = df_top_moves.groupby(level=0).head(10)
df_top_moves.reset_index(inplace=True)
df_top_moves

names = df_top_moves['Name'].unique()

for name in names:
    fig, ax = plt.subplots()
    df = df_top_moves[df_top_moves['Name'] == name]
    df[10::-1].plot.barh('Top Opening Move', 'Number of Games Won',
                         ax=ax, color='#BF5209', legend=False)
    ax.set_title(name)
    ax.set_xlabel('Number of Games Won')
```

:::  {#fig-top layout-ncol=1}

Top Winning Moves Per Top Player
:::

<h2>Conclusions and Recommendations</h2>

The clustering results show that **Reddit is a very American-centric platform**. Majority of the topics are related to the 2016 U.S. Presidential Election, specifically on the Republican nominee Donald Trump, Democrat candidates Hillary Clinton, and the Iowa caucuses. 

Also, the results show that the two primary motivations of users to post in Reddit is (a) to discuss politics, and (b) to seek advise/ask help from other Redditors, whether technological, legal, or culinary in nature.

Increasing the number of clusters also reveal more specific topics such as 'technical support' and ''Vines reaction compilation videos'.

For future studies, it is suggested to perform hierarchical clustering.

<h2>References</h2>
<i>
<ul>
<li> [All the news](https://towardsdatascience.com/all-the-news-17fa34b52b9d)
<li> [Clustering text documents using k-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)
<li> [Latent Semantic Analysis](http://www.scholarpedia.org/article/Latent_semantic_analysis)
</ul>
</i>