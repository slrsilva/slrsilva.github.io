[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hello, World!\n\n\nThis website will be where I share my journey of mastering the art and science of data science and analytics, showcasing my past projects, and writing about the topics that I find truly fascinating. So, please stay tuned and let’s explore this exciting world together!\n\n\nAbout Me\n\n\nWith over 8 years of experience in the energy and financial industries as an Analytics Officer, I have a proven track record of developing portfolio optimization tools, conducting financial and risk analysis, and providing data-driven recommendations. Through my proficiency in Python and other data analytics tools, I have streamlined data collection, mining, and wrangling, developed data-informed suggestions and reports, and successfully communicated my insights to a range of stakeholders.\nMy career has been focused on solving complex multidisciplinary problems, and I have developed a strong interest in data science as a tool to solve real-world business and policy problems. I am actively pursuing opportunities to utilize and enhance my data science and analytical skills, with a focus on improving business analytics and data science practices.\n\n\nEducation\n M.Sc., Data Science \n\nAsian Insitute of Management – Manila, Philippines\n\n\n2019 - 2020\n\n B.A., Double Major in Economics and Mathematics \n\nCollege of the Holy Cross – Massachusetts, USA\n\n\n2009 - 2013\n\n\n\nExperience\n Analytis and Optimization Manager \n\nAboitiz Power Corporation\n\n Data Analytics and Digital Solutions Manager \n\nTeaM Energy Corporation\n\n Financial Risk Consultant \n\nAIA Philippines\n\n Innovations Programs Manager \n\nAboitiz Power Corporation\n\n Margin Optimization Specialist \n\nAboitiz Power Corporation\n\n Market Analyst\n\nAboitiz Power Corporation"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Music Through The Years\n\n\n\npython\n\n\nbig-data\n\n\n\nA Big Data Explotatory Data Anslysis on the Music Industry\n\n\n\n\n\n\nApr 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere Comes The Sun\n\n\n\npython\n\n\nreinforced-learning\n\n\ntime-series\n\n\nlstm\n\n\n\nPredicting Solar Power Supply Generation in the Philippines using Deep Learning\n\n\n\n\n\n\nApr 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnwRappler 2\n\n\n\npython\n\n\nsupervised-learning\n\n\nsentiment-analysis\n\n\nnlp\n\n\n\nPredicting mood and polarity of Rappler articles using machine learning\n\n\n\n\n\n\nApr 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnwRappler\n\n\n\npython\n\n\nunsupervised-learning\n\n\nclustering\n\n\n\nRevealing Rappler’s underlying focus\n\n\n\n\n\n\nApr 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnraveling Subreddits\n\n\n\npython\n\n\nclustering\n\n\nunsupervised-learning\n\n\n\nIdentifying underlying themes in Reddit titles using non-supervised clustering\n\n\n\n\n\n\nApr 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn analysis of the opening moves of the highest-ranked chess players\n\n\n\npython\n\n\nEDA\n\n\nSQL\n\n\n\nThe Sicilian Defence is the best offense\n\n\n\n\n\n\nApr 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegional Voting Preferences in the 2019 Philippine Senatorial Elections\n\n\n\npython\n\n\ngeospatial\n\n\n\nLooks like voting in the Philippines is all about location, location, location! Literacy and religion may not be factors, but hometown pride definitely is.\n\n\n\n\n\n\nApr 17, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": "",
    "text": "This notebook was created for our Data Mining and Wrangling class in AIM MSDS. In particular, this was done during our 2nd semester of class, as one of the required lab reports. In this notebook, we explored the 2019 elections to see if there are any socio-economic relationships with the results. We analyzed the election results per region and wanted to answer the following questions: (1) Is Demographics Related To Voting Preference?, (2) Is literacy rate related to voting preference?, and (3) Is religion related to voting preference?\n\n\n\n\n\n\n\n\n\n&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML\"&gt;&lt;/script&gt;\n&lt;!-- MathJax configuration --&gt;\n&lt;script type=\"text/x-mathjax-config\"&gt;\nMathJax.Hub.Config({\n    tex2jax: {\n        inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n        displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n        processEscapes: true,\n        processEnvironments: true\n    },\n    // Center justify equations in code and markdown cells. Elsewhere\n    // we use CSS to left justify single line equations in code cells.\n    displayAlign: 'center',\n    \"HTML-CSS\": {\n        styles: {'.MathJax_Display': {\"margin\": 0}},\n        linebreaks: { automatic: true }\n    }\n});\n&lt;/script&gt;\n&lt;!-- End of mathjax configuration --&gt;&lt;/head&gt;\n\n\n&lt;div class=\"container\" id=\"notebook-container\"&gt;\n\n\n\n\n\n\n\n Regional Voting Preferences in the 2019 Philippine Senatorial Elections\n\n\n\n\n\n\n\n\n\n\n\nThe Philippines recently concluded its 2019 midterm elections last May 13. Sixty-two (62) candidates from various political parties contested for twelve (12) seats in the Senate of The Philippines. Given the unexpected results, the team was wondering on the following topics:\n\n\n\nHow did the various administrative regions of the Philippines voted for their senators?\n\n\nIs the voter preference homogeneous across the country, or is there a preferred candidate or party per region? More specifically, how does (1) religious affiliation, (2) educational attainment, and (2) sex play a major role on how the voters select their candidates.\n\n\n\n\n\n\n\n\nIn [1]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfrom IPython.display import HTML\n\nHTML('''&lt;script&gt;\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n\n  $( document ).ready(function(){\n    code_shown=false;\n    $('div.input').hide()\n  });\n&lt;/script&gt;\n&lt;form action=\"javascript:code_toggle()\"&gt;&lt;input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"&gt;&lt;/form&gt;''')\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[1]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore anything else, let us first load all important modules for this exercise.\n\n\n\n\n\n\n\nIn [2]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nimport os\nimport io\nimport re\nimport time\nimport glob\nimport requests\nimport urllib.request\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\n\n\n&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nIt is important to identify the datasets we are going to use for this exercise. The two identified datasets the group intends to use are: the 2019 National Data and the 2015-2016 Census data.\n\n\nWith regards to 2019 National data, the team used a web scapper provided Prof. Alis. The web scapper downloaded the election results from the Commission of Elections’ 2019 National and Local Elections website. The results were then stored in a local repository which is then easily accesible for the team. The 2019 elections results are broken down into two main directories: results and contest. In this exercise, the team will explore both directories to map out a comprehensive summary of the 2019 senatorial and party elections.\n\n\nSecondly, the 2015-2016 Census data has already been stored in a local repository for easier access. One of the main reasons why the team decided to use the 2015-2016 Census data is because of the lack of availability of recent data. The Philippine Statistics Authority only releases a comprehensive census survey ever six years. However for the purpose of this exercise, the team has agreed that the 2015-2016 census data can act as an appproximate for today’s population.\n\n\n\n\n\n\n\n\n\n\n\n A. Methodology\n\n\n\n\n\n\n\n\n\n\n\n Step 1: Extract and collect the 2019 Elections (Results) data\n\n\n\n\n\n\n\n\n\n\n\nThe first directory to explore is the 2019 Election results. The results directory contains all electoral results from a regional level down to the barangay level. For each level, a specific coc.json file is stored. This file contains all electoral results data and metadata for both national and local elections. However for the purposes of this analysis, we will only look at the aggregated elections data at the regional level. The files that we are interested are the coc.json files associated to each province, as these files contain the metadata and data on the election results.\n\n\n\n\n\n\n\n\n\n\n\nThe main structure of each coc.json file contains the following main keys: vbc, rs, sts, obs, and cos. For the purpose of this exercise, the important key the group needs to extract is the rs key as this provides the each candidate’s total votes per area. Under the rs key, the following keys can be found: cc, bo, v, tot, per, and ser. Cross referencing these keys with official statements and comelec documentations suggests that important keys are as follows: cc pertaining to contest_type, bo pertaining to the candidate_id, and v pertaining to votes_per_province.\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\ncc\n\n\nContest code\n\n\n\n\nbo\n\n\nContestant id\n\n\n\n\nv\n\n\nTotal votes per contestant\n\n\n\n\ntot\n\n\nTotal votes per province\n\n\n\n\n\n\nHowever, it must be pointed out that the available data only goes as high as provincial data. If we want to process the provincial level, the team will have to aggregate the data up.\n\n\n\n\n\n\n\n\n\n\n\nThe group created utility functions for easier retrieval of the provincial elections datasets. The purpose for the utility functions (and future utility functions) are for initial cleaning and manipulations. This is to ensure each dataset is ready for aggregation.\n\n\nThe get_province_coc method unpacks each key and value from the coc.json dictionary into a cleaned up dataframe. In addition, the method identifies which region and province the file originated from by examining the filepath that was passed.\n\n\nThe get_all_province_coc method is a walker that goes through each of the results directory. The walker checks if the filename has an equal value to coc.json. If a coc.json was located, the get_province_coc method is applied with the filepath as the parameter. The resulting dataframe is then appended to a master dataframe for further extraction and analysis. For this exercise, the group only had to extract data up to the regional and provincial levels so only three wildcard were use for the glob walker.\n\n\nSpecial methods (get_ncr_coc and get_all_ncr_coc) were established to get the cities’ coc.json. For the case of the NCR cities, theire associated coc.json files were one directory lower.\n\n\n\n\n\n\n\nIn [3]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndef get_province_coc(filepath):\n    \"\"\"\n    Loads a single coc file. \n\n    Adds additional columns `region` and `province to the DataFrame,\n    depending on filepath.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    output = []\n    with open(filepath, 'r') as f:\n        dirpath, filepath = os.path.split(filepath)\n        region = dirpath.split('/')[-2]\n        province = dirpath.split('/')[-1]\n        data = json.load(f)\n        for each in data['rs']:\n            row = [float(element) for element in list(each.values())]\n            output.append([data['vbc']] + row + [region] + [province])\n    df = pd.DataFrame(output,\n                      columns=['vbc', 'cc', 'bo', 'v', 'tot', 'per', 'ser',\n                               'region', 'province'])\n    return df\n\n\ndef get_all_province_coc(tree):\n    \"\"\"\n    Loads all province COC files and saves them to a dataframe\n\n    Checks the filepath if filename is 'coc.json'\n\n    Created a new column to deal with the reclassification of\n        \"NEGROS ORIENTAL\" and \"NEGROS OCCIDENTAL\" to \"NIR\" \n            to match the PSA 2016 dataset.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    total = pd.DataFrame()\n    for file in glob.glob(tree):\n        if os.path.basename(file) == 'coc.json':\n            df = get_province_coc(file)\n            total = total.append(df)\n    total.rename(columns={'region': 'region_raw'}, inplace=True)\n    total['region'] = total['region_raw'].copy()\n    total.loc[(total['province'] == \"NEGROS ORIENTAL\") |\n              (total['province'] == \"NEGROS OCCIDENTAL\"), 'region'] = 'NIR'\n    return total\n\n\n&lt;/div&gt;\n\n\n\n\n\n\nIn [4]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndef get_ncr_coc(filepath):\n    \"\"\"\n    Loads a single coc file. \n\n    Adds additional columns `region` and `province to the DataFrame,\n    depending on filepath.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe    \n\n    \"\"\"\n    output = []\n    with open(filepath, 'r') as f:\n        dirpath, filepath = os.path.split(filepath)\n        region = dirpath.split('/')[-3]\n        province = dirpath.split('/')[-2]\n        data = json.load(f)\n        for each in data['rs']:\n            row = [float(element) for element in list(each.values())]\n            output.append([data['vbc']] + row + [region] + [province])\n    df = pd.DataFrame(output,\n                      columns=['vbc', 'cc', 'bo', 'v', 'tot', 'per', 'ser',\n                               'region', 'province'])\n    return df\n\n\ndef get_all_ncr_coc(tree):\n    \"\"\"\n    Loads all province COC files and saves them to a dataframe\n\n    Checks the filepath if filename is 'coc.json'\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    total = pd.DataFrame()\n    for file in glob.glob(tree):\n        if file.split('/')[7] == 'NCR':\n            if os.path.basename(file) == 'coc.json':\n                df = get_ncr_coc(file)\n                total = total.append(df)\n    total.rename(columns={'region': 'region_raw'}, inplace=True)\n    total['region'] = total['region_raw'].copy()\n    return total\n\n\n&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nWith these utility functions inplace, the team can now apply these methods for easier access to the 2019 elections data.¶\n\n\nWe can now compile all of the election results with the following line:\n\n\n\n\n\n\n\nIn [5]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ntree = '/mnt/data/public/elections/nle2019/results/*/*/*'\nncr_tree = '/mnt/data/public/elections/nle2019/results/*/*/*/*'\ndf_results = get_all_province_coc(tree)\ndf_results = df_results.append(get_all_ncr_coc(ncr_tree))\ndf_results.drop_duplicates(inplace=True)\n\n\n&lt;/div&gt;\n\n\n\n\n\n\nIn [6]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndf_results.head(5)\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[6]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nvbc\n\n\ncc\n\n\nbo\n\n\nv\n\n\ntot\n\n\nper\n\n\nser\n\n\nregion_raw\n\n\nprovince\n\n\nregion\n\n\n\n\n\n\n0\n\n\n89550\n\n\n1.0\n\n\n1.0\n\n\n2004.0\n\n\n1708769.0\n\n\n0.11\n\n\n2800.0\n\n\nREGION I\n\n\nILOCOS NORTE\n\n\nREGION I\n\n\n\n\n1\n\n\n89550\n\n\n1.0\n\n\n2.0\n\n\n1607.0\n\n\n1708769.0\n\n\n0.09\n\n\n2800.0\n\n\nREGION I\n\n\nILOCOS NORTE\n\n\nREGION I\n\n\n\n\n2\n\n\n89550\n\n\n1.0\n\n\n3.0\n\n\n8772.0\n\n\n1708769.0\n\n\n0.51\n\n\n2800.0\n\n\nREGION I\n\n\nILOCOS NORTE\n\n\nREGION I\n\n\n\n\n3\n\n\n89550\n\n\n1.0\n\n\n4.0\n\n\n1767.0\n\n\n1708769.0\n\n\n0.10\n\n\n2800.0\n\n\nREGION I\n\n\nILOCOS NORTE\n\n\nREGION I\n\n\n\n\n4\n\n\n89550\n\n\n1.0\n\n\n5.0\n\n\n5068.0\n\n\n1708769.0\n\n\n0.29\n\n\n2800.0\n\n\nREGION I\n\n\nILOCOS NORTE\n\n\nREGION I\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, let us examine the obtained dataset with actual election results.¶\n\n\nBy cross checking the results with Comelec data, we can identify the senators and party names.\n\n\n\n\n\n\n\n\n\n\n\nJust to check our data, we can look at an example senator from the dataset. By choosing cc=1 and bo=46, we are actually highlighting Imee Marcos’ senatorial candidacy results.\n\n\n\n\n\n\n\nIn [7]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_marcos = df_results.query('cc == 1 & bo == 46').copy()\ndf_marcos.groupby('region').sum()['v'].sort_values(\n    ascending=True).plot.barh(figsize=(10, 10),\n                              title='Contestant: 46 - Imee Marcos',\n                              color='#BF5209', ax=ax);\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditionally, let us check some descriptive statistics for the 2019 Elections dataset. More specifically, let us examine the v or votes column. The group will be highly dependent on the votes data so let us first do some initial statistics and visualizations.\n\n\n\n\n\n\n\nIn [8]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndf_test = df_results.groupby(['region', 'province'])['v'].sum().to_frame()\ndf_test = df_test.rename(columns={'v': 'votes'})\ndf_test\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[8]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nvotes\n\n\n\n\nregion\n\n\nprovince\n\n\n\n\n\n\n\n\nBARMM\n\n\nBASILAN\n\n\n2093067.0\n\n\n\n\nLANAO DEL SUR\n\n\n4770462.0\n\n\n\n\nMAGUINDANAO\n\n\n5917983.0\n\n\n\n\nSULU\n\n\n3529555.0\n\n\n\n\nTAWI-TAWI\n\n\n1874486.0\n\n\n\n\nCAR\n\n\nABRA\n\n\n1923481.0\n\n\n\n\nAPAYAO\n\n\n703002.0\n\n\n\n\nBENGUET\n\n\n2426397.0\n\n\n\n\nIFUGAO\n\n\n1408688.0\n\n\n\n\nKALINGA\n\n\n1621414.0\n\n\n\n\nMOUNTAIN PROVINCE\n\n\n1074249.0\n\n\n\n\nNCR\n\n\nNATIONAL CAPITAL REGION - FOURTH DISTRICT\n\n\n22896771.0\n\n\n\n\nNATIONAL CAPITAL REGION - MANILA\n\n\n13461229.0\n\n\n\n\nNATIONAL CAPITAL REGION - SECOND DISTRICT\n\n\n29803007.0\n\n\n\n\nNATIONAL CAPITAL REGION - THIRD DISTRICT\n\n\n18481014.0\n\n\n\n\nTAGUIG - PATEROS\n\n\n10018306.0\n\n\n\n\nNIR\n\n\nNEGROS OCCIDENTAL\n\n\n12453486.0\n\n\n\n\nNEGROS ORIENTAL\n\n\n6900077.0\n\n\n\n\nREGION I\n\n\nILOCOS NORTE\n\n\n3614806.0\n\n\n\n\nILOCOS SUR\n\n\n4882048.0\n\n\n\n\nLA UNION\n\n\n5764844.0\n\n\n\n\nPANGASINAN\n\n\n19803192.0\n\n\n\n\nREGION II\n\n\nBATANES\n\n\n118412.0\n\n\n\n\nCAGAYAN\n\n\n6890994.0\n\n\n\n\nISABELA\n\n\n9632919.0\n\n\n\n\nNUEVA VIZCAYA\n\n\n3200395.0\n\n\n\n\nQUIRINO\n\n\n1198929.0\n\n\n\n\nREGION III\n\n\nAURORA\n\n\n1493909.0\n\n\n\n\nBATAAN\n\n\n6433641.0\n\n\n\n\nBULACAN\n\n\n19567555.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\nREGION VI\n\n\nGUIMARAS\n\n\n1217864.0\n\n\n\n\nILOILO\n\n\n11118379.0\n\n\n\n\nREGION VII\n\n\nBOHOL\n\n\n8977777.0\n\n\n\n\nCEBU\n\n\n19919139.0\n\n\n\n\nSIQUIJOR\n\n\n694813.0\n\n\n\n\nREGION VIII\n\n\nBILIRAN\n\n\n1051266.0\n\n\n\n\nEASTERN SAMAR\n\n\n3370187.0\n\n\n\n\nLEYTE\n\n\n9366390.0\n\n\n\n\nNORTHERN SAMAR\n\n\n3733282.0\n\n\n\n\nSAMAR (WESTERN SAMAR)\n\n\n5954800.0\n\n\n\n\nSOUTHERN LEYTE\n\n\n2558108.0\n\n\n\n\nREGION X\n\n\nBUKIDNON\n\n\n7415656.0\n\n\n\n\nCAMIGUIN\n\n\n768370.0\n\n\n\n\nLANAO DEL NORTE\n\n\n3584638.0\n\n\n\n\nMISAMIS OCCIDENTAL\n\n\n4391884.0\n\n\n\n\nMISAMIS ORIENTAL\n\n\n6488743.0\n\n\n\n\nREGION XI\n\n\nCOMPOSTELA VALLEY\n\n\n4281424.0\n\n\n\n\nDAVAO (DAVAO DEL NORTE)\n\n\n7056455.0\n\n\n\n\nDAVAO DEL SUR\n\n\n4457484.0\n\n\n\n\nDAVAO OCCIDENTAL\n\n\n1378314.0\n\n\n\n\nDAVAO ORIENTAL\n\n\n3200605.0\n\n\n\n\nREGION XII\n\n\nCOTABATO (NORTH COT.)\n\n\n7659458.0\n\n\n\n\nSARANGANI\n\n\n2990485.0\n\n\n\n\nSOUTH COTABATO\n\n\n9090438.0\n\n\n\n\nSULTAN KUDARAT\n\n\n4037874.0\n\n\n\n\nREGION XIII\n\n\nAGUSAN DEL NORTE\n\n\n4670054.0\n\n\n\n\nAGUSAN DEL SUR\n\n\n4155305.0\n\n\n\n\nDINAGAT ISLANDS\n\n\n735236.0\n\n\n\n\nSURIGAO DEL NORTE\n\n\n3979284.0\n\n\n\n\nSURIGAO DEL SUR\n\n\n4116338.0\n\n\n\n\n\n\n86 rows × 1 columns\n\n\n\n\n\n\n\n\n\n\nIn [9]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION III',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nIn [10]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['NCR',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nIn [11]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION IV-A',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nIn [12]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION VII',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo match the contestant ID to the contestant name, the contest files were also downloaded from the Commission of Elections’ 2019 National and Local Elections website and stored in the local repository. Similar to the results directory, the contest directory contained json files for each contest type/position. Upon inspection of a sample file within the directory, the following values were obtained:\n\n\nThe pertinent keys from each json files were:\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\ncc\n\n\nContest code\n\n\n\n\ncn\n\n\nContest code name - location\n\n\n\n\nccc\n\n\nContest code name\n\n\n\n\ntype\n\n\nContest type\n\n\n\n\nbos\n\n\nlist of candidate parameters\n\n\n\n\n\n\nUnder the bos key, we can extract each of the candidates’ parameters. The more useful ones for the group’s study include:\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\nboc\n\n\nContestant ID\n\n\n\n\nbon\n\n\nContestant Name\n\n\n\n\npn\n\n\nParty Name\n\n\n\n\n\n\n&lt;/font&gt;\n\n\n\n\n\n\n\n\n\n\n\n Step 2: Extract and collect the 2019 Elections (Contestant) data\n\n\n\n\n\n\n\n\n\n\n\nThe group also created utility functions for easier retrieval of the contestant datasets. This is to ensure each dataset is ready for aggregation.\n\n\nSimilar to the get_province_coc, the get_contestant_attrib method unpacks each key and value from the {contest_number}.json dictionary into a cleaned up dataframe. The method converts the bos directory into an additional list, which will also be appended into the resulting dataframe.\n\n\nThere are two (2) major political coalitions fighting for the senate seats:&lt;/font&gt;\n\n\n\nHugpong ng Pagbabago (HNP)\n\n\nOtso Diretso\n\n\n\nSimilar to the get_all_province_coc, the get_contestants_attrib method is a walker that goes through each of the contest directory. The method will first append all {contest_numer}.json files into a singular dataframe. Next, the method creates a new column that identifies who among the senatorial candidates are part of the Hugpong ng Pagbabago (HNP) or Otso Diretso campaign.\n\n\n\n\n\n\n\nIn [13]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndef get_contestant_attrib(filepath):\n    \"\"\"\n    Returns the contestant json file into a dataframe\n\n    Parameters\n    ----------\n    filepath   : string\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestnat attributes\n\n    \"\"\"\n\n    contestants_values = []\n    with open(filepath, 'r') as file:\n        data = json.load(file)\n        attrib_keys = [key for key in list(data.keys())\n                       if isinstance(key, (str, float, int))]\n        attrib_values = [value for value in list(data.values())\n                         if isinstance(value, (str, float, int))]\n        contest_values = [list(contest.values()) for contest in data['bos']]\n        df = pd.DataFrame(contest_values,\n                          columns=list(data['bos'][0].keys()))\n        for k, v in zip(attrib_keys, attrib_values):\n            df[k] = v\n    return df\n\n\ndef get_contestants_attrib(filepath):\n    \"\"\"\n    Returns ALL contestant json files into a dataframe\n\n    Parameters\n    ----------\n    filepath   : string\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestant attributes\n\n    \"\"\"\n    df = pd.DataFrame()\n    for each_filepath in glob.glob(filepath):\n        df = df.append(get_contestant_attrib(each_filepath))\n    senators = df[df.cc == 1].copy()\n    senators['bon'] = senators['bon'].str.extract(pat='(.*?) \\(')\n    party = df[df.cc == 5567].copy()\n    df = senators.append(party)\n    df.drop_duplicates(inplace=True)\n    df.rename(columns={'boc': 'bo'}, inplace=True)\n    otso = ['AQUINO, BENIGNO BAM ', 'DIOKNO, CHEL', 'HILBAY, PILO',\n            'MACALINTAL, MACAROMY', 'GUTOC, SAMIRA', 'ALEJANO, GARY',\n            'ROXAS, MAR', 'TAÑADA,LORENZO ERIN TAPAT']\n    hnp = ['ANGARA, EDGARDO SONNY', 'BONG REVILLA, RAMON JR', 'CAYETANO, PIA',\n           'DELA ROSA, BATO', 'EJERCITO, ESTRADA JV', 'ESTRADA, JINGGOY',\n           'GO, BONG GO', 'MANGUDADATU, DONG', 'MANICAD, JIGGY',\n           'MARCOS, IMEE', 'PIMENTEL, KOKO', 'TOLENTINO, FRANCIS', \n           'VILLAR, CYNTHIA']\n    for o in otso:\n        df.loc[df.bon == o, 'coalition'] = \"Otso Diretso\"\n    for h in hnp:\n        df.loc[df.bon == h, 'coalition'] = \"HNP\"\n    df['coalition'] = df['coalition'].fillna('None')\n    return df\n\n\n&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nLet us run the get_contestants_attrib. This will be used later in the blog for our further analysis.\n\n\n\n\n\n\n\nIn [15]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ncontestant_filepaths = '/mnt/data/public/elections/nle2019/contests/*'\ndf_contestants = get_contestants_attrib(contestant_filepaths)\ndf_contestants.head()\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[15]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nbo\n\n\nbon\n\n\nboi\n\n\nto\n\n\npc\n\n\npn\n\n\npcc\n\n\npcy\n\n\npcm\n\n\npck\n\n\ncc\n\n\ncn\n\n\nccc\n\n\nccn\n\n\npre\n\n\ntype\n\n\ncoalition\n\n\n\n\n\n\n0\n\n\n37\n\n\nHILBAY, PILO\n\n\n52.png\n\n\n37\n\n\n2\n\n\nAKSYON DEMOKRATIKO\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\nSENATOR PHILIPPINES\n\n\n1\n\n\nSENATOR\n\n\n3\n\n\nnational\n\n\nOtso Diretso\n\n\n\n\n1\n\n\n7\n\n\nALUNAN, RAFFY\n\n\n53.png\n\n\n7\n\n\n3\n\n\nBAGUMBAYAN VOLUNTEERS FOR A NEW PHILIPPINES\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\nSENATOR PHILIPPINES\n\n\n1\n\n\nSENATOR\n\n\n3\n\n\nnational\n\n\nNone\n\n\n\n\n2\n\n\n14\n\n\nBALDEVARONA, BALDE\n\n\n35.png\n\n\n14\n\n\n7\n\n\nFILIPINO FAMILY PARTY\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\nSENATOR PHILIPPINES\n\n\n1\n\n\nSENATOR\n\n\n3\n\n\nnational\n\n\nNone\n\n\n\n\n3\n\n\n18\n\n\nCASIÑO, TOTI\n\n\n20.png\n\n\n18\n\n\n8\n\n\nKATIPUNAN NG DEMOKRATIKONG PILIPINO(KDP)\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\nSENATOR PHILIPPINES\n\n\n1\n\n\nSENATOR\n\n\n3\n\n\nnational\n\n\nNone\n\n\n\n\n4\n\n\n21\n\n\nCHONG, GLENN\n\n\n61.png\n\n\n21\n\n\n8\n\n\nKATIPUNAN NG DEMOKRATIKONG PILIPINO(KDP)\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\nSENATOR PHILIPPINES\n\n\n1\n\n\nSENATOR\n\n\n3\n\n\nnational\n\n\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now have two dataframes: df_results containing the 2019 election results, and df_contestants containing the contestant information. These two dataframes can now be merged into a single dataframe. Let us also drop certain columns which we have deemed as unimportant.\n\n\n\n\n\n\n\nIn [16]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndef merge_comelec(results, contestants):\n    \"\"\"\n    Merge results dataframe with contestants dataframe\n\n    Parameters\n    ----------\n    results    : pd.DataFrame\n    contestants: pd.DataFrame\n\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestant attributes\n\n    \"\"\"\n\n    df = pd.merge(results, contestants, on=['bo', 'cc'], how='left')\n    df = df.drop(['vbc', 'boi', 'to', 'pc', 'pcc', 'pcy', 'pcm',\n                  'pck', 'ccc', 'pre', 'ser', 'cn'], axis=1)\n    df.columns = ['position', 'candidate_id', 'votes_per_province',\n                  'total_votes', 'votes_in_pct', 'region_raw', 'province',\n                  'region', 'candidate_name', 'party_name',\n                  'contest_position', 'contest_type', 'coalition']\n    return df\n\n\n&lt;/div&gt;\n\n\n\n\n\n\nIn [17]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nnle2019 = merge_comelec(df_results, df_contestants)\nnle2019.region.unique()\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[17]:&lt;/div&gt;\n\narray(['REGION I', 'REGION IV-B', 'BARMM', 'REGION II', 'REGION III',\n       'REGION V', 'REGION VI', 'NIR', 'REGION VII', 'REGION VIII',\n       'REGION IX', 'REGION X', 'REGION XI', 'REGION XII', 'REGION XIII',\n       'REGION IV-A', 'CAR', 'NCR'], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\n\n Step 3: Load Geopandas for geospatial processing ¶\n\n\nThe Philippines is composed of seventeen (17) administrative regions. We can use the geopandas module to manage and pre-process geospatial data.\n\n\nLet us first load up a geopandas graph of the Philippines.\n\n\n\n\n\n\n\nIn [18]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfig, ax = plt.subplots(1, figsize=(10, 15), frameon=True)\n\nph0 = gpd.GeoDataFrame.from_file(\"ph_regions.shp\")\nph0.plot(ax=ax, cmap='Greens', edgecolor='#555555', k=18)\nax.set_title('Regions of the Philippines')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\nfor ind, row in ph0.iterrows():\n    ax.text(row[\"geometry\"].centroid.x, row[\"geometry\"].centroid.y,\n            row[\"region\"])\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n B. National Senatorial Results¶\n\n\n\n\n\n\n\n\n\n\n\nLet us look at the senatorial candidates. Let us total up the votes by candidate and see the top 12 winners.¶\n\n\n\n\n\n\n\nIn [19]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndf_senators = nle2019[nle2019['contest_position'] == 'SENATOR']\ndf_senator = df_senators.groupby(['candidate_name']).agg(\n    {'votes_per_province': sum}).reset_index()\ndf_senator.sort_values('votes_per_province', ascending=False, inplace=True)\ndf_senator.columns = ['Candidate', 'Votes']\ndf_senator.head(12)\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[19]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nCandidate\n\n\nVotes\n\n\n\n\n\n\n61\n\n\nVILLAR, CYNTHIA\n\n\n23653546.0\n\n\n\n\n54\n\n\nPOE, GRACE\n\n\n20877585.0\n\n\n\n\n33\n\n\nGO, BONG GO\n\n\n18979132.0\n\n\n\n\n18\n\n\nCAYETANO, PIA\n\n\n18287782.0\n\n\n\n\n23\n\n\nDELA ROSA, BATO\n\n\n17396249.0\n\n\n\n\n7\n\n\nANGARA, EDGARDO SONNY\n\n\n16826634.0\n\n\n\n\n39\n\n\nLAPID, LITO\n\n\n16181906.0\n\n\n\n\n45\n\n\nMARCOS, IMEE\n\n\n14735294.0\n\n\n\n\n59\n\n\nTOLENTINO, FRANCIS\n\n\n14264142.0\n\n\n\n\n15\n\n\nBONG REVILLA, RAMON JR\n\n\n13899831.0\n\n\n\n\n14\n\n\nBINAY, NANCY\n\n\n13864931.0\n\n\n\n\n53\n\n\nPIMENTEL, KOKO\n\n\n13529531.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn [20]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfig, ax = plt.subplots(figsize=(15,8))\nplt.rcParams.update({'font.size': 14})\ndf_senator.set_index('Candidate').head(12).sort_values(\n    by='Votes', ascending=True).plot.barh(ax=ax,\n    title='Top 12 Candidates, in Millions', color='#BF5209', legend=False);\nax.set_xlabel('Total Votes');\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n C. Top Senator Per Region\n\n\nWe want to find out won across all the regions. If there is any bias for cetain candidates. Based on our findings, we can see that candidate Cynthia Villar won majority of the regions.\n\n\nIt is interesting to note that the top ranking senator for Ilocos Region (Region I) and the Cordillera Administrative Region (CAR) is Imee Marcos, which hails from that region. This confirms that there is a “Solid North”, and that support for the Marcoses still exists in that area.\n\n\nFor the Mindanao regions, the top candidate is Bong Go, former special assistant to President Duterte, who is from Mindanao.\n\n\nThese show that Philippine politics is very regional in nature. Voters will naturally support their hometown candidate, regardless of the issues surrounding that candidate.\n\n\n\n\n\n\n\nIn [21]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndf_senator_region = df_senators.groupby(\n    ['region', 'candidate_name']).agg({'votes_per_province': sum})\ndf_senator_region['rank'] = df_senator_region.groupby(\n    'region')['votes_per_province'].rank('dense', ascending=False)\ndf_senator_region = df_senator_region[df_senator_region['rank'] == 1].reset_index()\ndf_senator_region.columns = ['Region', 'Candidate Name', 'Votes', 'Rank']\ndf_senator_region\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[21]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nRegion\n\n\nCandidate Name\n\n\nVotes\n\n\nRank\n\n\n\n\n\n\n0\n\n\nBARMM\n\n\nGO, BONG GO\n\n\n768037.0\n\n\n1.0\n\n\n\n\n1\n\n\nCAR\n\n\nMARCOS, IMEE\n\n\n348303.0\n\n\n1.0\n\n\n\n\n2\n\n\nNCR\n\n\nVILLAR, CYNTHIA\n\n\n3345089.0\n\n\n1.0\n\n\n\n\n3\n\n\nNIR\n\n\nVILLAR, CYNTHIA\n\n\n856992.0\n\n\n1.0\n\n\n\n\n4\n\n\nREGION I\n\n\nMARCOS, IMEE\n\n\n1661318.0\n\n\n1.0\n\n\n\n\n5\n\n\nREGION II\n\n\nVILLAR, CYNTHIA\n\n\n915393.0\n\n\n1.0\n\n\n\n\n6\n\n\nREGION III\n\n\nVILLAR, CYNTHIA\n\n\n3056167.0\n\n\n1.0\n\n\n\n\n7\n\n\nREGION IV-A\n\n\nVILLAR, CYNTHIA\n\n\n3660112.0\n\n\n1.0\n\n\n\n\n8\n\n\nREGION IV-B\n\n\nVILLAR, CYNTHIA\n\n\n688494.0\n\n\n1.0\n\n\n\n\n9\n\n\nREGION IX\n\n\nGO, BONG GO\n\n\n663927.0\n\n\n1.0\n\n\n\n\n10\n\n\nREGION V\n\n\nPOE, GRACE\n\n\n1417114.0\n\n\n1.0\n\n\n\n\n11\n\n\nREGION VI\n\n\nVILLAR, CYNTHIA\n\n\n1064681.0\n\n\n1.0\n\n\n\n\n12\n\n\nREGION VII\n\n\nVILLAR, CYNTHIA\n\n\n1307605.0\n\n\n1.0\n\n\n\n\n13\n\n\nREGION VIII\n\n\nVILLAR, CYNTHIA\n\n\n1132757.0\n\n\n1.0\n\n\n\n\n14\n\n\nREGION X\n\n\nGO, BONG GO\n\n\n949392.0\n\n\n1.0\n\n\n\n\n15\n\n\nREGION XI\n\n\nGO, BONG GO\n\n\n1002771.0\n\n\n1.0\n\n\n\n\n16\n\n\nREGION XII\n\n\nVILLAR, CYNTHIA\n\n\n983354.0\n\n\n1.0\n\n\n\n\n17\n\n\nREGION XIII\n\n\nGO, BONG GO\n\n\n777931.0\n\n\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn [22]:\n\n\n&lt;div class=\"input_area\"&gt;\n\n(df_senator_region[df_senator_region['Rank'] == 1].reset_index().groupby(\n    'Candidate Name')['Rank'].sum().to_frame().sort_values(by='Rank')\n .reset_index())\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[22]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nCandidate Name\n\n\nRank\n\n\n\n\n\n\n0\n\n\nPOE, GRACE\n\n\n1.0\n\n\n\n\n1\n\n\nMARCOS, IMEE\n\n\n2.0\n\n\n\n\n2\n\n\nGO, BONG GO\n\n\n5.0\n\n\n\n\n3\n\n\nVILLAR, CYNTHIA\n\n\n10.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn [23]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nfig, ax = plt.subplots()\n(df_senator_region[df_senator_region['Rank'] == 1].reset_index().groupby(\n    'Candidate Name')['Rank'].sum().to_frame().sort_values(by='Rank').plot\n .barh(color='#BF5209', ax=ax));\nax.set_xlabel('Rank');\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n D. Dominant Party per Region\n\n\n\n\n\n\n\n\n\n\n\nThere are three main political parties vying for the senatorial seats:\n\n\n\nLIBERAL PARTY\n\n\nNACIONALISTA PARTY\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN (PDP-LABAN)\n\n\n\nWe looked at the dominant or majority political party per administrative region, and identify if any regions have any affiliations to a certain party.\n\n\n\n\n\n\n\nIn [24]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndf_party = df_senators.groupby(['region_raw', 'party_name']).agg({\n    'votes_per_province': sum})\ndf_party['rank'] = df_party.groupby(\n    'region_raw')['votes_per_province'].rank('dense', ascending=False)\ndf_party.reset_index(inplace=True)\ndf_leading_party = df_party[df_party['rank'] == 1].copy()\ndf_leading_party.columns = ['Region', 'Party', 'Votes', 'Rank']\ndf_leading_party.sort_values(['Region', 'Rank'], inplace=True)\ndf_leading_party\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[24]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nRegion\n\n\nParty\n\n\nVotes\n\n\nRank\n\n\n\n\n\n\n13\n\n\nBARMM\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n2596572.0\n\n\n1.0\n\n\n\n\n32\n\n\nCAR\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n1028536.0\n\n\n1.0\n\n\n\n\n51\n\n\nNCR\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n10400476.0\n\n\n1.0\n\n\n\n\n68\n\n\nREGION I\n\n\nNACIONALISTA PARTY\n\n\n4361859.0\n\n\n1.0\n\n\n\n\n87\n\n\nREGION II\n\n\nNACIONALISTA PARTY\n\n\n2474544.0\n\n\n1.0\n\n\n\n\n108\n\n\nREGION III\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n7804389.0\n\n\n1.0\n\n\n\n\n127\n\n\nREGION IV-A\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n9542856.0\n\n\n1.0\n\n\n\n\n146\n\n\nREGION IV-B\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n1733733.0\n\n\n1.0\n\n\n\n\n165\n\n\nREGION IX\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n2571518.0\n\n\n1.0\n\n\n\n\n180\n\n\nREGION V\n\n\nLIBERAL PARTY\n\n\n4885924.0\n\n\n1.0\n\n\n\n\n199\n\n\nREGION VI\n\n\nLIBERAL PARTY\n\n\n4335427.0\n\n\n1.0\n\n\n\n\n222\n\n\nREGION VII\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n5480074.0\n\n\n1.0\n\n\n\n\n241\n\n\nREGION VIII\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n3442654.0\n\n\n1.0\n\n\n\n\n260\n\n\nREGION X\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n3697535.0\n\n\n1.0\n\n\n\n\n279\n\n\nREGION XI\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n3654361.0\n\n\n1.0\n\n\n\n\n298\n\n\nREGION XII\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n3861166.0\n\n\n1.0\n\n\n\n\n317\n\n\nREGION XIII\n\n\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n\n\n2858376.0\n\n\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can clearly see a voting bias of each region.\n\n\nMajority of Region 1 and Region 2 has a voting preference towards the NACIONALISTA PARTY. This can be attributed to the fact that Imee Marcos, who hails from Ilocos Norte, is a Nacionalista.\n\n\nMajority of Region 5 and Region 6 has a voting preference towards the LIBERAL PARTY. This is also expected since Leni Robredo, incumbent Vice President who is from the Liberal Party, is a Bicolano.\n\n\nThe remainder of the Philippines has a voting preference towards the PDP-LABAN.\n\n\n\n\n\n\n\nIn [25]:\n\n\n&lt;div class=\"input_area\"&gt;\n\nmerged = ph0.merge(df_leading_party, left_on='region', right_on='Region')\ncolors = 18\ncolor_map = {'PARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN': 'red', \n             'LIBERAL PARTY': 'yellow',\n             'NACIONALISTA PARTY': 'green'}\n\nfig, ax = plt.subplots(1, 1, figsize = (10,12));\nax.set_title('Dominant political party per region', fontsize=20);\n\nfor party in list(merged['Party'].unique()):\n    color_map[party]\n    merged[merged['Party'] == party].plot(ax=ax, color = color_map[party], \n                                          categorical = True, \n                                          figsize=(10,12), legend=True)\n\nmerged.geometry.boundary.plot(color=None,edgecolor='k',linewidth = .5,ax=ax);\nplt.rcParams.update({'font.size': 18})\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n E. Dominant Coalition Per Region\n\n\n\n\n\n\n\n\n\n\n\nAside from individual candidates and parties, we also looked at the dominant coalition per region by counting the number of senate seats obtained by each coalition. The results indicate that HNP gained a majority of the seats across all regions, especially in Mindanao.\n\n\nThe only regions where HNP did not gain a solid majority are in Bicol Region (Region V) and Eastern Visayas (Region VI), known bailwicks of the Liberal Party.\n\n\n\n\n\n\n\nIn [26]:\n\n\n&lt;div class=\"input_area\"&gt;\n\n\ndef get_coalition_seats():\n    \"\"\"\n    Returns a dataframe of the number of seats won per coalition per region\n\n    Returns\n    -------\n    coalition : pd.DataFrame\n\n    \"\"\"\n    coalition_seats = nle2019.query('position == 1')\n\n    coalition_seats = coalition_seats.groupby(\n        ['region', 'candidate_name', 'coalition'],\n        as_index=False).agg({'votes_per_province': sum})\n    coalition_seats = coalition_seats.sort_values(\n        by=['region', 'votes_per_province'], ascending=[1, 0])\n    coalition_seats.set_index('region', inplace=True)\n\n    coalition_seats['rank'] = list(range(1, len(\n        coalition_seats.candidate_name.unique())+1))\\\n        * len(coalition_seats.index.unique())\n    coalition_seats['is_top_12'] = 1*(coalition_seats['rank'] &lt;= 12)\n    coalition_seats = coalition_seats.reset_index()\n    coalition_seats = pd.pivot_table(\n        coalition_seats, index='region', columns='coalition',\n        values='is_top_12', aggfunc=np.sum)\n    coalition_seats['coalition_seats_total'] = coalition_seats.sum(axis=1)\n\n    for coalition in coalition_seats.columns[:-1]:\n        coalition_seats['party_seats_pct_' + coalition] = \\\n            coalition_seats[coalition] / \\\n            coalition_seats['coalition_seats_total']\n        coalition_seats.rename(\n            columns={coalition: 'coalition_seats_count_' + coalition},\n            inplace=True)\n\n    coalition_seats = coalition_seats.round(5)\n    coalition_seats = coalition_seats.reset_index()\n    return coalition_seats\n\n\ncoalition = get_coalition_seats()\nmerged = ph0.merge(coalition[[\n                   'region', 'coalition_seats_count_HNP']], left_on='region',\n                   right_on='region')\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 15))\nmerged.plot('coalition_seats_count_HNP', ax=ax, cmap='YlOrRd', legend=True)\nax.set_title('Number of HNP senate seats won', fontsize=24)\nmerged.geometry.boundary.plot(color=None, edgecolor='k', linewidth=1, ax=ax)\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[26]:&lt;/div&gt;\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7e6f1d58d0&gt;\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n F. Is Demographics Related To Voting Preference?\n\n\n\n\n\n\n\n\n\n\n\nIs the voting preference of a region related to its demographics such as literacy rate and religious affiliation? To answer this, we obtained the 2015 Census Data. The directory is a collection of excel files, where each excel file corresponds to a certain region and province. If we explore each file, we can see that each sheet corresponds to a different demographic feature table. For this analysis, we are intrested at sheets T8 and T11.\n\n\nThe get_census_religion loads the imporant columns and rows from sheet T8. It also adds an additional column based on the region. Similarly, the get_census_education loads the imporant columns and rows from sheet T11. It also aggregates each individual years experience column into a singular cumulative column.\n\n\nFinally, the read_census_files aggregates the 2016 regional data into singular dataframe by using get_census_religion and get_census_education functions. To extract only the regional files, the read_census_files uses regex to get filenames with only underscores in the beginning (this is an indicator of regional data).\n\n\n\n\n\n\n\nIn [27]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndef get_census_religion(path):\n    \"\"\"\n    Returns a consolidated DataFrame of census data by religion\n\n    Parameters\n    ----------\n    path      : string, filepath to census directory\n\n\n    Returns\n    ----------\n    df        : pd.DataFrame\n\n    \"\"\"\n    filename = os.path.basename(path)\n    df = pd.read_excel(path, sheet_name='T8', header=None,\n                       usecols=[0, 1, 2, 3],\n                       skiprows=6, skip_blank_lines=True,\n                       skipfooter=3,\n                       names=['religion', 'total', 'm', 'f'])\n    df.sort_values('total', ascending=False, inplace=True)\n    df['region'] = re.search(r'\\_(.*?)\\_', os.path.basename(path)).group(1)\n    cols = ['region', 'religion', 'm', 'f', 'total']\n    df = df[cols]\n    return df\n\n\ndef get_census_education(path):\n    \"\"\"\n    Returns a consolidated DataFrame of census data by education\n\n    Parameters\n    ----------\n    path      : string, filepath to census directory\n\n\n    Returns\n    ----------\n    df        : pd.DataFrame\n\n    \"\"\"\n    filename = os.path.basename(path)\n    df = pd.read_excel(path, sheet_name='T11',\n                       usecols=[0, 15, 16, 17, 18, 19, 20],\n                       skiprows=3,\n                       skip_blank_lines=True, nrows=20,\n                       names=['education', '18', '19',\n                              '20_24', '25_29', '30_34', '35_above'])\n    df.dropna(how='any', inplace=True)\n    df.reset_index(inplace=True, drop=True)\n    df.drop(df.index[[0, 5, 6, 7, 9, 10, 12, 13]], inplace=True)\n    df['total'] = (df['18'] + df['19'] + df['20_24'] + df['25_29'] +\n                   df['30_34'] + df['35_above'])\n    df['region'] = re.search(r'\\_(.*?)\\_', os.path.basename(path)).group(1)\n    cols = ['region', 'education', '18', '19', '20_24', '25_29', '30_34',\n            '35_above', 'total']\n    df = df[cols]\n    return df\n\n\ndef read_census_files(path):\n    '''\n    Reads all census regional files\n\n    Parameter\n    ---------\n    path      : string, filepath to census directory\n\n    Returns\n    -------\n    Dictionary of dataframes\n    '''\n    total = {'religion': pd.DataFrame(),\n             'education': pd.DataFrame()}\n\n    for filepath in glob.glob(path + \".xls\", recursive=True):\n        if re.match('_(?!PHILIPPINES)', os.path.basename(filepath)):\n            total['religion'] = (total['religion']\n                                 .append(get_census_religion(filepath)))\n            total['education'] = (total['education']\n                                  .append(get_census_education(filepath)))\n    total['religion'].reset_index(inplace=True, drop=True)\n    total['education'].reset_index(inplace=True, drop=True)\n\n    for df in total.values():\n        df.loc[(df['region'] == \"ARMM\"), 'region'] = 'BARMM'\n        df.loc[(df['region'] == \"MIMAROPA\"), 'region'] = 'REGION IV-B'\n        df.loc[(df['region'] == \"CARAGA\"), 'region'] = 'REGION XIII'\n    return total\n\n\n&lt;/div&gt;\n\n\n\n\n\n\nIn [28]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ncensus_path = '/mnt/data/public/census/*'\ncensus_dict = read_census_files(census_path)\n\n\n&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n G. Is literacy rate related to voting preference?\n\n\n\n\n\n\n\n\n\n\n\nWe looked at the number of seats obtained by each coalition per region, then correlated it to literacy rate. Education information was obtained from the 2015 Census data.\n\n\n\n\n\n\n\n\n\n\n\nWe looked at the literacy rate \\(\\frac{n_{educated}}{n_{total}}\\) of each administrative region:\n\n\n\n\n\n\n\nIn [29]:\n\n\n&lt;div class=\"input_area\"&gt;\n\n\ndef get_education_percent():\n    '''\n    Gets percentage of educational level per region\n    '''\n    education = census_dict['education'].groupby(\n        ['region', 'education'], as_index=False).sum()\n    education = pd.pivot_table(\n        education, index='region', columns='education', values='total')\n    education.columns = ['education_pct_' + educ for educ in education.columns]\n    education['education_total'] = education.sum(axis=1)\n    for educ in education.columns[:-1]:\n        education[educ] /= education['education_total']\n    education.drop('education_total', axis=1, inplace=True)\n    education = education.round(5)\n\n    return education\n\n\ndef get_agg_education_percent():\n    '''\n    Gets aggregated percentage of educational level per region\n    '''\n    df_educ = get_education_percent()\n    df_educ = df_educ.reset_index()\n    df_educ['educated'] = (1 - df_educ['education_pct_No Grade Completed']\n                           - df_educ['education_pct_Not Stated'])\n    df_educ['not_educated'] = df_educ['education_pct_No Grade Completed']\n    df_educ['unknown'] = df_educ['education_pct_Not Stated']\n    df_educ.drop(columns=['education_pct_Academic Degree Holder',\n                          'education_pct_College Undergraduate',\n                          'education_pct_Elementary',\n                          'education_pct_High School',\n                          'education_pct_No Grade Completed',\n                          'education_pct_Not Stated',\n                          'education_pct_Post Baccalaureate',\n                          'education_pct_Post-Secondary',\n                          'education_pct_Pre-School',\n                          'education_pct_Special Education'], inplace=True)\n    df_educ.set_index('region', inplace=True)\n    df_educ.reset_index(inplace=True)\n    return df_educ\n\n\ndf_educ = get_agg_education_percent()\ndf_educ\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[29]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nregion\n\n\neducated\n\n\nnot_educated\n\n\nunknown\n\n\n\n\n\n\n0\n\n\nBARMM\n\n\n0.85355\n\n\n0.14105\n\n\n0.00540\n\n\n\n\n1\n\n\nCAR\n\n\n0.97376\n\n\n0.02622\n\n\n0.00002\n\n\n\n\n2\n\n\nNCR\n\n\n0.99234\n\n\n0.00221\n\n\n0.00545\n\n\n\n\n3\n\n\nNIR\n\n\n0.98085\n\n\n0.01882\n\n\n0.00033\n\n\n\n\n4\n\n\nREGION I\n\n\n0.99321\n\n\n0.00679\n\n\n0.00000\n\n\n\n\n5\n\n\nREGION II\n\n\n0.98610\n\n\n0.01389\n\n\n0.00001\n\n\n\n\n6\n\n\nREGION III\n\n\n0.99355\n\n\n0.00615\n\n\n0.00030\n\n\n\n\n7\n\n\nREGION IV-A\n\n\n0.99473\n\n\n0.00519\n\n\n0.00008\n\n\n\n\n8\n\n\nREGION IV-B\n\n\n0.96573\n\n\n0.03417\n\n\n0.00010\n\n\n\n\n9\n\n\nREGION IX\n\n\n0.96456\n\n\n0.03486\n\n\n0.00058\n\n\n\n\n10\n\n\nREGION V\n\n\n0.99126\n\n\n0.00851\n\n\n0.00023\n\n\n\n\n11\n\n\nREGION VI\n\n\n0.98721\n\n\n0.01257\n\n\n0.00022\n\n\n\n\n12\n\n\nREGION VII\n\n\n0.98753\n\n\n0.01209\n\n\n0.00038\n\n\n\n\n13\n\n\nREGION VIII\n\n\n0.97679\n\n\n0.02297\n\n\n0.00024\n\n\n\n\n14\n\n\nREGION X\n\n\n0.98095\n\n\n0.01856\n\n\n0.00049\n\n\n\n\n15\n\n\nREGION XI\n\n\n0.97818\n\n\n0.02050\n\n\n0.00132\n\n\n\n\n16\n\n\nREGION XII\n\n\n0.94792\n\n\n0.05034\n\n\n0.00174\n\n\n\n\n17\n\n\nREGION XIII\n\n\n0.98487\n\n\n0.01503\n\n\n0.00010\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe then checked if the number of seats obtained by each coalition is correlated to the literacy rate of that region. First, we obtained the number of seats obtained by each coalition per region:\n\n\n\n\n\n\n\nIn [30]:\n\n\n&lt;div class=\"input_area\"&gt;\n\n\ndef get_coalition_rank():\n    \"\"\"\n    Get number of seats obtained by each coalition\n    \"\"\"\n    coalition_seats = nle2019.query('position == 1')\n    coalition_seats = coalition_seats.groupby(\n        ['region', 'candidate_name', 'coalition'], as_index=False).agg({'votes_per_province': sum})\n    coalition_seats = coalition_seats.sort_values(\n        by=['region', 'votes_per_province'], ascending=[1, 0])\n    coalition_seats.set_index('region', inplace=True)\n\n    coalition_seats['rank'] = list(range(1, len(\n        coalition_seats.candidate_name.unique())+1)) * len(coalition_seats.index.unique())\n    coalition_seats['is_top_12'] = 1*(coalition_seats['rank'] &lt;= 12)\n    coalition_seats.reset_index(inplace=True)\n\n    coalition_seats = pd.pivot_table(\n        coalition_seats, index='region', columns='coalition', values='is_top_12', aggfunc=np.sum)\n    return coalition_seats\n\n\n&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nWe then merge this with the education dataframe, then get the correlation:\n\n\n\n\n\n\n\nIn [31]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndf_coal_educ = get_agg_education_percent().set_index('region').join(get_coalition_rank())\ndf_coal_educ['Educated'] = df_coal_educ['educated']\ndf_coal_educ['Not Educated'] = df_coal_educ['not_educated']\ncorr = df_coal_educ.corr().loc[['Otso Diretso', 'HNP'],['Educated', 'Not Educated']]\ncolormap = sns.diverging_palette(100, 100, n = 10)\nsns.heatmap(corr, cmap=colormap, annot=True, vmin = -1, vmax = 1);\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Census and election data show that the voting preference of a region has no correlation with its literacy rate. We now look at religion to see if it has a correlation with the voting preference.\n\n\n\n\n\n\n\n\n\n\n\n H. Is religion related to voting preference?\n\n\n\n\n\n\n\n\n\n\n\nWe also looked into the religious affiliation per region, and checked if it is correlatd with voting preference.\n\n\nFirst, we obtained the distribution of religions per region from the Census data:\n\n\n\n\n\n\n\nIn [32]:\n\n\n&lt;div class=\"input_area\"&gt;\n\n\ndef get_religion_percent():\n    \"\"\"\n    Get percentages of religion per region\n    \"\"\"\n    religion = census_dict['religion'].groupby(\n        ['region', 'religion'], as_index=False).sum()\n    religion = pd.pivot_table(\n        religion, index='region', columns='religion', values='total')\n    religion.columns = ['religion_pct_' + rel for rel in religion.columns]\n    religion['religion_total'] = religion.sum(axis=1)\n    for rel in religion.columns[:-1]:\n        religion[rel] /= religion['religion_total']\n    religion.drop('religion_total', axis=1, inplace=True)\n    religion = religion.round(5)\n    return religion\n\n\ndf_rel = get_religion_percent()\n\n\n&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nWe then merged the religion census data with the coalition ranking data to check if religion has correlation with the number of seats obtained by each coalition:\n\n\n\n\n\n\n\nIn [33]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndf_coal_rel = get_religion_percent().join(get_coalition_rank())\ndf_coal_rel.head()\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt output_prompt\"&gt;Out[33]:&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nreligion_pct_Aglipay\n\n\nreligion_pct_Association of Baptist Churches in Luzon, Visayas, and Mindanao\n\n\nreligion_pct_Association of Fundamental Baptist Churches in the Philippines\n\n\nreligion_pct_Bible Baptist Church\n\n\nreligion_pct_Bread of Life Ministries\n\n\nreligion_pct_Buddhist\n\n\nreligion_pct_Cathedral of Praise, Incorporated\n\n\nreligion_pct_Church of Christ\n\n\nreligion_pct_Church of Jesus Christ of the Latter Day Saints\n\n\nreligion_pct_Convention of the Philippine Baptist Church\n\n\n…\n\n\nreligion_pct_Tribal Religions\n\n\nreligion_pct_UNIDA Evangelical Church\n\n\nreligion_pct_Union Espiritista Cristiana de Filipinas, Incorporated\n\n\nreligion_pct_United Church of Christ in the Philippines\n\n\nreligion_pct_United Pentecostal Church (Philippines), Incorporated\n\n\nreligion_pct_Victory Chapel Christian Fellowship\n\n\nreligion_pct_Way of Salvation Church, Incorporated\n\n\nHNP\n\n\nNone\n\n\nOtso Diretso\n\n\n\n\nregion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBARMM\n\n\n0.00030\n\n\n0.00001\n\n\n0.00010\n\n\n0.00077\n\n\n0.00000\n\n\n0.00000\n\n\n0.00000\n\n\n0.00008\n\n\n0.00001\n\n\n0.00000\n\n\n…\n\n\n0.00425\n\n\n0.00000\n\n\n0.00067\n\n\n0.00029\n\n\n0.00021\n\n\n0.00001\n\n\n0.00000\n\n\n9\n\n\n2\n\n\n1\n\n\n\n\nCAR\n\n\n0.00340\n\n\n0.00006\n\n\n0.00487\n\n\n0.01283\n\n\n0.00004\n\n\n0.00011\n\n\n0.00002\n\n\n0.00619\n\n\n0.00129\n\n\n0.00001\n\n\n…\n\n\n0.00186\n\n\n0.00001\n\n\n0.01327\n\n\n0.02350\n\n\n0.01824\n\n\n0.00016\n\n\n0.00011\n\n\n9\n\n\n3\n\n\n0\n\n\n\n\nNCR\n\n\n0.00137\n\n\n0.00003\n\n\n0.00005\n\n\n0.00434\n\n\n0.00022\n\n\n0.00113\n\n\n0.00013\n\n\n0.00188\n\n\n0.00121\n\n\n0.00002\n\n\n…\n\n\n0.00018\n\n\n0.00002\n\n\n0.00001\n\n\n0.00024\n\n\n0.00045\n\n\n0.00532\n\n\n0.00003\n\n\n9\n\n\n3\n\n\n0\n\n\n\n\nNIR\n\n\n0.02162\n\n\n0.00006\n\n\n0.00728\n\n\n0.01026\n\n\n0.00007\n\n\n0.00007\n\n\n0.00002\n\n\n0.00267\n\n\n0.00434\n\n\n0.00117\n\n\n…\n\n\n0.00098\n\n\n0.00000\n\n\n0.00002\n\n\n0.01507\n\n\n0.00196\n\n\n0.00100\n\n\n0.00001\n\n\n7\n\n\n3\n\n\n2\n\n\n\n\nREGION I\n\n\n0.00694\n\n\n0.00004\n\n\n0.00010\n\n\n0.00425\n\n\n0.00009\n\n\n0.00009\n\n\n0.00004\n\n\n0.00370\n\n\n0.00388\n\n\n0.00005\n\n\n…\n\n\n0.00029\n\n\n0.00001\n\n\n0.00259\n\n\n0.00551\n\n\n0.00492\n\n\n0.00032\n\n\n0.00108\n\n\n9\n\n\n3\n\n\n0\n\n\n\n\n\n\n5 rows × 56 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom getting the correlation of the religion data with the number of seats per coalition, it is apparent that the Voting preference of a region has no correlation with its religious affiliation.\n\n\n\n\n\n\n\nIn [34]:\n\n\n&lt;div class=\"input_area\"&gt;\n\ndf_coal_rel['Roman Catholic'] = df_coal_rel['religion_pct_Roman Catholic, including Catholic Charismatic']\ndf_coal_rel['Islam'] = df_coal_rel['religion_pct_Islam']\ndf_coal_rel['Iglesia ni Cristo'] = df_coal_rel['religion_pct_Iglesia ni Cristo']\ncorr = df_coal_rel.corr().loc[\n    ['Roman Catholic','Islam','Iglesia ni Cristo'],['HNP','Otso Diretso']]\ncolormap = sns.diverging_palette(100, 100, n = 10)\nsns.heatmap(corr, cmap=colormap, annot=True, vmin = -1, vmax = 1);\n\n\n&lt;/div&gt;\n\n\n\n\n\n&lt;div class=\"prompt\"&gt;&lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n I. Conclusion\n\n\n\n\n\n\n\n\n\n\n\nUpon checking both the Comelec 2019 Election Results and the 2015 Philippine Census data, we found out that voting preference is characterized by high regionality. Candidates have a homecourt advantage, and voters tend to vote candidates or parties affiliated with their home region.\n\n\nAlso, literacy rate and religious affiliation is not correlated to voting preference."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": "",
    "text": "The Philippines recently concluded its 2019 midterm elections last May 13. Sixty-two (62) candidates from various political parties contested for twelve (12) seats in the Senate of The Philippines. Given the unexpected results, the team was wondering on the following topics:\nfrom IPython.display import HTML\n\nHTML('''&lt;script&gt;\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n\n  $( document ).ready(function(){\n    code_shown=false;\n    $('div.input').hide()\n  });\n&lt;/script&gt;\n&lt;form action=\"javascript:code_toggle()\"&gt;&lt;input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"&gt;&lt;/form&gt;''')\nBefore anything else, let us first load all important modules for this exercise.\nimport os\nimport io\nimport re\nimport time\nimport glob\nimport requests\nimport urllib.request\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nIt is important to identify the datasets we are going to use for this exercise. The two identified datasets the group intends to use are: the 2019 National Data and the 2015-2016 Census data.\nWith regards to 2019 National data, the team used a web scapper provided Prof. Alis. The web scapper downloaded the election results from the Commission of Elections’ 2019 National and Local Elections website. The results were then stored in a local repository which is then easily accesible for the team. The 2019 elections results are broken down into two main directories: results and contest. In this exercise, the team will explore both directories to map out a comprehensive summary of the 2019 senatorial and party elections.\nSecondly, the 2015-2016 Census data has already been stored in a local repository for easier access. One of the main reasons why the team decided to use the 2015-2016 Census data is because of the lack of availability of recent data. The Philippine Statistics Authority only releases a comprehensive census survey ever six years. However for the purpose of this exercise, the team has agreed that the 2015-2016 census data can act as an appproximate for today’s population."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#a.-methodology",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#a.-methodology",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " A. Methodology",
    "text": "A. Methodology\n\n Step 1: Extract and collect the 2019 Elections (Results) data\nThe first directory to explore is the 2019 Election results. The results directory contains all electoral results from a regional level down to the barangay level. For each level, a specific coc.json file is stored. This file contains all electoral results data and metadata for both national and local elections. However for the purposes of this analysis, we will only look at the aggregated elections data at the regional level. The files that we are interested are the coc.json files associated to each province, as these files contain the metadata and data on the election results.\nThe main structure of each coc.json file contains the following main keys: vbc, rs, sts, obs, and cos. For the purpose of this exercise, the important key the group needs to extract is the rs key as this provides the each candidate’s total votes per area. Under the rs key, the following keys can be found: cc, bo, v, tot, per, and ser. Cross referencing these keys with official statements and comelec documentations suggests that important keys are as follows: cc pertaining to contest_type, bo pertaining to the candidate_id, and v pertaining to votes_per_province.\n\n\n\nParameter\nDescription\n\n\n\n\ncc\nContest code\n\n\nbo\nContestant id\n\n\nv\nTotal votes per contestant\n\n\ntot\nTotal votes per province\n\n\n\nHowever, it must be pointed out that the available data only goes as high as provincial data. If we want to process the provincial level, the team will have to aggregate the data up.\nThe group created utility functions for easier retrieval of the provincial elections datasets. The purpose for the utility functions (and future utility functions) are for initial cleaning and manipulations. This is to ensure each dataset is ready for aggregation.\nThe get_province_coc method unpacks each key and value from the coc.json dictionary into a cleaned up dataframe. In addition, the method identifies which region and province the file originated from by examining the filepath that was passed.\nThe get_all_province_coc method is a walker that goes through each of the results directory. The walker checks if the filename has an equal value to coc.json. If a coc.json was located, the get_province_coc method is applied with the filepath as the parameter. The resulting dataframe is then appended to a master dataframe for further extraction and analysis. For this exercise, the group only had to extract data up to the regional and provincial levels so only three wildcard were use for the glob walker.\nSpecial methods (get_ncr_coc and get_all_ncr_coc) were established to get the cities’ coc.json. For the case of the NCR cities, theire associated coc.json files were one directory lower.\n\ndef get_province_coc(filepath):\n    \"\"\"\n    Loads a single coc file. \n\n    Adds additional columns `region` and `province to the DataFrame,\n    depending on filepath.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    output = []\n    with open(filepath, 'r') as f:\n        dirpath, filepath = os.path.split(filepath)\n        region = dirpath.split('/')[-2]\n        province = dirpath.split('/')[-1]\n        data = json.load(f)\n        for each in data['rs']:\n            row = [float(element) for element in list(each.values())]\n            output.append([data['vbc']] + row + [region] + [province])\n    df = pd.DataFrame(output,\n                      columns=['vbc', 'cc', 'bo', 'v', 'tot', 'per', 'ser',\n                               'region', 'province'])\n    return df\n\n\ndef get_all_province_coc(tree):\n    \"\"\"\n    Loads all province COC files and saves them to a dataframe\n\n    Checks the filepath if filename is 'coc.json'\n\n    Created a new column to deal with the reclassification of\n        \"NEGROS ORIENTAL\" and \"NEGROS OCCIDENTAL\" to \"NIR\" \n            to match the PSA 2016 dataset.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    total = pd.DataFrame()\n    for file in glob.glob(tree):\n        if os.path.basename(file) == 'coc.json':\n            df = get_province_coc(file)\n            total = total.append(df)\n    total.rename(columns={'region': 'region_raw'}, inplace=True)\n    total['region'] = total['region_raw'].copy()\n    total.loc[(total['province'] == \"NEGROS ORIENTAL\") |\n              (total['province'] == \"NEGROS OCCIDENTAL\"), 'region'] = 'NIR'\n    return total\n\n\ndef get_ncr_coc(filepath):\n    \"\"\"\n    Loads a single coc file. \n\n    Adds additional columns `region` and `province to the DataFrame,\n    depending on filepath.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe    \n\n    \"\"\"\n    output = []\n    with open(filepath, 'r') as f:\n        dirpath, filepath = os.path.split(filepath)\n        region = dirpath.split('/')[-3]\n        province = dirpath.split('/')[-2]\n        data = json.load(f)\n        for each in data['rs']:\n            row = [float(element) for element in list(each.values())]\n            output.append([data['vbc']] + row + [region] + [province])\n    df = pd.DataFrame(output,\n                      columns=['vbc', 'cc', 'bo', 'v', 'tot', 'per', 'ser',\n                               'region', 'province'])\n    return df\n\n\ndef get_all_ncr_coc(tree):\n    \"\"\"\n    Loads all province COC files and saves them to a dataframe\n\n    Checks the filepath if filename is 'coc.json'\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    total = pd.DataFrame()\n    for file in glob.glob(tree):\n        if file.split('/')[7] == 'NCR':\n            if os.path.basename(file) == 'coc.json':\n                df = get_ncr_coc(file)\n                total = total.append(df)\n    total.rename(columns={'region': 'region_raw'}, inplace=True)\n    total['region'] = total['region_raw'].copy()\n    return total\n\n\n\nWith these utility functions inplace, the team can now apply these methods for easier access to the 2019 elections data.\nWe can now compile all of the election results with the following line:\n\ntree = '/mnt/data/public/elections/nle2019/results/*/*/*'\nncr_tree = '/mnt/data/public/elections/nle2019/results/*/*/*/*'\ndf_results = get_all_province_coc(tree)\ndf_results = df_results.append(get_all_ncr_coc(ncr_tree))\ndf_results.drop_duplicates(inplace=True)\n\n\ndf_results.head(5)\n\n\n\n\n\n\n\n\nvbc\ncc\nbo\nv\ntot\nper\nser\nregion_raw\nprovince\nregion\n\n\n\n\n0\n89550\n1.0\n1.0\n2004.0\n1708769.0\n0.11\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n1\n89550\n1.0\n2.0\n1607.0\n1708769.0\n0.09\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n2\n89550\n1.0\n3.0\n8772.0\n1708769.0\n0.51\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n3\n89550\n1.0\n4.0\n1767.0\n1708769.0\n0.10\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n4\n89550\n1.0\n5.0\n5068.0\n1708769.0\n0.29\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n\n\n\n\n\n\n\nNext, let us examine the obtained dataset with actual election results.\nBy cross checking the results with Comelec data, we can identify the senators and party names.\nJust to check our data, we can look at an example senator from the dataset. By choosing cc=1 and bo=46, we are actually highlighting Imee Marcos’ senatorial candidacy results.\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_marcos = df_results.query('cc == 1 & bo == 46').copy()\ndf_marcos.groupby('region').sum()['v'].sort_values(\n    ascending=True).plot.barh(figsize=(10, 10),\n                              title='Contestant: 46 - Imee Marcos',\n                              color='#BF5209', ax=ax);\n\n\n\n\nAdditionally, let us check some descriptive statistics for the 2019 Elections dataset. More specifically, let us examine the v or votes column. The group will be highly dependent on the votes data so let us first do some initial statistics and visualizations.\n\ndf_test = df_results.groupby(['region', 'province'])['v'].sum().to_frame()\ndf_test = df_test.rename(columns={'v': 'votes'})\ndf_test\n\n\n\n\n\n\n\n\n\nvotes\n\n\nregion\nprovince\n\n\n\n\n\nBARMM\nBASILAN\n2093067.0\n\n\nLANAO DEL SUR\n4770462.0\n\n\nMAGUINDANAO\n5917983.0\n\n\nSULU\n3529555.0\n\n\nTAWI-TAWI\n1874486.0\n\n\nCAR\nABRA\n1923481.0\n\n\nAPAYAO\n703002.0\n\n\nBENGUET\n2426397.0\n\n\nIFUGAO\n1408688.0\n\n\nKALINGA\n1621414.0\n\n\nMOUNTAIN PROVINCE\n1074249.0\n\n\nNCR\nNATIONAL CAPITAL REGION - FOURTH DISTRICT\n22896771.0\n\n\nNATIONAL CAPITAL REGION - MANILA\n13461229.0\n\n\nNATIONAL CAPITAL REGION - SECOND DISTRICT\n29803007.0\n\n\nNATIONAL CAPITAL REGION - THIRD DISTRICT\n18481014.0\n\n\nTAGUIG - PATEROS\n10018306.0\n\n\nNIR\nNEGROS OCCIDENTAL\n12453486.0\n\n\nNEGROS ORIENTAL\n6900077.0\n\n\nREGION I\nILOCOS NORTE\n3614806.0\n\n\nILOCOS SUR\n4882048.0\n\n\nLA UNION\n5764844.0\n\n\nPANGASINAN\n19803192.0\n\n\nREGION II\nBATANES\n118412.0\n\n\nCAGAYAN\n6890994.0\n\n\nISABELA\n9632919.0\n\n\nNUEVA VIZCAYA\n3200395.0\n\n\nQUIRINO\n1198929.0\n\n\nREGION III\nAURORA\n1493909.0\n\n\nBATAAN\n6433641.0\n\n\nBULACAN\n19567555.0\n\n\n...\n...\n...\n\n\nREGION VI\nGUIMARAS\n1217864.0\n\n\nILOILO\n11118379.0\n\n\nREGION VII\nBOHOL\n8977777.0\n\n\nCEBU\n19919139.0\n\n\nSIQUIJOR\n694813.0\n\n\nREGION VIII\nBILIRAN\n1051266.0\n\n\nEASTERN SAMAR\n3370187.0\n\n\nLEYTE\n9366390.0\n\n\nNORTHERN SAMAR\n3733282.0\n\n\nSAMAR (WESTERN SAMAR)\n5954800.0\n\n\nSOUTHERN LEYTE\n2558108.0\n\n\nREGION X\nBUKIDNON\n7415656.0\n\n\nCAMIGUIN\n768370.0\n\n\nLANAO DEL NORTE\n3584638.0\n\n\nMISAMIS OCCIDENTAL\n4391884.0\n\n\nMISAMIS ORIENTAL\n6488743.0\n\n\nREGION XI\nCOMPOSTELA VALLEY\n4281424.0\n\n\nDAVAO (DAVAO DEL NORTE)\n7056455.0\n\n\nDAVAO DEL SUR\n4457484.0\n\n\nDAVAO OCCIDENTAL\n1378314.0\n\n\nDAVAO ORIENTAL\n3200605.0\n\n\nREGION XII\nCOTABATO (NORTH COT.)\n7659458.0\n\n\nSARANGANI\n2990485.0\n\n\nSOUTH COTABATO\n9090438.0\n\n\nSULTAN KUDARAT\n4037874.0\n\n\nREGION XIII\nAGUSAN DEL NORTE\n4670054.0\n\n\nAGUSAN DEL SUR\n4155305.0\n\n\nDINAGAT ISLANDS\n735236.0\n\n\nSURIGAO DEL NORTE\n3979284.0\n\n\nSURIGAO DEL SUR\n4116338.0\n\n\n\n\n86 rows × 1 columns\n\n\n\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION III',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['NCR',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION IV-A',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION VII',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\nTo match the contestant ID to the contestant name, the contest files were also downloaded from the Commission of Elections’ 2019 National and Local Elections website and stored in the local repository. Similar to the results directory, the contest directory contained json files for each contest type/position. Upon inspection of a sample file within the directory, the following values were obtained:\nThe pertinent keys from each json files were:\n\n\n\nParameter\nDescription\n\n\n\n\ncc\nContest code\n\n\ncn\nContest code name - location\n\n\nccc\nContest code name\n\n\ntype\nContest type\n\n\nbos\nlist of candidate parameters\n\n\n\nUnder the bos key, we can extract each of the candidates’ parameters. The more useful ones for the group’s study include:\n\n\n\nParameter\nDescription\n\n\n\n\nboc\nContestant ID\n\n\nbon\nContestant Name\n\n\npn\nParty Name\n\n\n\n\n\n\n Step 2: Extract and collect the 2019 Elections (Contestant) data\nThe group also created utility functions for easier retrieval of the contestant datasets. This is to ensure each dataset is ready for aggregation.\nSimilar to the get_province_coc, the get_contestant_attrib method unpacks each key and value from the {contest_number}.json dictionary into a cleaned up dataframe. The method converts the bos directory into an additional list, which will also be appended into the resulting dataframe.\nThere are two (2) major political coalitions fighting for the senate seats: * Hugpong ng Pagbabago (HNP) * Otso Diretso\nSimilar to the get_all_province_coc, the get_contestants_attrib method is a walker that goes through each of the contest directory. The method will first append all {contest_numer}.json files into a singular dataframe. Next, the method creates a new column that identifies who among the senatorial candidates are part of the Hugpong ng Pagbabago (HNP) or Otso Diretso campaign.\n\ndef get_contestant_attrib(filepath):\n    \"\"\"\n    Returns the contestant json file into a dataframe\n\n    Parameters\n    ----------\n    filepath   : string\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestnat attributes\n\n    \"\"\"\n\n    contestants_values = []\n    with open(filepath, 'r') as file:\n        data = json.load(file)\n        attrib_keys = [key for key in list(data.keys())\n                       if isinstance(key, (str, float, int))]\n        attrib_values = [value for value in list(data.values())\n                         if isinstance(value, (str, float, int))]\n        contest_values = [list(contest.values()) for contest in data['bos']]\n        df = pd.DataFrame(contest_values,\n                          columns=list(data['bos'][0].keys()))\n        for k, v in zip(attrib_keys, attrib_values):\n            df[k] = v\n    return df\n\n\ndef get_contestants_attrib(filepath):\n    \"\"\"\n    Returns ALL contestant json files into a dataframe\n\n    Parameters\n    ----------\n    filepath   : string\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestant attributes\n\n    \"\"\"\n    df = pd.DataFrame()\n    for each_filepath in glob.glob(filepath):\n        df = df.append(get_contestant_attrib(each_filepath))\n    senators = df[df.cc == 1].copy()\n    senators['bon'] = senators['bon'].str.extract(pat='(.*?) \\(')\n    party = df[df.cc == 5567].copy()\n    df = senators.append(party)\n    df.drop_duplicates(inplace=True)\n    df.rename(columns={'boc': 'bo'}, inplace=True)\n    otso = ['AQUINO, BENIGNO BAM ', 'DIOKNO, CHEL', 'HILBAY, PILO',\n            'MACALINTAL, MACAROMY', 'GUTOC, SAMIRA', 'ALEJANO, GARY',\n            'ROXAS, MAR', 'TAÑADA,LORENZO ERIN TAPAT']\n    hnp = ['ANGARA, EDGARDO SONNY', 'BONG REVILLA, RAMON JR', 'CAYETANO, PIA',\n           'DELA ROSA, BATO', 'EJERCITO, ESTRADA JV', 'ESTRADA, JINGGOY',\n           'GO, BONG GO', 'MANGUDADATU, DONG', 'MANICAD, JIGGY',\n           'MARCOS, IMEE', 'PIMENTEL, KOKO', 'TOLENTINO, FRANCIS', \n           'VILLAR, CYNTHIA']\n    for o in otso:\n        df.loc[df.bon == o, 'coalition'] = \"Otso Diretso\"\n    for h in hnp:\n        df.loc[df.bon == h, 'coalition'] = \"HNP\"\n    df['coalition'] = df['coalition'].fillna('None')\n    return df\n\nLet us run the get_contestants_attrib. This will be used later in the blog for our further analysis.\n\ncontestant_filepaths = '/mnt/data/public/elections/nle2019/contests/*'\ndf_contestants = get_contestants_attrib(contestant_filepaths)\ndf_contestants.head()\n\nWe now have two dataframes: df_results containing the 2019 election results, and df_contestants containing the contestant information. These two dataframes can now be merged into a single dataframe. Let us also drop certain columns which we have deemed as unimportant.\n\ndef merge_comelec(results, contestants):\n    \"\"\"\n    Merge results dataframe with contestants dataframe\n\n    Parameters\n    ----------\n    results    : pd.DataFrame\n    contestants: pd.DataFrame\n\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestant attributes\n\n    \"\"\"\n\n    df = pd.merge(results, contestants, on=['bo', 'cc'], how='left')\n    df = df.drop(['vbc', 'boi', 'to', 'pc', 'pcc', 'pcy', 'pcm',\n                  'pck', 'ccc', 'pre', 'ser', 'cn'], axis=1)\n    df.columns = ['position', 'candidate_id', 'votes_per_province',\n                  'total_votes', 'votes_in_pct', 'region_raw', 'province',\n                  'region', 'candidate_name', 'party_name',\n                  'contest_position', 'contest_type', 'coalition']\n    return df\n\n\nnle2019 = merge_comelec(df_results, df_contestants)\nnle2019.region.unique()\n\n\n\n Step 3: Load Geopandas for geospatial processing \nThe Philippines is composed of seventeen (17) administrative regions. We can use the geopandas module to manage and pre-process geospatial data.\nLet us first load up a geopandas graph of the Philippines.\n\nfig, ax = plt.subplots(1, figsize=(10, 15), frameon=True)\n\nph0 = gpd.GeoDataFrame.from_file(\"ph_regions.shp\")\nph0.plot(ax=ax, cmap='Greens', edgecolor='#555555', k=18)\nax.set_title('Regions of the Philippines')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\nfor ind, row in ph0.iterrows():\n    ax.text(row[\"geometry\"].centroid.x, row[\"geometry\"].centroid.y,\n            row[\"region\"])"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#b.-national-senatorial-results",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#b.-national-senatorial-results",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " B. National Senatorial Results",
    "text": "B. National Senatorial Results\n\nLet us look at the senatorial candidates. Let us total up the votes by candidate and see the top 12 winners.\n\ndf_senators = nle2019[nle2019['contest_position'] == 'SENATOR']\ndf_senator = df_senators.groupby(['candidate_name']).agg(\n    {'votes_per_province': sum}).reset_index()\ndf_senator.sort_values('votes_per_province', ascending=False, inplace=True)\ndf_senator.columns = ['Candidate', 'Votes']\ndf_senator.head(12)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\nplt.rcParams.update({'font.size': 14})\ndf_senator.set_index('Candidate').head(12).sort_values(\n    by='Votes', ascending=True).plot.barh(ax=ax,\n    title='Top 12 Candidates, in Millions', color='#BF5209', legend=False);\nax.set_xlabel('Total Votes');"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#c.-top-senator-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#c.-top-senator-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " C. Top Senator Per Region",
    "text": "C. Top Senator Per Region\nWe want to find out won across all the regions. If there is any bias for cetain candidates. Based on our findings, we can see that candidate Cynthia Villar won majority of the regions.\nIt is interesting to note that the top ranking senator for Ilocos Region (Region I) and the Cordillera Administrative Region (CAR) is Imee Marcos, which hails from that region. This confirms that there is a “Solid North”, and that support for the Marcoses still exists in that area.\nFor the Mindanao regions, the top candidate is Bong Go, former special assistant to President Duterte, who is from Mindanao.\nThese show that Philippine politics is very regional in nature. Voters will naturally support their hometown candidate, regardless of the issues surrounding that candidate.\n\ndf_senator_region = df_senators.groupby(\n    ['region', 'candidate_name']).agg({'votes_per_province': sum})\ndf_senator_region['rank'] = df_senator_region.groupby(\n    'region')['votes_per_province'].rank('dense', ascending=False)\ndf_senator_region = df_senator_region[df_senator_region['rank'] == 1].reset_index()\ndf_senator_region.columns = ['Region', 'Candidate Name', 'Votes', 'Rank']\ndf_senator_region\n\n\n(df_senator_region[df_senator_region['Rank'] == 1].reset_index().groupby(\n    'Candidate Name')['Rank'].sum().to_frame().sort_values(by='Rank')\n .reset_index())\n\n\nfig, ax = plt.subplots()\n(df_senator_region[df_senator_region['Rank'] == 1].reset_index().groupby(\n    'Candidate Name')['Rank'].sum().to_frame().sort_values(by='Rank').plot\n .barh(color='#BF5209', ax=ax));\nax.set_xlabel('Rank');"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#d.-dominant-party-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#d.-dominant-party-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " D. Dominant Party per Region",
    "text": "D. Dominant Party per Region\nThere are three main political parties vying for the senatorial seats: 1. LIBERAL PARTY 2. NACIONALISTA PARTY 3. PARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN (PDP-LABAN)\nWe looked at the dominant or majority political party per administrative region, and identify if any regions have any affiliations to a certain party.\n\ndf_party = df_senators.groupby(['region_raw', 'party_name']).agg({\n    'votes_per_province': sum})\ndf_party['rank'] = df_party.groupby(\n    'region_raw')['votes_per_province'].rank('dense', ascending=False)\ndf_party.reset_index(inplace=True)\ndf_leading_party = df_party[df_party['rank'] == 1].copy()\ndf_leading_party.columns = ['Region', 'Party', 'Votes', 'Rank']\ndf_leading_party.sort_values(['Region', 'Rank'], inplace=True)\ndf_leading_party\n\nWe can clearly see a voting bias of each region.\nMajority of Region 1 and Region 2 has a voting preference towards the NACIONALISTA PARTY. This can be attributed to the fact that Imee Marcos, who hails from Ilocos Norte, is a Nacionalista.\nMajority of Region 5 and Region 6 has a voting preference towards the LIBERAL PARTY. This is also expected since Leni Robredo, incumbent Vice President who is from the Liberal Party, is a Bicolano.\nThe remainder of the Philippines has a voting preference towards the PDP-LABAN.\n\nmerged = ph0.merge(df_leading_party, left_on='region', right_on='Region')\ncolors = 18\ncolor_map = {'PARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN': 'red', \n             'LIBERAL PARTY': 'yellow',\n             'NACIONALISTA PARTY': 'green'}\n\nfig, ax = plt.subplots(1, 1, figsize = (10,12));\nax.set_title('Dominant political party per region', fontsize=20);\n\nfor party in list(merged['Party'].unique()):\n    color_map[party]\n    merged[merged['Party'] == party].plot(ax=ax, color = color_map[party], \n                                          categorical = True, \n                                          figsize=(10,12), legend=True)\n\nmerged.geometry.boundary.plot(color=None,edgecolor='k',linewidth = .5,ax=ax);\nplt.rcParams.update({'font.size': 18})"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#e.-dominant-coalition-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#e.-dominant-coalition-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " E. Dominant Coalition Per Region",
    "text": "E. Dominant Coalition Per Region\nAside from individual candidates and parties, we also looked at the dominant coalition per region by counting the number of senate seats obtained by each coalition. The results indicate that HNP gained a majority of the seats across all regions, especially in Mindanao.\nThe only regions where HNP did not gain a solid majority are in Bicol Region (Region V) and Eastern Visayas (Region VI), known bailwicks of the Liberal Party.\n\ndef get_coalition_seats():\n    \"\"\"\n    Returns a dataframe of the number of seats won per coalition per region\n\n    Returns\n    -------\n    coalition : pd.DataFrame\n\n    \"\"\"\n    coalition_seats = nle2019.query('position == 1')\n\n    coalition_seats = coalition_seats.groupby(\n        ['region', 'candidate_name', 'coalition'],\n        as_index=False).agg({'votes_per_province': sum})\n    coalition_seats = coalition_seats.sort_values(\n        by=['region', 'votes_per_province'], ascending=[1, 0])\n    coalition_seats.set_index('region', inplace=True)\n\n    coalition_seats['rank'] = list(range(1, len(\n        coalition_seats.candidate_name.unique())+1))\\\n        * len(coalition_seats.index.unique())\n    coalition_seats['is_top_12'] = 1*(coalition_seats['rank'] &lt;= 12)\n    coalition_seats = coalition_seats.reset_index()\n    coalition_seats = pd.pivot_table(\n        coalition_seats, index='region', columns='coalition',\n        values='is_top_12', aggfunc=np.sum)\n    coalition_seats['coalition_seats_total'] = coalition_seats.sum(axis=1)\n\n    for coalition in coalition_seats.columns[:-1]:\n        coalition_seats['party_seats_pct_' + coalition] = \\\n            coalition_seats[coalition] / \\\n            coalition_seats['coalition_seats_total']\n        coalition_seats.rename(\n            columns={coalition: 'coalition_seats_count_' + coalition},\n            inplace=True)\n\n    coalition_seats = coalition_seats.round(5)\n    coalition_seats = coalition_seats.reset_index()\n    return coalition_seats\n\n\ncoalition = get_coalition_seats()\nmerged = ph0.merge(coalition[[\n                   'region', 'coalition_seats_count_HNP']], left_on='region',\n                   right_on='region')\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 15))\nmerged.plot('coalition_seats_count_HNP', ax=ax, cmap='YlOrRd', legend=True)\nax.set_title('Number of HNP senate seats won', fontsize=24)\nmerged.geometry.boundary.plot(color=None, edgecolor='k', linewidth=1, ax=ax)"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#f.-is-demographics-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#f.-is-demographics-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " F. Is Demographics Related To Voting Preference?",
    "text": "F. Is Demographics Related To Voting Preference?\nIs the voting preference of a region related to its demographics such as literacy rate and religious affiliation? To answer this, we obtained the 2015 Census Data. The directory is a collection of excel files, where each excel file corresponds to a certain region and province. If we explore each file, we can see that each sheet corresponds to a different demographic feature table. For this analysis, we are intrested at sheets T8 and T11.\nThe get_census_religion loads the imporant columns and rows from sheet T8. It also adds an additional column based on the region. Similarly, the get_census_education loads the imporant columns and rows from sheet T11. It also aggregates each individual years experience column into a singular cumulative column.\nFinally, the read_census_files aggregates the 2016 regional data into singular dataframe by using get_census_religion and get_census_education functions. To extract only the regional files, the read_census_files uses regex to get filenames with only underscores in the beginning (this is an indicator of regional data).\n\ndef get_census_religion(path):\n    \"\"\"\n    Returns a consolidated DataFrame of census data by religion\n\n    Parameters\n    ----------\n    path      : string, filepath to census directory\n\n\n    Returns\n    ----------\n    df        : pd.DataFrame\n\n    \"\"\"\n    filename = os.path.basename(path)\n    df = pd.read_excel(path, sheet_name='T8', header=None,\n                       usecols=[0, 1, 2, 3],\n                       skiprows=6, skip_blank_lines=True,\n                       skipfooter=3,\n                       names=['religion', 'total', 'm', 'f'])\n    df.sort_values('total', ascending=False, inplace=True)\n    df['region'] = re.search(r'\\_(.*?)\\_', os.path.basename(path)).group(1)\n    cols = ['region', 'religion', 'm', 'f', 'total']\n    df = df[cols]\n    return df\n\n\ndef get_census_education(path):\n    \"\"\"\n    Returns a consolidated DataFrame of census data by education\n\n    Parameters\n    ----------\n    path      : string, filepath to census directory\n\n\n    Returns\n    ----------\n    df        : pd.DataFrame\n\n    \"\"\"\n    filename = os.path.basename(path)\n    df = pd.read_excel(path, sheet_name='T11',\n                       usecols=[0, 15, 16, 17, 18, 19, 20],\n                       skiprows=3,\n                       skip_blank_lines=True, nrows=20,\n                       names=['education', '18', '19',\n                              '20_24', '25_29', '30_34', '35_above'])\n    df.dropna(how='any', inplace=True)\n    df.reset_index(inplace=True, drop=True)\n    df.drop(df.index[[0, 5, 6, 7, 9, 10, 12, 13]], inplace=True)\n    df['total'] = (df['18'] + df['19'] + df['20_24'] + df['25_29'] +\n                   df['30_34'] + df['35_above'])\n    df['region'] = re.search(r'\\_(.*?)\\_', os.path.basename(path)).group(1)\n    cols = ['region', 'education', '18', '19', '20_24', '25_29', '30_34',\n            '35_above', 'total']\n    df = df[cols]\n    return df\n\n\ndef read_census_files(path):\n    '''\n    Reads all census regional files\n\n    Parameter\n    ---------\n    path      : string, filepath to census directory\n\n    Returns\n    -------\n    Dictionary of dataframes\n    '''\n    total = {'religion': pd.DataFrame(),\n             'education': pd.DataFrame()}\n\n    for filepath in glob.glob(path + \".xls\", recursive=True):\n        if re.match('_(?!PHILIPPINES)', os.path.basename(filepath)):\n            total['religion'] = (total['religion']\n                                 .append(get_census_religion(filepath)))\n            total['education'] = (total['education']\n                                  .append(get_census_education(filepath)))\n    total['religion'].reset_index(inplace=True, drop=True)\n    total['education'].reset_index(inplace=True, drop=True)\n\n    for df in total.values():\n        df.loc[(df['region'] == \"ARMM\"), 'region'] = 'BARMM'\n        df.loc[(df['region'] == \"MIMAROPA\"), 'region'] = 'REGION IV-B'\n        df.loc[(df['region'] == \"CARAGA\"), 'region'] = 'REGION XIII'\n    return total\n\n\ncensus_path = '/mnt/data/public/census/*'\ncensus_dict = read_census_files(census_path)"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#g.-is-literacy-rate-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#g.-is-literacy-rate-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " G. Is literacy rate related to voting preference?",
    "text": "G. Is literacy rate related to voting preference?\nWe looked at the number of seats obtained by each coalition per region, then correlated it to literacy rate. Education information was obtained from the 2015 Census data.\nWe looked at the literacy rate \\(\\frac{n_{educated}}{n_{total}}\\) of each administrative region:\n\ndef get_education_percent():\n    '''\n    Gets percentage of educational level per region\n    '''\n    education = census_dict['education'].groupby(\n        ['region', 'education'], as_index=False).sum()\n    education = pd.pivot_table(\n        education, index='region', columns='education', values='total')\n    education.columns = ['education_pct_' + educ for educ in education.columns]\n    education['education_total'] = education.sum(axis=1)\n    for educ in education.columns[:-1]:\n        education[educ] /= education['education_total']\n    education.drop('education_total', axis=1, inplace=True)\n    education = education.round(5)\n\n    return education\n\n\ndef get_agg_education_percent():\n    '''\n    Gets aggregated percentage of educational level per region\n    '''\n    df_educ = get_education_percent()\n    df_educ = df_educ.reset_index()\n    df_educ['educated'] = (1 - df_educ['education_pct_No Grade Completed']\n                           - df_educ['education_pct_Not Stated'])\n    df_educ['not_educated'] = df_educ['education_pct_No Grade Completed']\n    df_educ['unknown'] = df_educ['education_pct_Not Stated']\n    df_educ.drop(columns=['education_pct_Academic Degree Holder',\n                          'education_pct_College Undergraduate',\n                          'education_pct_Elementary',\n                          'education_pct_High School',\n                          'education_pct_No Grade Completed',\n                          'education_pct_Not Stated',\n                          'education_pct_Post Baccalaureate',\n                          'education_pct_Post-Secondary',\n                          'education_pct_Pre-School',\n                          'education_pct_Special Education'], inplace=True)\n    df_educ.set_index('region', inplace=True)\n    df_educ.reset_index(inplace=True)\n    return df_educ\n\n\ndf_educ = get_agg_education_percent()\ndf_educ\n\nWe then checked if the number of seats obtained by each coalition is correlated to the literacy rate of that region. First, we obtained the number of seats obtained by each coalition per region:\n\ndef get_coalition_rank():\n    \"\"\"\n    Get number of seats obtained by each coalition\n    \"\"\"\n    coalition_seats = nle2019.query('position == 1')\n    coalition_seats = coalition_seats.groupby(\n        ['region', 'candidate_name', 'coalition'], as_index=False).agg({'votes_per_province': sum})\n    coalition_seats = coalition_seats.sort_values(\n        by=['region', 'votes_per_province'], ascending=[1, 0])\n    coalition_seats.set_index('region', inplace=True)\n\n    coalition_seats['rank'] = list(range(1, len(\n        coalition_seats.candidate_name.unique())+1)) * len(coalition_seats.index.unique())\n    coalition_seats['is_top_12'] = 1*(coalition_seats['rank'] &lt;= 12)\n    coalition_seats.reset_index(inplace=True)\n\n    coalition_seats = pd.pivot_table(\n        coalition_seats, index='region', columns='coalition', values='is_top_12', aggfunc=np.sum)\n    return coalition_seats\n\nWe then merge this with the education dataframe, then get the correlation:\n\ndf_coal_educ = get_agg_education_percent().set_index('region').join(get_coalition_rank())\ndf_coal_educ['Educated'] = df_coal_educ['educated']\ndf_coal_educ['Not Educated'] = df_coal_educ['not_educated']\ncorr = df_coal_educ.corr().loc[['Otso Diretso', 'HNP'],['Educated', 'Not Educated']]\ncolormap = sns.diverging_palette(100, 100, n = 10)\nsns.heatmap(corr, cmap=colormap, annot=True, vmin = -1, vmax = 1);\n\nThe Census and election data show that the voting preference of a region has no correlation with its literacy rate. We now look at religion to see if it has a correlation with the voting preference."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#h.-is-religion-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#h.-is-religion-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " H. Is religion related to voting preference?",
    "text": "H. Is religion related to voting preference?\nWe also looked into the religious affiliation per region, and checked if it is correlatd with voting preference.\nFirst, we obtained the distribution of religions per region from the Census data:\n\ndef get_religion_percent():\n    \"\"\"\n    Get percentages of religion per region\n    \"\"\"\n    religion = census_dict['religion'].groupby(\n        ['region', 'religion'], as_index=False).sum()\n    religion = pd.pivot_table(\n        religion, index='region', columns='religion', values='total')\n    religion.columns = ['religion_pct_' + rel for rel in religion.columns]\n    religion['religion_total'] = religion.sum(axis=1)\n    for rel in religion.columns[:-1]:\n        religion[rel] /= religion['religion_total']\n    religion.drop('religion_total', axis=1, inplace=True)\n    religion = religion.round(5)\n    return religion\n\n\ndf_rel = get_religion_percent()\n\nWe then merged the religion census data with the coalition ranking data to check if religion has correlation with the number of seats obtained by each coalition:\n\ndf_coal_rel = get_religion_percent().join(get_coalition_rank())\ndf_coal_rel.head()\n\nFrom getting the correlation of the religion data with the number of seats per coalition, it is apparent that the Voting preference of a region has no correlation with its religious affiliation.\n\ndf_coal_rel['Roman Catholic'] = df_coal_rel['religion_pct_Roman Catholic, including Catholic Charismatic']\ndf_coal_rel['Islam'] = df_coal_rel['religion_pct_Islam']\ndf_coal_rel['Iglesia ni Cristo'] = df_coal_rel['religion_pct_Iglesia ni Cristo']\ncorr = df_coal_rel.corr().loc[\n    ['Roman Catholic','Islam','Iglesia ni Cristo'],['HNP','Otso Diretso']]\ncolormap = sns.diverging_palette(100, 100, n = 10)\nsns.heatmap(corr, cmap=colormap, annot=True, vmin = -1, vmax = 1);"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#i.-conclusion",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#i.-conclusion",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " I. Conclusion",
    "text": "I. Conclusion\nUpon checking both the Comelec 2019 Election Results and the 2015 Philippine Census data, we found out that voting preference is characterized by high regionality. Candidates have a homecourt advantage, and voters tend to vote candidates or parties affiliated with their home region.\nAlso, literacy rate and religious affiliation is not correlated to voting preference."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#acknowledgements",
    "href": "posts/20230414_Philippine_Voting_Preferences/DMW_Lab01_20190529_2.html#acknowledgements",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Prof Christian Alis and the ACCeSS Laboratory for the access to the high-performance computing facility."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#a.-methodology",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#a.-methodology",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " A. Methodology",
    "text": "A. Methodology\n\n Step 1: Extract and collect the 2019 Elections (Results) data\nThe first directory to explore is the 2019 Election results. The results directory contains all electoral results from a regional level down to the barangay level. For each level, a specific coc.json file is stored. This file contains all electoral results data and metadata for both national and local elections. However for the purposes of this analysis, we will only look at the aggregated elections data at the regional level. The files that we are interested are the coc.json files associated to each province, as these files contain the metadata and data on the election results.\nThe main structure of each coc.json file contains the following main keys: vbc, rs, sts, obs, and cos. For the purpose of this exercise, the important key the group needs to extract is the rs key as this provides the each candidate’s total votes per area. Under the rs key, the following keys can be found: cc, bo, v, tot, per, and ser. Cross referencing these keys with official statements and comelec documentations suggests that important keys are as follows: cc pertaining to contest_type, bo pertaining to the candidate_id, and v pertaining to votes_per_province.\n\n\n\nParameter\nDescription\n\n\n\n\ncc\nContest code\n\n\nbo\nContestant id\n\n\nv\nTotal votes per contestant\n\n\ntot\nTotal votes per province\n\n\n\nHowever, it must be pointed out that the available data only goes as high as provincial data. If we want to process the provincial level, the team will have to aggregate the data up.\nThe group created utility functions for easier retrieval of the provincial elections datasets. The purpose for the utility functions (and future utility functions) are for initial cleaning and manipulations. This is to ensure each dataset is ready for aggregation.\nThe get_province_coc method unpacks each key and value from the coc.json dictionary into a cleaned up dataframe. In addition, the method identifies which region and province the file originated from by examining the filepath that was passed.\nThe get_all_province_coc method is a walker that goes through each of the results directory. The walker checks if the filename has an equal value to coc.json. If a coc.json was located, the get_province_coc method is applied with the filepath as the parameter. The resulting dataframe is then appended to a master dataframe for further extraction and analysis. For this exercise, the group only had to extract data up to the regional and provincial levels so only three wildcard were use for the glob walker.\nSpecial methods (get_ncr_coc and get_all_ncr_coc) were established to get the cities’ coc.json. For the case of the NCR cities, theire associated coc.json files were one directory lower.\n\n\nWith these utility functions inplace, the team can now apply these methods for easier access to the 2019 elections data.\nWe can now compile all of the election results with the following line:\n\n\n\n\n\n\n\n\n\nvbc\ncc\nbo\nv\ntot\nper\nser\nregion_raw\nprovince\nregion\n\n\n\n\n0\n89550\n1.0\n1.0\n2004.0\n1708769.0\n0.11\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n1\n89550\n1.0\n2.0\n1607.0\n1708769.0\n0.09\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n2\n89550\n1.0\n3.0\n8772.0\n1708769.0\n0.51\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n3\n89550\n1.0\n4.0\n1767.0\n1708769.0\n0.10\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n4\n89550\n1.0\n5.0\n5068.0\n1708769.0\n0.29\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n\n\n\n\n\n\n\nNext, let us examine the obtained dataset with actual election results.\nBy cross checking the results with Comelec data, we can identify the senators and party names.\nJust to check our data, we can look at an example senator from the dataset. By choosing cc=1 and bo=46, we are actually highlighting Imee Marcos’ senatorial candidacy results.\n\n\n\n\n\nAdditionally, let us check some descriptive statistics for the 2019 Elections dataset. More specifically, let us examine the v or votes column. The group will be highly dependent on the votes data so let us first do some initial statistics and visualizations.\n\n\n\n\n\n\n\n\n\n\nvotes\n\n\nregion\nprovince\n\n\n\n\n\nBARMM\nBASILAN\n2093067.0\n\n\nLANAO DEL SUR\n4770462.0\n\n\nMAGUINDANAO\n5917983.0\n\n\nSULU\n3529555.0\n\n\nTAWI-TAWI\n1874486.0\n\n\nCAR\nABRA\n1923481.0\n\n\nAPAYAO\n703002.0\n\n\nBENGUET\n2426397.0\n\n\nIFUGAO\n1408688.0\n\n\nKALINGA\n1621414.0\n\n\nMOUNTAIN PROVINCE\n1074249.0\n\n\nNCR\nNATIONAL CAPITAL REGION - FOURTH DISTRICT\n22896771.0\n\n\nNATIONAL CAPITAL REGION - MANILA\n13461229.0\n\n\nNATIONAL CAPITAL REGION - SECOND DISTRICT\n29803007.0\n\n\nNATIONAL CAPITAL REGION - THIRD DISTRICT\n18481014.0\n\n\nTAGUIG - PATEROS\n10018306.0\n\n\nNIR\nNEGROS OCCIDENTAL\n12453486.0\n\n\nNEGROS ORIENTAL\n6900077.0\n\n\nREGION I\nILOCOS NORTE\n3614806.0\n\n\nILOCOS SUR\n4882048.0\n\n\nLA UNION\n5764844.0\n\n\nPANGASINAN\n19803192.0\n\n\nREGION II\nBATANES\n118412.0\n\n\nCAGAYAN\n6890994.0\n\n\nISABELA\n9632919.0\n\n\nNUEVA VIZCAYA\n3200395.0\n\n\nQUIRINO\n1198929.0\n\n\nREGION III\nAURORA\n1493909.0\n\n\nBATAAN\n6433641.0\n\n\nBULACAN\n19567555.0\n\n\n...\n...\n...\n\n\nREGION VI\nGUIMARAS\n1217864.0\n\n\nILOILO\n11118379.0\n\n\nREGION VII\nBOHOL\n8977777.0\n\n\nCEBU\n19919139.0\n\n\nSIQUIJOR\n694813.0\n\n\nREGION VIII\nBILIRAN\n1051266.0\n\n\nEASTERN SAMAR\n3370187.0\n\n\nLEYTE\n9366390.0\n\n\nNORTHERN SAMAR\n3733282.0\n\n\nSAMAR (WESTERN SAMAR)\n5954800.0\n\n\nSOUTHERN LEYTE\n2558108.0\n\n\nREGION X\nBUKIDNON\n7415656.0\n\n\nCAMIGUIN\n768370.0\n\n\nLANAO DEL NORTE\n3584638.0\n\n\nMISAMIS OCCIDENTAL\n4391884.0\n\n\nMISAMIS ORIENTAL\n6488743.0\n\n\nREGION XI\nCOMPOSTELA VALLEY\n4281424.0\n\n\nDAVAO (DAVAO DEL NORTE)\n7056455.0\n\n\nDAVAO DEL SUR\n4457484.0\n\n\nDAVAO OCCIDENTAL\n1378314.0\n\n\nDAVAO ORIENTAL\n3200605.0\n\n\nREGION XII\nCOTABATO (NORTH COT.)\n7659458.0\n\n\nSARANGANI\n2990485.0\n\n\nSOUTH COTABATO\n9090438.0\n\n\nSULTAN KUDARAT\n4037874.0\n\n\nREGION XIII\nAGUSAN DEL NORTE\n4670054.0\n\n\nAGUSAN DEL SUR\n4155305.0\n\n\nDINAGAT ISLANDS\n735236.0\n\n\nSURIGAO DEL NORTE\n3979284.0\n\n\nSURIGAO DEL SUR\n4116338.0\n\n\n\n\n86 rows × 1 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo match the contestant ID to the contestant name, the contest files were also downloaded from the Commission of Elections’ 2019 National and Local Elections website and stored in the local repository. Similar to the results directory, the contest directory contained json files for each contest type/position. Upon inspection of a sample file within the directory, the following values were obtained:\nThe pertinent keys from each json files were:\n\n\n\nParameter\nDescription\n\n\n\n\ncc\nContest code\n\n\ncn\nContest code name - location\n\n\nccc\nContest code name\n\n\ntype\nContest type\n\n\nbos\nlist of candidate parameters\n\n\n\nUnder the bos key, we can extract each of the candidates’ parameters. The more useful ones for the group’s study include:\n\n\n\nParameter\nDescription\n\n\n\n\nboc\nContestant ID\n\n\nbon\nContestant Name\n\n\npn\nParty Name\n\n\n\n\n\n\n Step 2: Extract and collect the 2019 Elections (Contestant) data\nThe group also created utility functions for easier retrieval of the contestant datasets. This is to ensure each dataset is ready for aggregation.\nSimilar to the get_province_coc, the get_contestant_attrib method unpacks each key and value from the {contest_number}.json dictionary into a cleaned up dataframe. The method converts the bos directory into an additional list, which will also be appended into the resulting dataframe.\nThere are two (2) major political coalitions fighting for the senate seats: * Hugpong ng Pagbabago (HNP) * Otso Diretso\nSimilar to the get_all_province_coc, the get_contestants_attrib method is a walker that goes through each of the contest directory. The method will first append all {contest_numer}.json files into a singular dataframe. Next, the method creates a new column that identifies who among the senatorial candidates are part of the Hugpong ng Pagbabago (HNP) or Otso Diretso campaign.\nLet us run the get_contestants_attrib. This will be used later in the blog for our further analysis.\nWe now have two dataframes: df_results containing the 2019 election results, and df_contestants containing the contestant information. These two dataframes can now be merged into a single dataframe. Let us also drop certain columns which we have deemed as unimportant.\n\n\n Step 3: Load Geopandas for geospatial processing \nThe Philippines is composed of seventeen (17) administrative regions. We can use the geopandas module to manage and pre-process geospatial data.\nLet us first load up a geopandas graph of the Philippines."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#c.-top-senator-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#c.-top-senator-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " C. Top Senator Per Region",
    "text": "C. Top Senator Per Region\nWe want to find out won across all the regions. If there is any bias for cetain candidates. Based on our findings, we can see that candidate Cynthia Villar won majority of the regions.\nIt is interesting to note that the top ranking senator for Ilocos Region (Region I) and the Cordillera Administrative Region (CAR) is Imee Marcos, which hails from that region. This confirms that there is a “Solid North”, and that support for the Marcoses still exists in that area.\nFor the Mindanao regions, the top candidate is Bong Go, former special assistant to President Duterte, who is from Mindanao.\nThese show that Philippine politics is very regional in nature. Voters will naturally support their hometown candidate, regardless of the issues surrounding that candidate."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#d.-dominant-party-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#d.-dominant-party-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " D. Dominant Party per Region",
    "text": "D. Dominant Party per Region\nThere are three main political parties vying for the senatorial seats: 1. LIBERAL PARTY 2. NACIONALISTA PARTY 3. PARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN (PDP-LABAN)\nWe looked at the dominant or majority political party per administrative region, and identify if any regions have any affiliations to a certain party.\nWe can clearly see a voting bias of each region.\nMajority of Region 1 and Region 2 has a voting preference towards the NACIONALISTA PARTY. This can be attributed to the fact that Imee Marcos, who hails from Ilocos Norte, is a Nacionalista.\nMajority of Region 5 and Region 6 has a voting preference towards the LIBERAL PARTY. This is also expected since Leni Robredo, incumbent Vice President who is from the Liberal Party, is a Bicolano.\nThe remainder of the Philippines has a voting preference towards the PDP-LABAN."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#e.-dominant-coalition-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#e.-dominant-coalition-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " E. Dominant Coalition Per Region",
    "text": "E. Dominant Coalition Per Region\nAside from individual candidates and parties, we also looked at the dominant coalition per region by counting the number of senate seats obtained by each coalition. The results indicate that HNP gained a majority of the seats across all regions, especially in Mindanao.\nThe only regions where HNP did not gain a solid majority are in Bicol Region (Region V) and Eastern Visayas (Region VI), known bailwicks of the Liberal Party."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#f.-is-demographics-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#f.-is-demographics-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " F. Is Demographics Related To Voting Preference?",
    "text": "F. Is Demographics Related To Voting Preference?\nIs the voting preference of a region related to its demographics such as literacy rate and religious affiliation? To answer this, we obtained the 2015 Census Data. The directory is a collection of excel files, where each excel file corresponds to a certain region and province. If we explore each file, we can see that each sheet corresponds to a different demographic feature table. For this analysis, we are intrested at sheets T8 and T11.\nThe get_census_religion loads the imporant columns and rows from sheet T8. It also adds an additional column based on the region. Similarly, the get_census_education loads the imporant columns and rows from sheet T11. It also aggregates each individual years experience column into a singular cumulative column.\nFinally, the read_census_files aggregates the 2016 regional data into singular dataframe by using get_census_religion and get_census_education functions. To extract only the regional files, the read_census_files uses regex to get filenames with only underscores in the beginning (this is an indicator of regional data)."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#g.-is-literacy-rate-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#g.-is-literacy-rate-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " G. Is literacy rate related to voting preference?",
    "text": "G. Is literacy rate related to voting preference?\nWe looked at the number of seats obtained by each coalition per region, then correlated it to literacy rate. Education information was obtained from the 2015 Census data.\nWe looked at the literacy rate \\(\\frac{n_{educated}}{n_{total}}\\) of each administrative region:\nWe then checked if the number of seats obtained by each coalition is correlated to the literacy rate of that region. First, we obtained the number of seats obtained by each coalition per region:\nWe then merge this with the education dataframe, then get the correlation:\nThe Census and election data show that the voting preference of a region has no correlation with its literacy rate. We now look at religion to see if it has a correlation with the voting preference."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#h.-is-religion-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#h.-is-religion-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " H. Is religion related to voting preference?",
    "text": "H. Is religion related to voting preference?\nWe also looked into the religious affiliation per region, and checked if it is correlatd with voting preference.\nFirst, we obtained the distribution of religions per region from the Census data:\nWe then merged the religion census data with the coalition ranking data to check if religion has correlation with the number of seats obtained by each coalition:\nFrom getting the correlation of the religion data with the number of seats per coalition, it is apparent that the Voting preference of a region has no correlation with its religious affiliation."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#i.-conclusion",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#i.-conclusion",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " I. Conclusion",
    "text": "I. Conclusion\nUpon checking both the Comelec 2019 Election Results and the 2015 Philippine Census data, we found out that voting preference is characterized by high regionality. Candidates have a homecourt advantage, and voters tend to vote candidates or parties affiliated with their home region.\nAlso, literacy rate and religious affiliation is not correlated to voting preference."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#acknowledgements",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#acknowledgements",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Prof Christian Alis and the ACCeSS Laboratory for the access to the high-performance computing facility."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index.html#b.-national-senatorial-results",
    "href": "posts/20230414_Philippine_Voting_Preferences/index.html#b.-national-senatorial-results",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " B. National Senatorial Results",
    "text": "B. National Senatorial Results\n\nLet us look at the senatorial candidates. Let us total up the votes by candidate and see the top 12 winners."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": "",
    "text": "The Philippines recently concluded its 2019 midterm elections last May 13. Sixty-two (62) candidates from various political parties contested for twelve (12) seats in the Senate of The Philippines. Given the unexpected results, the team was wondering on the following topics:\nfrom IPython.display import HTML\n\nHTML('''&lt;script&gt;\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n\n  $( document ).ready(function(){\n    code_shown=false;\n    $('div.input').hide()\n  });\n&lt;/script&gt;\n&lt;form action=\"javascript:code_toggle()\"&gt;&lt;input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"&gt;&lt;/form&gt;''')\nBefore anything else, let us first load all important modules for this exercise.\nimport os\nimport io\nimport re\nimport time\nimport glob\nimport requests\nimport urllib.request\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\nIt is important to identify the datasets we are going to use for this exercise. The two identified datasets the group intends to use are: the 2019 National Data and the 2015-2016 Census data.\nWith regards to 2019 National data, the team used a web scapper provided Prof. Alis. The web scapper downloaded the election results from the Commission of Elections’ 2019 National and Local Elections website. The results were then stored in a local repository which is then easily accesible for the team. The 2019 elections results are broken down into two main directories: results and contest. In this exercise, the team will explore both directories to map out a comprehensive summary of the 2019 senatorial and party elections.\nSecondly, the 2015-2016 Census data has already been stored in a local repository for easier access. One of the main reasons why the team decided to use the 2015-2016 Census data is because of the lack of availability of recent data. The Philippine Statistics Authority only releases a comprehensive census survey ever six years. However for the purpose of this exercise, the team has agreed that the 2015-2016 census data can act as an appproximate for today’s population."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#a.-methodology",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#a.-methodology",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " A. Methodology",
    "text": "A. Methodology\n\n Step 1: Extract and collect the 2019 Elections (Results) data\nThe first directory to explore is the 2019 Election results. The results directory contains all electoral results from a regional level down to the barangay level. For each level, a specific coc.json file is stored. This file contains all electoral results data and metadata for both national and local elections. However for the purposes of this analysis, we will only look at the aggregated elections data at the regional level. The files that we are interested are the coc.json files associated to each province, as these files contain the metadata and data on the election results.\nThe main structure of each coc.json file contains the following main keys: vbc, rs, sts, obs, and cos. For the purpose of this exercise, the important key the group needs to extract is the rs key as this provides the each candidate’s total votes per area. Under the rs key, the following keys can be found: cc, bo, v, tot, per, and ser. Cross referencing these keys with official statements and comelec documentations suggests that important keys are as follows: cc pertaining to contest_type, bo pertaining to the candidate_id, and v pertaining to votes_per_province.\n\n\n\nParameter\nDescription\n\n\n\n\ncc\nContest code\n\n\nbo\nContestant id\n\n\nv\nTotal votes per contestant\n\n\ntot\nTotal votes per province\n\n\n\nHowever, it must be pointed out that the available data only goes as high as provincial data. If we want to process the provincial level, the team will have to aggregate the data up.\nThe group created utility functions for easier retrieval of the provincial elections datasets. The purpose for the utility functions (and future utility functions) are for initial cleaning and manipulations. This is to ensure each dataset is ready for aggregation.\nThe get_province_coc method unpacks each key and value from the coc.json dictionary into a cleaned up dataframe. In addition, the method identifies which region and province the file originated from by examining the filepath that was passed.\nThe get_all_province_coc method is a walker that goes through each of the results directory. The walker checks if the filename has an equal value to coc.json. If a coc.json was located, the get_province_coc method is applied with the filepath as the parameter. The resulting dataframe is then appended to a master dataframe for further extraction and analysis. For this exercise, the group only had to extract data up to the regional and provincial levels so only three wildcard were use for the glob walker.\nSpecial methods (get_ncr_coc and get_all_ncr_coc) were established to get the cities’ coc.json. For the case of the NCR cities, theire associated coc.json files were one directory lower.\n\ndef get_province_coc(filepath):\n    \"\"\"\n    Loads a single coc file. \n\n    Adds additional columns `region` and `province to the DataFrame,\n    depending on filepath.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    output = []\n    with open(filepath, 'r') as f:\n        dirpath, filepath = os.path.split(filepath)\n        region = dirpath.split('/')[-2]\n        province = dirpath.split('/')[-1]\n        data = json.load(f)\n        for each in data['rs']:\n            row = [float(element) for element in list(each.values())]\n            output.append([data['vbc']] + row + [region] + [province])\n    df = pd.DataFrame(output,\n                      columns=['vbc', 'cc', 'bo', 'v', 'tot', 'per', 'ser',\n                               'region', 'province'])\n    return df\n\n\ndef get_all_province_coc(tree):\n    \"\"\"\n    Loads all province COC files and saves them to a dataframe\n\n    Checks the filepath if filename is 'coc.json'\n\n    Created a new column to deal with the reclassification of\n        \"NEGROS ORIENTAL\" and \"NEGROS OCCIDENTAL\" to \"NIR\" \n            to match the PSA 2016 dataset.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    total = pd.DataFrame()\n    for file in glob.glob(tree):\n        if os.path.basename(file) == 'coc.json':\n            df = get_province_coc(file)\n            total = total.append(df)\n    total.rename(columns={'region': 'region_raw'}, inplace=True)\n    total['region'] = total['region_raw'].copy()\n    total.loc[(total['province'] == \"NEGROS ORIENTAL\") |\n              (total['province'] == \"NEGROS OCCIDENTAL\"), 'region'] = 'NIR'\n    return total\n\n\ndef get_ncr_coc(filepath):\n    \"\"\"\n    Loads a single coc file. \n\n    Adds additional columns `region` and `province to the DataFrame,\n    depending on filepath.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe    \n\n    \"\"\"\n    output = []\n    with open(filepath, 'r') as f:\n        dirpath, filepath = os.path.split(filepath)\n        region = dirpath.split('/')[-3]\n        province = dirpath.split('/')[-2]\n        data = json.load(f)\n        for each in data['rs']:\n            row = [float(element) for element in list(each.values())]\n            output.append([data['vbc']] + row + [region] + [province])\n    df = pd.DataFrame(output,\n                      columns=['vbc', 'cc', 'bo', 'v', 'tot', 'per', 'ser',\n                               'region', 'province'])\n    return df\n\n\ndef get_all_ncr_coc(tree):\n    \"\"\"\n    Loads all province COC files and saves them to a dataframe\n\n    Checks the filepath if filename is 'coc.json'\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    total = pd.DataFrame()\n    for file in glob.glob(tree):\n        if file.split('/')[7] == 'NCR':\n            if os.path.basename(file) == 'coc.json':\n                df = get_ncr_coc(file)\n                total = total.append(df)\n    total.rename(columns={'region': 'region_raw'}, inplace=True)\n    total['region'] = total['region_raw'].copy()\n    return total\n\n\n\nWith these utility functions inplace, the team can now apply these methods for easier access to the 2019 elections data.\nWe can now compile all of the election results with the following line:\n\ntree = '/mnt/data/public/elections/nle2019/results/*/*/*'\nncr_tree = '/mnt/data/public/elections/nle2019/results/*/*/*/*'\ndf_results = get_all_province_coc(tree)\ndf_results = df_results.append(get_all_ncr_coc(ncr_tree))\ndf_results.drop_duplicates(inplace=True)\n\n\ndf_results.head(5)\n\n\n\n\n\n\n\n\nvbc\ncc\nbo\nv\ntot\nper\nser\nregion_raw\nprovince\nregion\n\n\n\n\n0\n89550\n1.0\n1.0\n2004.0\n1708769.0\n0.11\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n1\n89550\n1.0\n2.0\n1607.0\n1708769.0\n0.09\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n2\n89550\n1.0\n3.0\n8772.0\n1708769.0\n0.51\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n3\n89550\n1.0\n4.0\n1767.0\n1708769.0\n0.10\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n4\n89550\n1.0\n5.0\n5068.0\n1708769.0\n0.29\n2800.0\nREGION I\nILOCOS NORTE\nREGION I\n\n\n\n\n\n\n\n\n\nNext, let us examine the obtained dataset with actual election results.\nBy cross checking the results with Comelec data, we can identify the senators and party names.\nJust to check our data, we can look at an example senator from the dataset. By choosing cc=1 and bo=46, we are actually highlighting Imee Marcos’ senatorial candidacy results.\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_marcos = df_results.query('cc == 1 & bo == 46').copy()\ndf_marcos.groupby('region').sum()['v'].sort_values(\n    ascending=True).plot.barh(figsize=(10, 10),\n                              title='Contestant: 46 - Imee Marcos',\n                              color='#BF5209', ax=ax);\n\n\n\n\nAdditionally, let us check some descriptive statistics for the 2019 Elections dataset. More specifically, let us examine the v or votes column. The group will be highly dependent on the votes data so let us first do some initial statistics and visualizations.\n\ndf_test = df_results.groupby(['region', 'province'])['v'].sum().to_frame()\ndf_test = df_test.rename(columns={'v': 'votes'})\ndf_test\n\n\n\n\n\n\n\n\n\nvotes\n\n\nregion\nprovince\n\n\n\n\n\nBARMM\nBASILAN\n2093067.0\n\n\nLANAO DEL SUR\n4770462.0\n\n\nMAGUINDANAO\n5917983.0\n\n\nSULU\n3529555.0\n\n\nTAWI-TAWI\n1874486.0\n\n\nCAR\nABRA\n1923481.0\n\n\nAPAYAO\n703002.0\n\n\nBENGUET\n2426397.0\n\n\nIFUGAO\n1408688.0\n\n\nKALINGA\n1621414.0\n\n\nMOUNTAIN PROVINCE\n1074249.0\n\n\nNCR\nNATIONAL CAPITAL REGION - FOURTH DISTRICT\n22896771.0\n\n\nNATIONAL CAPITAL REGION - MANILA\n13461229.0\n\n\nNATIONAL CAPITAL REGION - SECOND DISTRICT\n29803007.0\n\n\nNATIONAL CAPITAL REGION - THIRD DISTRICT\n18481014.0\n\n\nTAGUIG - PATEROS\n10018306.0\n\n\nNIR\nNEGROS OCCIDENTAL\n12453486.0\n\n\nNEGROS ORIENTAL\n6900077.0\n\n\nREGION I\nILOCOS NORTE\n3614806.0\n\n\nILOCOS SUR\n4882048.0\n\n\nLA UNION\n5764844.0\n\n\nPANGASINAN\n19803192.0\n\n\nREGION II\nBATANES\n118412.0\n\n\nCAGAYAN\n6890994.0\n\n\nISABELA\n9632919.0\n\n\nNUEVA VIZCAYA\n3200395.0\n\n\nQUIRINO\n1198929.0\n\n\nREGION III\nAURORA\n1493909.0\n\n\nBATAAN\n6433641.0\n\n\nBULACAN\n19567555.0\n\n\n...\n...\n...\n\n\nREGION VI\nGUIMARAS\n1217864.0\n\n\nILOILO\n11118379.0\n\n\nREGION VII\nBOHOL\n8977777.0\n\n\nCEBU\n19919139.0\n\n\nSIQUIJOR\n694813.0\n\n\nREGION VIII\nBILIRAN\n1051266.0\n\n\nEASTERN SAMAR\n3370187.0\n\n\nLEYTE\n9366390.0\n\n\nNORTHERN SAMAR\n3733282.0\n\n\nSAMAR (WESTERN SAMAR)\n5954800.0\n\n\nSOUTHERN LEYTE\n2558108.0\n\n\nREGION X\nBUKIDNON\n7415656.0\n\n\nCAMIGUIN\n768370.0\n\n\nLANAO DEL NORTE\n3584638.0\n\n\nMISAMIS OCCIDENTAL\n4391884.0\n\n\nMISAMIS ORIENTAL\n6488743.0\n\n\nREGION XI\nCOMPOSTELA VALLEY\n4281424.0\n\n\nDAVAO (DAVAO DEL NORTE)\n7056455.0\n\n\nDAVAO DEL SUR\n4457484.0\n\n\nDAVAO OCCIDENTAL\n1378314.0\n\n\nDAVAO ORIENTAL\n3200605.0\n\n\nREGION XII\nCOTABATO (NORTH COT.)\n7659458.0\n\n\nSARANGANI\n2990485.0\n\n\nSOUTH COTABATO\n9090438.0\n\n\nSULTAN KUDARAT\n4037874.0\n\n\nREGION XIII\nAGUSAN DEL NORTE\n4670054.0\n\n\nAGUSAN DEL SUR\n4155305.0\n\n\nDINAGAT ISLANDS\n735236.0\n\n\nSURIGAO DEL NORTE\n3979284.0\n\n\nSURIGAO DEL SUR\n4116338.0\n\n\n\n\n86 rows × 1 columns\n\n\n\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION III',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['NCR',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION IV-A',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION VII',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\nTo match the contestant ID to the contestant name, the contest files were also downloaded from the Commission of Elections’ 2019 National and Local Elections website and stored in the local repository. Similar to the results directory, the contest directory contained json files for each contest type/position. Upon inspection of a sample file within the directory, the following values were obtained:\nThe pertinent keys from each json files were:\n\n\n\nParameter\nDescription\n\n\n\n\ncc\nContest code\n\n\ncn\nContest code name - location\n\n\nccc\nContest code name\n\n\ntype\nContest type\n\n\nbos\nlist of candidate parameters\n\n\n\nUnder the bos key, we can extract each of the candidates’ parameters. The more useful ones for the group’s study include:\n\n\n\nParameter\nDescription\n\n\n\n\nboc\nContestant ID\n\n\nbon\nContestant Name\n\n\npn\nParty Name\n\n\n\n\n\n\n Step 2: Extract and collect the 2019 Elections (Contestant) data\nThe group also created utility functions for easier retrieval of the contestant datasets. This is to ensure each dataset is ready for aggregation.\nSimilar to the get_province_coc, the get_contestant_attrib method unpacks each key and value from the {contest_number}.json dictionary into a cleaned up dataframe. The method converts the bos directory into an additional list, which will also be appended into the resulting dataframe.\nThere are two (2) major political coalitions fighting for the senate seats: * Hugpong ng Pagbabago (HNP) * Otso Diretso\nSimilar to the get_all_province_coc, the get_contestants_attrib method is a walker that goes through each of the contest directory. The method will first append all {contest_numer}.json files into a singular dataframe. Next, the method creates a new column that identifies who among the senatorial candidates are part of the Hugpong ng Pagbabago (HNP) or Otso Diretso campaign.\n\ndef get_contestant_attrib(filepath):\n    \"\"\"\n    Returns the contestant json file into a dataframe\n\n    Parameters\n    ----------\n    filepath   : string\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestnat attributes\n\n    \"\"\"\n\n    contestants_values = []\n    with open(filepath, 'r') as file:\n        data = json.load(file)\n        attrib_keys = [key for key in list(data.keys())\n                       if isinstance(key, (str, float, int))]\n        attrib_values = [value for value in list(data.values())\n                         if isinstance(value, (str, float, int))]\n        contest_values = [list(contest.values()) for contest in data['bos']]\n        df = pd.DataFrame(contest_values,\n                          columns=list(data['bos'][0].keys()))\n        for k, v in zip(attrib_keys, attrib_values):\n            df[k] = v\n    return df\n\n\ndef get_contestants_attrib(filepath):\n    \"\"\"\n    Returns ALL contestant json files into a dataframe\n\n    Parameters\n    ----------\n    filepath   : string\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestant attributes\n\n    \"\"\"\n    df = pd.DataFrame()\n    for each_filepath in glob.glob(filepath):\n        df = df.append(get_contestant_attrib(each_filepath))\n    senators = df[df.cc == 1].copy()\n    senators['bon'] = senators['bon'].str.extract(pat='(.*?) \\(')\n    party = df[df.cc == 5567].copy()\n    df = senators.append(party)\n    df.drop_duplicates(inplace=True)\n    df.rename(columns={'boc': 'bo'}, inplace=True)\n    otso = ['AQUINO, BENIGNO BAM ', 'DIOKNO, CHEL', 'HILBAY, PILO',\n            'MACALINTAL, MACAROMY', 'GUTOC, SAMIRA', 'ALEJANO, GARY',\n            'ROXAS, MAR', 'TAÑADA,LORENZO ERIN TAPAT']\n    hnp = ['ANGARA, EDGARDO SONNY', 'BONG REVILLA, RAMON JR', 'CAYETANO, PIA',\n           'DELA ROSA, BATO', 'EJERCITO, ESTRADA JV', 'ESTRADA, JINGGOY',\n           'GO, BONG GO', 'MANGUDADATU, DONG', 'MANICAD, JIGGY',\n           'MARCOS, IMEE', 'PIMENTEL, KOKO', 'TOLENTINO, FRANCIS', \n           'VILLAR, CYNTHIA']\n    for o in otso:\n        df.loc[df.bon == o, 'coalition'] = \"Otso Diretso\"\n    for h in hnp:\n        df.loc[df.bon == h, 'coalition'] = \"HNP\"\n    df['coalition'] = df['coalition'].fillna('None')\n    return df\n\nLet us run the get_contestants_attrib. This will be used later in the blog for our further analysis.\n\ncontestant_filepaths = '/mnt/data/public/elections/nle2019/contests/*'\ndf_contestants = get_contestants_attrib(contestant_filepaths)\ndf_contestants.head()\n\nWe now have two dataframes: df_results containing the 2019 election results, and df_contestants containing the contestant information. These two dataframes can now be merged into a single dataframe. Let us also drop certain columns which we have deemed as unimportant.\n\ndef merge_comelec(results, contestants):\n    \"\"\"\n    Merge results dataframe with contestants dataframe\n\n    Parameters\n    ----------\n    results    : pd.DataFrame\n    contestants: pd.DataFrame\n\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestant attributes\n\n    \"\"\"\n\n    df = pd.merge(results, contestants, on=['bo', 'cc'], how='left')\n    df = df.drop(['vbc', 'boi', 'to', 'pc', 'pcc', 'pcy', 'pcm',\n                  'pck', 'ccc', 'pre', 'ser', 'cn'], axis=1)\n    df.columns = ['position', 'candidate_id', 'votes_per_province',\n                  'total_votes', 'votes_in_pct', 'region_raw', 'province',\n                  'region', 'candidate_name', 'party_name',\n                  'contest_position', 'contest_type', 'coalition']\n    return df\n\n\nnle2019 = merge_comelec(df_results, df_contestants)\nnle2019.region.unique()\n\n\n\n Step 3: Load Geopandas for geospatial processing \nThe Philippines is composed of seventeen (17) administrative regions. We can use the geopandas module to manage and pre-process geospatial data.\nLet us first load up a geopandas graph of the Philippines.\n\nfig, ax = plt.subplots(1, figsize=(10, 15), frameon=True)\n\nph0 = gpd.GeoDataFrame.from_file(\"ph_regions.shp\")\nph0.plot(ax=ax, cmap='Greens', edgecolor='#555555', k=18)\nax.set_title('Regions of the Philippines')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\nfor ind, row in ph0.iterrows():\n    ax.text(row[\"geometry\"].centroid.x, row[\"geometry\"].centroid.y,\n            row[\"region\"])"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#b.-national-senatorial-results",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#b.-national-senatorial-results",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " B. National Senatorial Results",
    "text": "B. National Senatorial Results\n\nLet us look at the senatorial candidates. Let us total up the votes by candidate and see the top 12 winners.\n\ndf_senators = nle2019[nle2019['contest_position'] == 'SENATOR']\ndf_senator = df_senators.groupby(['candidate_name']).agg(\n    {'votes_per_province': sum}).reset_index()\ndf_senator.sort_values('votes_per_province', ascending=False, inplace=True)\ndf_senator.columns = ['Candidate', 'Votes']\ndf_senator.head(12)\n\n\nfig, ax = plt.subplots(figsize=(15,8))\nplt.rcParams.update({'font.size': 14})\ndf_senator.set_index('Candidate').head(12).sort_values(\n    by='Votes', ascending=True).plot.barh(ax=ax,\n    title='Top 12 Candidates, in Millions', color='#BF5209', legend=False);\nax.set_xlabel('Total Votes');"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#c.-top-senator-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#c.-top-senator-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " C. Top Senator Per Region",
    "text": "C. Top Senator Per Region\nWe want to find out won across all the regions. If there is any bias for cetain candidates. Based on our findings, we can see that candidate Cynthia Villar won majority of the regions.\nIt is interesting to note that the top ranking senator for Ilocos Region (Region I) and the Cordillera Administrative Region (CAR) is Imee Marcos, which hails from that region. This confirms that there is a “Solid North”, and that support for the Marcoses still exists in that area.\nFor the Mindanao regions, the top candidate is Bong Go, former special assistant to President Duterte, who is from Mindanao.\nThese show that Philippine politics is very regional in nature. Voters will naturally support their hometown candidate, regardless of the issues surrounding that candidate.\n\ndf_senator_region = df_senators.groupby(\n    ['region', 'candidate_name']).agg({'votes_per_province': sum})\ndf_senator_region['rank'] = df_senator_region.groupby(\n    'region')['votes_per_province'].rank('dense', ascending=False)\ndf_senator_region = df_senator_region[df_senator_region['rank'] == 1].reset_index()\ndf_senator_region.columns = ['Region', 'Candidate Name', 'Votes', 'Rank']\ndf_senator_region\n\n\n(df_senator_region[df_senator_region['Rank'] == 1].reset_index().groupby(\n    'Candidate Name')['Rank'].sum().to_frame().sort_values(by='Rank')\n .reset_index())\n\n\nfig, ax = plt.subplots()\n(df_senator_region[df_senator_region['Rank'] == 1].reset_index().groupby(\n    'Candidate Name')['Rank'].sum().to_frame().sort_values(by='Rank').plot\n .barh(color='#BF5209', ax=ax));\nax.set_xlabel('Rank');"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#d.-dominant-party-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#d.-dominant-party-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " D. Dominant Party per Region",
    "text": "D. Dominant Party per Region\nThere are three main political parties vying for the senatorial seats: 1. LIBERAL PARTY 2. NACIONALISTA PARTY 3. PARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN (PDP-LABAN)\nWe looked at the dominant or majority political party per administrative region, and identify if any regions have any affiliations to a certain party.\n\ndf_party = df_senators.groupby(['region_raw', 'party_name']).agg({\n    'votes_per_province': sum})\ndf_party['rank'] = df_party.groupby(\n    'region_raw')['votes_per_province'].rank('dense', ascending=False)\ndf_party.reset_index(inplace=True)\ndf_leading_party = df_party[df_party['rank'] == 1].copy()\ndf_leading_party.columns = ['Region', 'Party', 'Votes', 'Rank']\ndf_leading_party.sort_values(['Region', 'Rank'], inplace=True)\ndf_leading_party\n\nWe can clearly see a voting bias of each region.\nMajority of Region 1 and Region 2 has a voting preference towards the NACIONALISTA PARTY. This can be attributed to the fact that Imee Marcos, who hails from Ilocos Norte, is a Nacionalista.\nMajority of Region 5 and Region 6 has a voting preference towards the LIBERAL PARTY. This is also expected since Leni Robredo, incumbent Vice President who is from the Liberal Party, is a Bicolano.\nThe remainder of the Philippines has a voting preference towards the PDP-LABAN.\n\nmerged = ph0.merge(df_leading_party, left_on='region', right_on='Region')\ncolors = 18\ncolor_map = {'PARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN': 'red', \n             'LIBERAL PARTY': 'yellow',\n             'NACIONALISTA PARTY': 'green'}\n\nfig, ax = plt.subplots(1, 1, figsize = (10,12));\nax.set_title('Dominant political party per region', fontsize=20);\n\nfor party in list(merged['Party'].unique()):\n    color_map[party]\n    merged[merged['Party'] == party].plot(ax=ax, color = color_map[party], \n                                          categorical = True, \n                                          figsize=(10,12), legend=True)\n\nmerged.geometry.boundary.plot(color=None,edgecolor='k',linewidth = .5,ax=ax);\nplt.rcParams.update({'font.size': 18})"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#e.-dominant-coalition-per-region",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#e.-dominant-coalition-per-region",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " E. Dominant Coalition Per Region",
    "text": "E. Dominant Coalition Per Region\nAside from individual candidates and parties, we also looked at the dominant coalition per region by counting the number of senate seats obtained by each coalition. The results indicate that HNP gained a majority of the seats across all regions, especially in Mindanao.\nThe only regions where HNP did not gain a solid majority are in Bicol Region (Region V) and Eastern Visayas (Region VI), known bailwicks of the Liberal Party.\n\ndef get_coalition_seats():\n    \"\"\"\n    Returns a dataframe of the number of seats won per coalition per region\n\n    Returns\n    -------\n    coalition : pd.DataFrame\n\n    \"\"\"\n    coalition_seats = nle2019.query('position == 1')\n\n    coalition_seats = coalition_seats.groupby(\n        ['region', 'candidate_name', 'coalition'],\n        as_index=False).agg({'votes_per_province': sum})\n    coalition_seats = coalition_seats.sort_values(\n        by=['region', 'votes_per_province'], ascending=[1, 0])\n    coalition_seats.set_index('region', inplace=True)\n\n    coalition_seats['rank'] = list(range(1, len(\n        coalition_seats.candidate_name.unique())+1))\\\n        * len(coalition_seats.index.unique())\n    coalition_seats['is_top_12'] = 1*(coalition_seats['rank'] &lt;= 12)\n    coalition_seats = coalition_seats.reset_index()\n    coalition_seats = pd.pivot_table(\n        coalition_seats, index='region', columns='coalition',\n        values='is_top_12', aggfunc=np.sum)\n    coalition_seats['coalition_seats_total'] = coalition_seats.sum(axis=1)\n\n    for coalition in coalition_seats.columns[:-1]:\n        coalition_seats['party_seats_pct_' + coalition] = \\\n            coalition_seats[coalition] / \\\n            coalition_seats['coalition_seats_total']\n        coalition_seats.rename(\n            columns={coalition: 'coalition_seats_count_' + coalition},\n            inplace=True)\n\n    coalition_seats = coalition_seats.round(5)\n    coalition_seats = coalition_seats.reset_index()\n    return coalition_seats\n\n\ncoalition = get_coalition_seats()\nmerged = ph0.merge(coalition[[\n                   'region', 'coalition_seats_count_HNP']], left_on='region',\n                   right_on='region')\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 15))\nmerged.plot('coalition_seats_count_HNP', ax=ax, cmap='YlOrRd', legend=True)\nax.set_title('Number of HNP senate seats won', fontsize=24)\nmerged.geometry.boundary.plot(color=None, edgecolor='k', linewidth=1, ax=ax)"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#f.-is-demographics-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#f.-is-demographics-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " F. Is Demographics Related To Voting Preference?",
    "text": "F. Is Demographics Related To Voting Preference?\nIs the voting preference of a region related to its demographics such as literacy rate and religious affiliation? To answer this, we obtained the 2015 Census Data. The directory is a collection of excel files, where each excel file corresponds to a certain region and province. If we explore each file, we can see that each sheet corresponds to a different demographic feature table. For this analysis, we are intrested at sheets T8 and T11.\nThe get_census_religion loads the imporant columns and rows from sheet T8. It also adds an additional column based on the region. Similarly, the get_census_education loads the imporant columns and rows from sheet T11. It also aggregates each individual years experience column into a singular cumulative column.\nFinally, the read_census_files aggregates the 2016 regional data into singular dataframe by using get_census_religion and get_census_education functions. To extract only the regional files, the read_census_files uses regex to get filenames with only underscores in the beginning (this is an indicator of regional data).\n\ndef get_census_religion(path):\n    \"\"\"\n    Returns a consolidated DataFrame of census data by religion\n\n    Parameters\n    ----------\n    path      : string, filepath to census directory\n\n\n    Returns\n    ----------\n    df        : pd.DataFrame\n\n    \"\"\"\n    filename = os.path.basename(path)\n    df = pd.read_excel(path, sheet_name='T8', header=None,\n                       usecols=[0, 1, 2, 3],\n                       skiprows=6, skip_blank_lines=True,\n                       skipfooter=3,\n                       names=['religion', 'total', 'm', 'f'])\n    df.sort_values('total', ascending=False, inplace=True)\n    df['region'] = re.search(r'\\_(.*?)\\_', os.path.basename(path)).group(1)\n    cols = ['region', 'religion', 'm', 'f', 'total']\n    df = df[cols]\n    return df\n\n\ndef get_census_education(path):\n    \"\"\"\n    Returns a consolidated DataFrame of census data by education\n\n    Parameters\n    ----------\n    path      : string, filepath to census directory\n\n\n    Returns\n    ----------\n    df        : pd.DataFrame\n\n    \"\"\"\n    filename = os.path.basename(path)\n    df = pd.read_excel(path, sheet_name='T11',\n                       usecols=[0, 15, 16, 17, 18, 19, 20],\n                       skiprows=3,\n                       skip_blank_lines=True, nrows=20,\n                       names=['education', '18', '19',\n                              '20_24', '25_29', '30_34', '35_above'])\n    df.dropna(how='any', inplace=True)\n    df.reset_index(inplace=True, drop=True)\n    df.drop(df.index[[0, 5, 6, 7, 9, 10, 12, 13]], inplace=True)\n    df['total'] = (df['18'] + df['19'] + df['20_24'] + df['25_29'] +\n                   df['30_34'] + df['35_above'])\n    df['region'] = re.search(r'\\_(.*?)\\_', os.path.basename(path)).group(1)\n    cols = ['region', 'education', '18', '19', '20_24', '25_29', '30_34',\n            '35_above', 'total']\n    df = df[cols]\n    return df\n\n\ndef read_census_files(path):\n    '''\n    Reads all census regional files\n\n    Parameter\n    ---------\n    path      : string, filepath to census directory\n\n    Returns\n    -------\n    Dictionary of dataframes\n    '''\n    total = {'religion': pd.DataFrame(),\n             'education': pd.DataFrame()}\n\n    for filepath in glob.glob(path + \".xls\", recursive=True):\n        if re.match('_(?!PHILIPPINES)', os.path.basename(filepath)):\n            total['religion'] = (total['religion']\n                                 .append(get_census_religion(filepath)))\n            total['education'] = (total['education']\n                                  .append(get_census_education(filepath)))\n    total['religion'].reset_index(inplace=True, drop=True)\n    total['education'].reset_index(inplace=True, drop=True)\n\n    for df in total.values():\n        df.loc[(df['region'] == \"ARMM\"), 'region'] = 'BARMM'\n        df.loc[(df['region'] == \"MIMAROPA\"), 'region'] = 'REGION IV-B'\n        df.loc[(df['region'] == \"CARAGA\"), 'region'] = 'REGION XIII'\n    return total\n\n\ncensus_path = '/mnt/data/public/census/*'\ncensus_dict = read_census_files(census_path)"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#g.-is-literacy-rate-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#g.-is-literacy-rate-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " G. Is literacy rate related to voting preference?",
    "text": "G. Is literacy rate related to voting preference?\nWe looked at the number of seats obtained by each coalition per region, then correlated it to literacy rate. Education information was obtained from the 2015 Census data.\nWe looked at the literacy rate \\(\\frac{n_{educated}}{n_{total}}\\) of each administrative region:\n\ndef get_education_percent():\n    '''\n    Gets percentage of educational level per region\n    '''\n    education = census_dict['education'].groupby(\n        ['region', 'education'], as_index=False).sum()\n    education = pd.pivot_table(\n        education, index='region', columns='education', values='total')\n    education.columns = ['education_pct_' + educ for educ in education.columns]\n    education['education_total'] = education.sum(axis=1)\n    for educ in education.columns[:-1]:\n        education[educ] /= education['education_total']\n    education.drop('education_total', axis=1, inplace=True)\n    education = education.round(5)\n\n    return education\n\n\ndef get_agg_education_percent():\n    '''\n    Gets aggregated percentage of educational level per region\n    '''\n    df_educ = get_education_percent()\n    df_educ = df_educ.reset_index()\n    df_educ['educated'] = (1 - df_educ['education_pct_No Grade Completed']\n                           - df_educ['education_pct_Not Stated'])\n    df_educ['not_educated'] = df_educ['education_pct_No Grade Completed']\n    df_educ['unknown'] = df_educ['education_pct_Not Stated']\n    df_educ.drop(columns=['education_pct_Academic Degree Holder',\n                          'education_pct_College Undergraduate',\n                          'education_pct_Elementary',\n                          'education_pct_High School',\n                          'education_pct_No Grade Completed',\n                          'education_pct_Not Stated',\n                          'education_pct_Post Baccalaureate',\n                          'education_pct_Post-Secondary',\n                          'education_pct_Pre-School',\n                          'education_pct_Special Education'], inplace=True)\n    df_educ.set_index('region', inplace=True)\n    df_educ.reset_index(inplace=True)\n    return df_educ\n\n\ndf_educ = get_agg_education_percent()\ndf_educ\n\nWe then checked if the number of seats obtained by each coalition is correlated to the literacy rate of that region. First, we obtained the number of seats obtained by each coalition per region:\n\ndef get_coalition_rank():\n    \"\"\"\n    Get number of seats obtained by each coalition\n    \"\"\"\n    coalition_seats = nle2019.query('position == 1')\n    coalition_seats = coalition_seats.groupby(\n        ['region', 'candidate_name', 'coalition'], as_index=False).agg({'votes_per_province': sum})\n    coalition_seats = coalition_seats.sort_values(\n        by=['region', 'votes_per_province'], ascending=[1, 0])\n    coalition_seats.set_index('region', inplace=True)\n\n    coalition_seats['rank'] = list(range(1, len(\n        coalition_seats.candidate_name.unique())+1)) * len(coalition_seats.index.unique())\n    coalition_seats['is_top_12'] = 1*(coalition_seats['rank'] &lt;= 12)\n    coalition_seats.reset_index(inplace=True)\n\n    coalition_seats = pd.pivot_table(\n        coalition_seats, index='region', columns='coalition', values='is_top_12', aggfunc=np.sum)\n    return coalition_seats\n\nWe then merge this with the education dataframe, then get the correlation:\n\ndf_coal_educ = get_agg_education_percent().set_index('region').join(get_coalition_rank())\ndf_coal_educ['Educated'] = df_coal_educ['educated']\ndf_coal_educ['Not Educated'] = df_coal_educ['not_educated']\ncorr = df_coal_educ.corr().loc[['Otso Diretso', 'HNP'],['Educated', 'Not Educated']]\ncolormap = sns.diverging_palette(100, 100, n = 10)\nsns.heatmap(corr, cmap=colormap, annot=True, vmin = -1, vmax = 1);\n\nThe Census and election data show that the voting preference of a region has no correlation with its literacy rate. We now look at religion to see if it has a correlation with the voting preference."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#h.-is-religion-related-to-voting-preference",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#h.-is-religion-related-to-voting-preference",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " H. Is religion related to voting preference?",
    "text": "H. Is religion related to voting preference?\nWe also looked into the religious affiliation per region, and checked if it is correlatd with voting preference.\nFirst, we obtained the distribution of religions per region from the Census data:\n\ndef get_religion_percent():\n    \"\"\"\n    Get percentages of religion per region\n    \"\"\"\n    religion = census_dict['religion'].groupby(\n        ['region', 'religion'], as_index=False).sum()\n    religion = pd.pivot_table(\n        religion, index='region', columns='religion', values='total')\n    religion.columns = ['religion_pct_' + rel for rel in religion.columns]\n    religion['religion_total'] = religion.sum(axis=1)\n    for rel in religion.columns[:-1]:\n        religion[rel] /= religion['religion_total']\n    religion.drop('religion_total', axis=1, inplace=True)\n    religion = religion.round(5)\n    return religion\n\n\ndf_rel = get_religion_percent()\n\nWe then merged the religion census data with the coalition ranking data to check if religion has correlation with the number of seats obtained by each coalition:\n\ndf_coal_rel = get_religion_percent().join(get_coalition_rank())\ndf_coal_rel.head()\n\nFrom getting the correlation of the religion data with the number of seats per coalition, it is apparent that the Voting preference of a region has no correlation with its religious affiliation.\n\ndf_coal_rel['Roman Catholic'] = df_coal_rel['religion_pct_Roman Catholic, including Catholic Charismatic']\ndf_coal_rel['Islam'] = df_coal_rel['religion_pct_Islam']\ndf_coal_rel['Iglesia ni Cristo'] = df_coal_rel['religion_pct_Iglesia ni Cristo']\ncorr = df_coal_rel.corr().loc[\n    ['Roman Catholic','Islam','Iglesia ni Cristo'],['HNP','Otso Diretso']]\ncolormap = sns.diverging_palette(100, 100, n = 10)\nsns.heatmap(corr, cmap=colormap, annot=True, vmin = -1, vmax = 1);"
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#i.-conclusion",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#i.-conclusion",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": " I. Conclusion",
    "text": "I. Conclusion\nUpon checking both the Comelec 2019 Election Results and the 2015 Philippine Census data, we found out that voting preference is characterized by high regionality. Candidates have a homecourt advantage, and voters tend to vote candidates or parties affiliated with their home region.\nAlso, literacy rate and religious affiliation is not correlated to voting preference."
  },
  {
    "objectID": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#acknowledgements",
    "href": "posts/20230414_Philippine_Voting_Preferences/index_files/DMW_Lab01_20190529_2.html#acknowledgements",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank Prof Christian Alis and the ACCeSS Laboratory for the access to the high-performance computing facility."
  },
  {
    "objectID": "posts/2023-04-14-Elections/index.html",
    "href": "posts/2023-04-14-Elections/index.html",
    "title": "Regional Voting Preferences in the 2019 Philippine Senatorial Elections",
    "section": "",
    "text": "Overview\n\nOriginal creation and submission for this notebook was last May 29, 2019.\nThis jupyter notebook was created for my Data Mining and Wrangling class for my Masters in Data Science program in the Asian Institute of Management. In particular, this was done during the 2nd semester for a class - Data Mining and Wrangling - as one of the core requirements. In this report, we shall explore and understand the results of the 2019 elections through descriptive statistics. We shall show some exploratory data analysis on the following questions: \n\n\nHow did the various administrative regions of the Philippines voted for their senators?\n\nIs the voter preference homogeneous across the country, or is there a preferred candidate or party per region? More specifically, how does (1) religious affiliation, (2) educational attainment, and (2) sex play a major role on how the voters select their candidates.\n\n\nAcknowledgements\n\nThis analysis was done together with my Lab partner, George Esleta. I would like to thank Prof Christian Alis and the ACCeSS Laboratory for the access to the high-performance computing facility.\n\nMethodology\n\n\nPre-requisites: Load Requirement Package\n\nBefore anything else, let us first load all important modules for this exercise.\n\n\nLoading required modules\nimport os\nimport io\nimport re\nimport time\nimport glob\nimport requests\nimport urllib.request\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\n\n\nIt is important to identify the datasets we are going to use for this exercise. The two identified datasets the group intends to use are: the 2019 National Data and the 2015-2016 Census data.\nWith regards to 2019 National data, the team used a web scapper provided Prof. Alis. The web scapper downloaded the election results from the Commission of Elections’ 2019 National and Local Elections website. The results were then stored in a local repository which is then easily accesible for the team. The 2019 elections results are broken down into two main directories: results and contest. In this exercise, the team will explore both directories to map out a comprehensive summary of the 2019 senatorial and party elections.\nSecondly, the 2015-2016 Census data has already been stored in a local repository for easier access. One of the main reasons why the team decided to use the 2015-2016 Census data is because of the lack of availability of recent data. The Philippine Statistics Authority only releases a comprehensive census survey ever six years. However for the purpose of this exercise, the team has agreed that the 2015-2016 census data can act as an appproximate for today’s population.\n\nStep 1: Extract and collect the 2019 Elections (Results) data\n\nThe first directory to explore is the 2019 Election results. The results directory contains all electoral results from a regional level down to the barangay level. For each level, a specific coc.json file is stored. This file contains all electoral results data and metadata for both national and local elections. However for the purposes of this analysis, we will only look at the aggregated elections data at the regional level. The files that we are interested are the coc.json files associated to each province, as these files contain the metadata and data on the election results.\nThe main structure of each coc.json file contains the following main keys: vbc, rs, sts, obs, and cos. For the purpose of this exercise, the important key the group needs to extract is the rs key as this provides the each candidate’s total votes per area. Under the rs key, the following keys can be found: cc, bo, v, tot, per, and ser. Cross referencing these keys with official statements and comelec documentations suggests that important keys are as follows: cc pertaining to contest_type, bo pertaining to the candidate_id, and v pertaining to votes_per_province.\n\n\n\nParemeter\nDescription\n\n\n\n\ncc\nContestant Code\n\n\nbo\nContestant ID\n\n\nv\nTotal Votes Per Contestant\n\n\ntot\nTotal Votes Per Province\n\n\n\nHowever, it must be pointed out that the available data only goes as high as provincial data. If we want to process the provincial level, the team will have to aggregate the data up.\nThe group created utility functions for easier retrieval of the provincial elections datasets. The purpose for the utility functions (and future utility functions) are for initial cleaning and manipulations. This is to ensure each dataset is ready for aggregation.\nThe get_province_coc method unpacks each key and value from the coc.json dictionary into a cleaned up dataframe. In addition, the method identifies which region and province the file originated from by examining the filepath that was passed.\nThe get_all_province_coc method is a walker that goes through each of the results directory. The walker checks if the filename has an equal value to coc.json. If a coc.jsonwas located, the get_province_coc method is applied with the filepath as the parameter. The resulting dataframe is then appended to a master dataframe for further extraction and analysis. For this exercise, the group only had to extract data up to the regional and provincial levels so only three wildcard were use for the glob walker.\nSpecial methods (get_ncr_coc and get_all_ncr_coc) were established to get the cities’ coc.json. For the case of the NCR cities, theire associated coc.json files were one directory lower.\n\n\n\n\nFunction for get_province_coc()\ndef get_province_coc(filepath):\n    \"\"\"\n    Loads a single coc file. \n\n    Adds additional columns `region` and `province to the DataFrame,\n    depending on filepath.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    output = []\n    with open(filepath, 'r') as f:\n        dirpath, filepath = os.path.split(filepath)\n        region = dirpath.split('/')[-2]\n        province = dirpath.split('/')[-1]\n        data = json.load(f)\n        for each in data['rs']:\n            row = [float(element) for element in list(each.values())]\n            output.append([data['vbc']] + row + [region] + [province])\n    df = pd.DataFrame(output,\n                      columns=['vbc', 'cc', 'bo', 'v', 'tot', 'per', 'ser',\n                               'region', 'province'])\n    return df\n\n\n\n\nFunction for get_all_province_coc()\ndef get_all_province_coc(tree):\n    \"\"\"\n    Loads all province COC files and saves them to a dataframe\n\n    Checks the filepath if filename is 'coc.json'\n\n    Created a new column to deal with the reclassification of\n        \"NEGROS ORIENTAL\" and \"NEGROS OCCIDENTAL\" to \"NIR\" \n            to match the PSA 2016 dataset.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    total = pd.DataFrame()\n    for file in glob.glob(tree):\n        if os.path.basename(file) == 'coc.json':\n            df = get_province_coc(file)\n            total = total.append(df)\n    total.rename(columns={'region': 'region_raw'}, inplace=True)\n    total['region'] = total['region_raw'].copy()\n    total.loc[(total['province'] == \"NEGROS ORIENTAL\") |\n              (total['province'] == \"NEGROS OCCIDENTAL\"), 'region'] = 'NIR'\n    return total\n\n\n\n\nFunction for get_ncr_coc()\ndef get_ncr_coc(filepath):\n    \"\"\"\n    Loads a single coc file. \n\n    Adds additional columns `region` and `province to the DataFrame,\n    depending on filepath.\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe    \n\n    \"\"\"\n    output = []\n    with open(filepath, 'r') as f:\n        dirpath, filepath = os.path.split(filepath)\n        region = dirpath.split('/')[-3]\n        province = dirpath.split('/')[-2]\n        data = json.load(f)\n        for each in data['rs']:\n            row = [float(element) for element in list(each.values())]\n            output.append([data['vbc']] + row + [region] + [province])\n    df = pd.DataFrame(output,\n                      columns=['vbc', 'cc', 'bo', 'v', 'tot', 'per', 'ser',\n                               'region', 'province'])\n    return df\n\n\n\n\nFunction for get_all_ncr_coc()\ndef get_all_ncr_coc(tree):\n    \"\"\"\n    Loads all province COC files and saves them to a dataframe\n\n    Checks the filepath if filename is 'coc.json'\n\n    Parameters\n    ----------\n    filepath    : filepath\n\n    Return\n    ------\n    df          : a dataframe\n    \"\"\"\n    total = pd.DataFrame()\n    for file in glob.glob(tree):\n        if file.split('/')[7] == 'NCR':\n            if os.path.basename(file) == 'coc.json':\n                df = get_ncr_coc(file)\n                total = total.append(df)\n    total.rename(columns={'region': 'region_raw'}, inplace=True)\n    total['region'] = total['region_raw'].copy()\n    return total\n\n\n\nWith these utility functions inplace, the team can now apply these methods for easier access to the 2019 elections data.\nWe can now compile all of the election results with the following line:\n\n\nCompiling provinces data with NCR data\ntree = '/mnt/data/public/elections/nle2019/results/*/*/*'\nncr_tree = '/mnt/data/public/elections/nle2019/results/*/*/*/*'\ndf_results = get_all_province_coc(tree)\ndf_results = df_results.append(get_all_ncr_coc(ncr_tree))\ndf_results.drop_duplicates(inplace=True)\n\n\nLet’s see what we have:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvbc\ncc\nbo\nv\ntot\nper\nser\nregion_raw\nprovince\nregion\n\n\n\n\n0\n89550\n1.0\n1.0\n2004.0\n1708769.0\n0.11\n2800.0\nREGION I\nILOCOS NORTE\n\n\n1\n89550\n1.0\n2.0\n1607.0\n1708769.0\n0.09\n2800.0\nREGION I\nILOCOS NORTE\n\n\n2\n89550\n1.0\n3.0\n8772.0\n1708769.0\n0.51\n2800.0\nREGION I\nILOCOS NORTE\n\n\n3\n89550\n1.0\n4.0\n1767.0\n1708769.0\n0.10\n2800.0\nREGION I\nILOCOS NORTE\n\n\n4\n89550\n1.0\n5.0\n5068.0\n1708769.0\n0.29\n2800.0\nREGION I\nILOCOS NORTE\n\n\n\n\n\nNext, let us examine the obtained dataset with actual election results.\nBy cross checking the results with Comelec data, we can identify the senators and party names.\nJust to check our data, we can look at an example senator from the dataset. By choosing cc=1 and bo=46, we are actually highlighting Imee Marcos’ senatorial candidacy results.\n\n\nCode of creating a bar graph for Imee Marcos highest voting province in ascending order\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_marcos = df_results.query('cc == 1 & bo == 46').copy()\ndf_marcos.groupby('region').sum()['v'].sort_values(\n    ascending=True).plot.barh(figsize=(10, 10),\n                              title='Contestant: 46 - Imee Marcos',\n                              color='#BF5209', ax=ax);\n\n\n\n\n\nFIGURE: Total Votes for Imee Marcos by Region\n\n\nAdditionally, let us check some descriptive statistics for the 2019 Elections dataset. More specifically, let us examine the v or votes column. The group will be highly dependent on the votes data so let us first do some initial statistics and visualizations.\n\n\nCode to groupby sum up by region and province\ndf_regions = df_results.groupby(['region', 'province'])['v'].sum().to_frame()\ndf_regions = df_regions.rename(columns={'v': 'votes'})\ndf_regions.head(16)\n\n\n\n\n\nregion\nprovince\nvotes\n\n\n\n\nBARMM\nBASILAN\n2093067.0\n\n\n\nLANAO DEL SUR\n4770462.0\n\n\n\nMAGUINDANAO\n5917983.0\n\n\n\nSULU\n3529555.0\n\n\n\nTAWI-TAWI\n1874486.0\n\n\nCAR\nABRA\n1923481.0\n\n\n\nAPAYAO\n703002.0\n\n\n\nBENGUET\n2426397.0\n\n\n\nIFUGAO\n1408688.0\n\n\n\nKALINGA\n1621414.0\n\n\n\nMOUNTAIN PROVINCE\n1074249.0\n\n\nNCR\nNATIONAL CAPITAL REGION - FOURTH DISTRICT\n22896771.0\n\n\n\nNATIONAL CAPITAL REGION - MANILA\n13461229.0\n\n\n\nNATIONAL CAPITAL REGION - SECOND DISTRICT\n29803007.0\n\n\n\nNATIONAL CAPITAL REGION - THIRD DISTRICT\n18481014.0\n\n\n\nTAGUIG - PATEROS\n10018306.0\n\n\n\nJust to explore, the code and image below shows the total votes in region 3. Notice that Bulacan has the highest number of votes, followed by Nueva Ecija, then Pampanga.\n\n\nCode to plot summary for Region 3 total votes\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION III',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nFIGURE: Votes Distribution in Region III\n\n\nHere we explore the vote distribution in the National Capital Region. The classification of district and cities were broken down by area (Manila, Taguig-Pateros, Second District, Third District, and Fourth District).The top number of votes came from the second district (mainly because of Quezon City), followed by the Fourth District, then the Third District.\n\n\nCode to plot summary for NCR total votes\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['NCQ',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nFIGURE: Votes Distribution in the National Capital Region\n\n\nNext, we explore the distribution of votes in Region 4-A. Unsurprisingly, the population of votes is high in the population centers of Cavite, then Batangas, then Laguna.\n\n\nCode to plot summary for Region 4-A total votes\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION IV-A',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nFIGURE: Votes Distribution in the Region IV-A\n\n\nFor now, let us also examine the distribution of election votes in region 7. Unsurprisingly, the votes are concentrated first in Cebu, followed by Bohol and Siquijor.\n\n\nCode to plot summary for Region 7 total votes\nfig, ax = plt.subplots()\nax.set_xlabel('Votes')\ndf_test.loc['REGION VII',\"votes\"].plot.barh(color='#BF5209', ax=ax);\n\n\n\n\n\nFIGURE: Votes Distribution in the Region VII\n\n\nTo match the contestant ID to the contestant name, the contest files were also downloaded from the Commission of Elections’ 2019 National and Local Elections website and stored in the local repository. Similar to the results directory, the contest directory contained json files for each contest type/position. Upon inspection of a sample file within the directory, the following values were obtained:\nThe pertinent keys from each json files were:\n\n\n\nParameter\nDescription\n\n\n\n\ncc\nContest code\n\n\ncn\nContest code name - location\n\n\nccc\nContest code name\n\n\ntype\nContest type\n\n\nbos\nlist of candidate parameters\n\n\n\nUnder the bos key, we can extract each of the candidates’ parameters. The more useful ones for the group’s study include:\n\n\n\nParameter\nDescription\n\n\n\n\nboc\nContestant ID\n\n\nbon\nContestant Name\n\n\npn\nParty Name\n\n\n\n\nStep 2: Extract and collect the 2019 Elections (Contestant) data\n\nThe group also created utility functions for easier retrieval of the contestant datasets. This is to ensure each dataset is ready for aggregation.\nSimilar to the get_province_coc, the get_contestant_attrib method unpacks each key and value from the {contest_number}.json dictionary into a cleaned up dataframe. The method converts the bos directory into an additional list, which will also be appended into the resulting dataframe.\nThere are two (2) major political coalitions fighting for the senate seats:\n\nHugpong ng Pagbabago (HNP)\nOtso Diretso\n\nSimilar to the get_all_province_coc, the get_contestants_attrib method is a walker that goes through each of the contest directory. The method will first append all {contest_numer}.json files into a singular dataframe. Next, the method creates a new column that identifies who among the senatorial candidates are part of the Hugpong ng Pagbabago (HNP) or Otso Diretso campaign.\n\n\nFunction for get_contestant_attrib()\ndef get_contestant_attrib(filepath):\n    \"\"\"\n    Returns the contestant json file into a dataframe\n\n    Parameters\n    ----------\n    filepath   : string\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestnat attributes\n\n    \"\"\"\n\n    contestants_values = []\n    with open(filepath, 'r') as file:\n        data = json.load(file)\n        attrib_keys = [key for key in list(data.keys())\n                       if isinstance(key, (str, float, int))]\n        attrib_values = [value for value in list(data.values())\n                         if isinstance(value, (str, float, int))]\n        contest_values = [list(contest.values()) for contest in data['bos']]\n        df = pd.DataFrame(contest_values,\n                          columns=list(data['bos'][0].keys()))\n        for k, v in zip(attrib_keys, attrib_values):\n            df[k] = v\n    return df\n\n\n\n\nFunction for get_contestants_attrib\ndef get_contestants_attrib(filepath):\n    \"\"\"\n    Returns ALL contestant json files into a dataframe\n\n    Parameters\n    ----------\n    filepath   : string\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestant attributes\n\n    \"\"\"\n    df = pd.DataFrame()\n    for each_filepath in glob.glob(filepath):\n        df = df.append(get_contestant_attrib(each_filepath))\n    senators = df[df.cc == 1].copy()\n    senators['bon'] = senators['bon'].str.extract(pat='(.*?) \\(')\n    party = df[df.cc == 5567].copy()\n    df = senators.append(party)\n    df.drop_duplicates(inplace=True)\n    df.rename(columns={'boc': 'bo'}, inplace=True)\n    otso = ['AQUINO, BENIGNO BAM ', 'DIOKNO, CHEL', 'HILBAY, PILO',\n            'MACALINTAL, MACAROMY', 'GUTOC, SAMIRA', 'ALEJANO, GARY',\n            'ROXAS, MAR', 'TAÑADA,LORENZO ERIN TAPAT']\n    hnp = ['ANGARA, EDGARDO SONNY', 'BONG REVILLA, RAMON JR', 'CAYETANO, PIA',\n           'DELA ROSA, BATO', 'EJERCITO, ESTRADA JV', 'ESTRADA, JINGGOY',\n           'GO, BONG GO', 'MANGUDADATU, DONG', 'MANICAD, JIGGY',\n           'MARCOS, IMEE', 'PIMENTEL, KOKO', 'TOLENTINO, FRANCIS', \n           'VILLAR, CYNTHIA']\n    for o in otso:\n        df.loc[df.bon == o, 'coalition'] = \"Otso Diretso\"\n    for h in hnp:\n        df.loc[df.bon == h, 'coalition'] = \"HNP\"\n    df['coalition'] = df['coalition'].fillna('None')\n    return df\n\n\nLet us run the get_contestants_attrib. This will be used later in the blog for our further analysis.\n\n\nExecuting get_contestants_attrib function\ncontestant_filepaths = '/mnt/data/public/elections/nle2019/contests/*'\ndf_contestants = get_contestants_attrib(contestant_filepaths)\ndf_contestants.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbo\nbon\nboi\nto\npc\npn\npcc\npcy\npcm\npck\ncc\ncn\nccc\nccn\npre\ntype\ncoalition\n\n\n\n\n37\nHILBAY, PILO\n52.png\n37\n2\nAKSYON DEMOKRATIKO\n1\n1\n1\n1\n1\nSENATOR PHILIPPINES\n1\nSENATOR\n3\nnational\nOtso Diretso\n\n\n7\nALUNAN, RAFFY\n53.png\n7\n3\nBAGUMBAYAN VOLUNTEERS FOR A NEW PHILIPPINES\n1\n1\n1\n1\n1\nSENATOR PHILIPPINES\n1\nSENATOR\n3\nnational\nNone\n\n\n14\nBALDEVARONA, BALDE\n35.png\n14\n7\nFILIPINO FAMILY PARTY\n1\n1\n1\n1\n1\nSENATOR PHILIPPINES\n1\nSENATOR\n3\nnational\nNone\n\n\n18\nCASIÑO, TOTI\n20.png\n18\n8\nKATIPUNAN NG DEMOKRATIKONG PILIPINO(KDP)\n1\n1\n1\n1\n1\nSENATOR PHILIPPINES\n1\nSENATOR\n3\nnational\nNone\n\n\n21\nCHONG, GLENN\n61.png\n21\n8\nKATIPUNAN NG DEMOKRATIKONG PILIPINO(KDP)\n1\n1\n1\n1\n1\nSENATOR PHILIPPINES\n1\nSENATOR\n3\nnational\nNone\n\n\n\n\nWe now have two dataframes: df_results containing the 2019 election results, and df_contestants containing the contestant information. These two dataframes can now be merged into a single dataframe. Let us also drop certain columns which we have deemed as unimportant.\n\n\nFunction for merge_comelec()\ndef merge_comelec(results, contestants):\n    \"\"\"\n    Merge results dataframe with contestants dataframe\n\n    Parameters\n    ----------\n    results    : pd.DataFrame\n    contestants: pd.DataFrame\n\n\n    Returns\n    ----------\n    df         : pd.DataFrame of contestant attributes\n\n    \"\"\"\n\n    df = pd.merge(results, contestants, on=['bo', 'cc'], how='left')\n    df = df.drop(['vbc', 'boi', 'to', 'pc', 'pcc', 'pcy', 'pcm',\n                  'pck', 'ccc', 'pre', 'ser', 'cn'], axis=1)\n    df.columns = ['position', 'candidate_id', 'votes_per_province',\n                  'total_votes', 'votes_in_pct', 'region_raw', 'province',\n                  'region', 'candidate_name', 'party_name',\n                  'contest_position', 'contest_type', 'coalition']\n    return df\n\n\nLet’s merge the tables, then just check the unique regions using the unique method.\n\n\nMerging the results and contestants dataframes\nnle2019 = merge_comelec(df_results, df_contestants)\nnle2019.region.unique()\n\narray(['REGION I', 'REGION IV-B', 'BARMM', 'REGION II', 'REGION III',\n       'REGION V', 'REGION VI', 'NIR', 'REGION VII', 'REGION VIII',\n       'REGION IX', 'REGION X', 'REGION XI', 'REGION XII', 'REGION XIII',\n       'REGION IV-A', 'CAR', 'NCR'], dtype=object)\n\n\n\nStep 3: Load Geopandas for geospatial processing\n\n\nLoading the geospatial data\n\nThe Philippines is composed of seventeen (17) administrative regions. We can use the geopandas module to manage and pre-process geospatial data.\nLet us first load up a geopandas graph of the Philippines.\n\n\nCode to plot a map of the Philippines\nfig, ax = plt.subplots(1, figsize=(10, 15), frameon=True)\n\nph0 = gpd.GeoDataFrame.from_file(\"ph_regions.shp\")\nph0.plot(ax=ax, cmap='Greens', edgecolor='#555555', k=18)\nax.set_title('Regions of the Philippines')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\nfor ind, row in ph0.iterrows():\n    ax.text(row[\"geometry\"].centroid.x, row[\"geometry\"].centroid.y,\n            row[\"region\"])\n\n\n\n\n\nFIGURE: Map of the Philippines\n\n\n\nLooking at Senatorial Results\n\nLet us look at the senatorial candidates. Let us total up the votes by candidate and see the top 12 winners.\n\n\n\nIndex\nCandidate\nVotes\n\n\n\n\n61\nVILLAR, CYNTHIA\n23653546.0\n\n\n54\nPOE, GRACE\n20877585.0\n\n\n33\nGO, BONG GO\n18979132.0\n\n\n18\nCAYETANO, PIA\n18287782.0\n\n\n23\nDELA ROSA, BATO\n17396249.0\n\n\n7\nANGARA, EDGARDO SONNY\n16826634.0\n\n\n39\nLAPID, LITO\n16181906.0\n\n\n45\nMARCOS, IMEE\n14735294.0\n\n\n59\nTOLENTINO, FRANCIS\n14264142.0\n\n\n15\nBONG REVILLA, RAMON JR\n13899831.0\n\n\n14\nBINAY, NANCY\n13864931.0\n\n\n53\nPIMENTEL, KOKO\n13529531.0\n\n\n\nLet’s visualize this with a plot:\n\n\nCode to plot top senators by vote\nfig, ax = plt.subplots(figsize=(15,8))\nplt.rcParams.update({'font.size': 14})\ndf_senator.set_index('Candidate').head(12).sort_values(\n    by='Votes', ascending=True).plot.barh(ax=ax,\n    title='Top 12 Candidates, in Millions', color='#BF5209', legend=False);\nax.set_xlabel('Total Votes');im\n\n\n\n\n\nFIGURE:Top 12 senators by vote\n\n\n\nTop Senator Per Region\n\nWe want to find out won across all the regions. If there is any bias for cetain candidates. Based on our findings, we can see that candidate Cynthia Villar won majority of the regions.\nIt is interesting to note that the top ranking senator for Ilocos Region (Region I) and the Cordillera Administrative Region (CAR) is Imee Marcos, which hails from that region. This confirms that there is a “Solid North”, and that support for the Marcoses still exists in that area.\nFor the Mindanao regions, the top candidate is Bong Go, former special assistant to President Duterte, who is from Mindanao.\nThese show that Philippine politics is very regional in nature. Voters will naturally support their hometown candidate, regardless of the issues surrounding that candidate.\n\n\nCode to get top senator by region and the associated number of votes\ndf_senator_region = df_senators.groupby(\n    ['region', 'candidate_name']).agg({'votes_per_province': sum})\ndf_senator_region['rank'] = df_senator_region.groupby(\n    'region')['votes_per_province'].rank('dense', ascending=False)\ndf_senator_region = df_senator_region[df_senator_region['rank'] == 1].reset_index()\ndf_senator_region.columns = ['Region', 'Candidate Name', 'Votes', 'Rank']\ndf_senator_region\n\n\n\n\n\n\nRegion\nCandidate Name\nVotes\nRank\n\n\n\n\n0\nBARMM\nGO, BONG GO\n768037\n1\n\n\n1\nCAR\nMARCOS, IMEE\n348303\n1\n\n\n2\nNCR\nVILLAR, CYNTHIA\n3345089\n1\n\n\n3\nNIR\nVILLAR, CYNTHIA\n856992\n1\n\n\n4\nREGION I\nMARCOS, IMEE\n1661318\n1\n\n\n5\nREGION II\nVILLAR, CYNTHIA\n915393\n1\n\n\n6\nREGION III\nVILLAR, CYNTHIA\n3056167\n1\n\n\n7\nREGION IV-A\nVILLAR, CYNTHIA\n3660112\n1\n\n\n8\nREGION IV-B\nVILLAR, CYNTHIA\n688494\n1\n\n\n9\nREGION IX\nGO, BONG GO\n663927\n1\n\n\n10\nREGION V\nPOE, GRACE\n1417114\n1\n\n\n11\nREGION VI\nVILLAR, CYNTHIA\n1064681\n1\n\n\n12\nREGION VII\nVILLAR, CYNTHIA\n1307605\n1\n\n\n13\nREGION VIII\nVILLAR, CYNTHIA\n1132757\n1\n\n\n14\nREGION X\nGO, BONG GO\n949392\n1\n\n\n15\nREGION XI\nGO, BONG GO\n1002771\n1\n\n\n16\nREGION XII\nVILLAR, CYNTHIA\n983354\n1\n\n\n17\nREGION XIII\nGO, BONG GO\n777931\n1\n\n\n\n\n\nCode to get top senator by region and the associated number of votes\n(df_senator_region[df_senator_region['Rank'] == 1].reset_index().groupby(\n    'Candidate Name')['Rank'].sum().to_frame().sort_values(by='Rank')\n .reset_index())\n\n\n\n\n\nCandidate Name\nCount\n\n\n\n\nPOE, GRACE\n1.0\n\n\nMARCOS, IMEE\n2.0\n\n\nGO, BONG GO\n5.0\n\n\nVILLAR, CYNTHIA\n10.0\n\n\n\n\n\nCode\n fig, ax = plt.subplots()\n(df_senator_region[df_senator_region['Rank'] == 1].reset_index().groupby(\n    'Candidate Name')['Rank'].sum().to_frame().sort_values(by='Rank').plot\n .barh(color='#BF5209', ax=ax));\nax.set_xlabel('Rank');\n\n\n\n\n\nFIGURE: Instances of candidate ranking 1 per province\n\n\nAs we can see, Cynthia Villar was ranked 1st 10 times, followed by Bong Go at 5 times, Imee Marcos at 2 times, and Grace Poe 1 time.\n\nStep 4: Finding the Dominant Party Per Region\n\nThere are three main political parties vying for the senatorial seats:\n\n\nLiberal Party\n\nNacionalista Party\n\nPartito Demokratiko Pilipino Lakas ng Bayan (PDP-Laban)\n\nWe looked at the dominant or majority political party per administrative region, and identify if any regions have any affiliations to a certain party.\n\n\nCode to retrieve top Party Per Region\ndf_party = df_senators.groupby(['region_raw', 'party_name']).agg({\n    'votes_per_province': sum})\ndf_party['rank'] = df_party.groupby(\n    'region_raw')['votes_per_province'].rank('dense', ascending=False)\ndf_party.reset_index(inplace=True)\ndf_leading_party = df_party[df_party['rank'] == 1].copy()\ndf_leading_party.columns = ['Region', 'Party', 'Votes', 'Rank']\ndf_leading_party.sort_values(['Region', 'Rank'], inplace=True)\ndf_leading_party.drop(['Rank', 'index'], inplace=True)\n\n\nThis is the resulting table:\n\n\n\n\n\n\n\n\nRegion\nParty\nVotes\n\n\n\n\nBARMM\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n2596572.0\n\n\nCAR\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n1028536.0\n\n\nNCR\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n10400476.0\n\n\nREGION I\nNACIONALISTA PARTY\n4361859.0\n\n\nREGION II\nNACIONALISTA PARTY\n2474544.0\n\n\nREGION III\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n7804389.0\n\n\nREGION IV-A\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n9542856.0\n\n\nREGION IV-B\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n1733733.0\n\n\nREGION IX\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n2571518.0\n\n\nREGION V\nLIBERAL PARTY\n4885924.0\n\n\nREGION VI\nLIBERAL PARTY\n4335427.0\n\n\nREGION VII\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n5480074.0\n\n\nREGION VIII\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n3442654.0\n\n\nREGION X\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n3697535.0\n\n\nREGION XI\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n3654361.0\n\n\nREGION XII\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n3861166.0\n\n\nREGION XIII\nPARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN\n2858376.0\n\n\n\nNext, let us generate the code to create a map with majority winners.\n\n\nCode to create a map of majority winners\nmerged = ph0.merge(df_leading_party, left_on='region', right_on='Region')\ncolors = 18\ncolor_map = {'PARTIDO DEMOKRATIKO PILIPINO LAKAS NG BAYAN': 'red', \n             'LIBERAL PARTY': 'yellow',\n             'NACIONALISTA PARTY': 'green'}\n\nfig, ax = plt.subplots(1, 1, figsize = (10,12));\nax.set_title('Dominant political party per region', fontsize=20);\n\nfor party in list(merged['Party'].unique()):\n    color_map[party]\n    merged[merged['Party'] == party].plot(ax=ax, color = color_map[party], \n                                          categorical = True, \n                                          figsize=(10,12), legend=True)\n\nmerged.geometry.boundary.plot(color=None,edgecolor='k',linewidth = .5,ax=ax);\nplt.rcParams.update({'font.size': 18})\n\n\n\n\n\nFIGURE: Top Party by Region\n\n\nWe can clearly see a voting bias of each region.\nMajority of Region 1 and Region 2 has a voting preference towards the Nacionalista Party. This can be attributed to the fact that Imee Marcos, who hails from Ilocos Norte, is a Nacionalista.\nMajority of Region 5 and Region 6 has a voting preference towards the LIberal Party. This is also expected since Leni Robredo, incumbent Vice President who is from the Liberal Party, is a Bicolano.\nThe remainder of the Philippines has a voting preference towards the PDP-LABAN.\n\nStep 5: Finding the Dominant Coalition Per Region\n\nAside from individual candidates and parties, we also looked at the dominant coalition per region by counting the number of senate seats obtained by each coalition. The results indicate that HNP gained a majority of the seats across all regions, especially in Mindanao.\nThe only regions where HNP did not gain a solid majority are in Bicol Region (Region V) and Eastern Visayas (Region VI), known bailwicks of the Liberal Party. Below we created a function to get the number of seats won per coalition per region, and the associated map. The more seats won, the darker the color.\n\n\nFunction for get_coalition_sets()\ndef get_coalition_seats():\n    \"\"\"\n    Returns a dataframe of the number of seats won per coalition per region\n\n    Returns\n    -------\n    coalition : pd.DataFrame\n\n    \"\"\"\n    coalition_seats = nle2019.query('position == 1')\n\n    coalition_seats = coalition_seats.groupby(\n        ['region', 'candidate_name', 'coalition'],\n        as_index=False).agg({'votes_per_province': sum})\n    coalition_seats = coalition_seats.sort_values(\n        by=['region', 'votes_per_province'], ascending=[1, 0])\n    coalition_seats.set_index('region', inplace=True)\n\n    coalition_seats['rank'] = list(range(1, len(\n        coalition_seats.candidate_name.unique())+1))\\\n        * len(coalition_seats.index.unique())\n    coalition_seats['is_top_12'] = 1*(coalition_seats['rank'] &lt;= 12)\n    coalition_seats = coalition_seats.reset_index()\n    coalition_seats = pd.pivot_table(\n        coalition_seats, index='region', columns='coalition',\n        values='is_top_12', aggfunc=np.sum)\n    coalition_seats['coalition_seats_total'] = coalition_seats.sum(axis=1)\n\n    for coalition in coalition_seats.columns[:-1]:\n        coalition_seats['party_seats_pct_' + coalition] = \\\n            coalition_seats[coalition] / \\\n            coalition_seats['coalition_seats_total']\n        coalition_seats.rename(\n            columns={coalition: 'coalition_seats_count_' + coalition},\n            inplace=True)\n\n    coalition_seats = coalition_seats.round(5)\n    coalition_seats = coalition_seats.reset_index()\n    return coalition_seats\n\n\n\n\nCode to merge coalition dataframe and coalition_seat_counts, code to display heat map.\ncoalition = get_coalition_seats()\nmerged = ph0.merge(coalition[[\n                   'region', 'coalition_seats_count_HNP']], left_on='region',\n                   right_on='region')\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 15))\nmerged.plot('coalition_seats_count_HNP', ax=ax, cmap='YlOrRd', legend=True)\nax.set_title('Number of HNP senate seats won', fontsize=24)\nmerged.geometry.boundary.plot(color=None, edgecolor='k', linewidth=1, ax=ax)\n\n\n\n\n\nFIGURE: Number of HNP Senate Seats Won\n\n\n\nStep 6: Is Demographics Related To Voting Preferences?\n\nIs the voting preference of a region related to its demographics such as literacy rate and religious affiliation? To answer this, we obtained the 2015 Census Data. The directory is a collection of excel files, where each excel file corresponds to a certain region and province. If we explore each file, we can see that each sheet corresponds to a different demographic feature table. For this analysis, we are intrested at sheets T8 and T11.\nThe get_census_religion loads the imporant columns and rows from sheet T8. It also adds an additional column based on the region. Similarly, the get_census_education loads the imporant columns and rows from sheet T11. It also aggregates each individual years experience column into a singular cumulative column.\nFinally, the read_census_files aggregates the 2016 regional data into singular dataframe by using get_census_religion and get_census_education functions. To extract only the regional files, the read_census_files uses regex to get filenames with only underscores in the beginning (this is an indicator of regional data).\n\n\nFunction for get_censis_religion()\ndef get_census_religion(path):\n    \"\"\"\n    Returns a consolidated DataFrame of census data by religion\n\n    Parameters\n    ----------\n    path      : string, filepath to census directory\n\n\n    Returns\n    ----------\n    df        : pd.DataFrame\n\n    \"\"\"\n    filename = os.path.basename(path)\n    df = pd.read_excel(path, sheet_name='T8', header=None,\n                       usecols=[0, 1, 2, 3],\n                       skiprows=6, skip_blank_lines=True,\n                       skipfooter=3,\n                       names=['religion', 'total', 'm', 'f'])\n    df.sort_values('total', ascending=False, inplace=True)\n    df['region'] = re.search(r'\\_(.*?)\\_', os.path.basename(path)).group(1)\n    cols = ['region', 'religion', 'm', 'f', 'total']\n    df = df[cols]\n    return df\n\n\n\n\nFunction for get_census_education()\ndef get_census_education(path):\n    \"\"\"\n    Returns a consolidated DataFrame of census data by education\n\n    Parameters\n    ----------\n    path      : string, filepath to census directory\n\n\n    Returns\n    ----------\n    df        : pd.DataFrame\n\n    \"\"\"\n    filename = os.path.basename(path)\n    df = pd.read_excel(path, sheet_name='T11',\n                       usecols=[0, 15, 16, 17, 18, 19, 20],\n                       skiprows=3,\n                       skip_blank_lines=True, nrows=20,\n                       names=['education', '18', '19',\n                              '20_24', '25_29', '30_34', '35_above'])\n    df.dropna(how='any', inplace=True)\n    df.reset_index(inplace=True, drop=True)\n    df.drop(df.index[[0, 5, 6, 7, 9, 10, 12, 13]], inplace=True)\n    df['total'] = (df['18'] + df['19'] + df['20_24'] + df['25_29'] +\n                   df['30_34'] + df['35_above'])\n    df['region'] = re.search(r'\\_(.*?)\\_', os.path.basename(path)).group(1)\n    cols = ['region', 'education', '18', '19', '20_24', '25_29', '30_34',\n            '35_above', 'total']\n    df = df[cols]\n    return df\n\n\n\n\nFunction for get_census_files()\ndef read_census_files(path):\n    '''\n    Reads all census regional files\n\n    Parameter\n    ---------\n    path      : string, filepath to census directory\n\n    Returns\n    -------\n    Dictionary of dataframes\n    '''\n    total = {'religion': pd.DataFrame(),\n             'education': pd.DataFrame()}\n\n    for filepath in glob.glob(path + \".xls\", recursive=True):\n        if re.match('_(?!PHILIPPINES)', os.path.basename(filepath)):\n            total['religion'] = (total['religion']\n                                 .append(get_census_religion(filepath)))\n            total['education'] = (total['education']\n                                  .append(get_census_education(filepath)))\n    total['religion'].reset_index(inplace=True, drop=True)\n    total['education'].reset_index(inplace=True, drop=True)\n\n    for df in total.values():\n        df.loc[(df['region'] == \"ARMM\"), 'region'] = 'BARMM'\n        df.loc[(df['region'] == \"MIMAROPA\"), 'region'] = 'REGION IV-B'\n        df.loc[(df['region'] == \"CARAGA\"), 'region'] = 'REGION XIII'\n    return total\n\n\n\n\nCode to execute get_census_education amd get_census_religion\ncensus_path = '/mnt/data/public/census/*'\ncensus_dict = read_census_files(census_path)\n\n\n\nStep 6.1: Is literacy rate related to voting preference?\n\nWe looked at the number of seats obtained by each coalition per region, then correlated it to literacy rate. Education information was obtained from the 2015 Census data.\nWe looked at the literacy rate \\(\\frac{n_{educated}}{n_{total}}\\) of each administrative region:\n\n\nFunction for get_education_percent()\ndef get_education_percent():\n    '''\n    Gets percentage of educational level per region\n    '''\n    education = census_dict['education'].groupby(\n        ['region', 'education'], as_index=False).sum()\n    education = pd.pivot_table(\n        education, index='region', columns='education', values='total')\n    education.columns = ['education_pct_' + educ for educ in education.columns]\n    education['education_total'] = education.sum(axis=1)\n    for educ in education.columns[:-1]:\n        education[educ] /= education['education_total']\n    education.drop('education_total', axis=1, inplace=True)\n    education = education.round(5)\n\n    return education\n\n\n\n\nFunction for get_agg_education_percent()\ndef get_agg_education_percent():\n    '''\n    Gets aggregated percentage of educational level per region\n    '''\n    df_educ = get_education_percent()\n    df_educ = df_educ.reset_index()\n    df_educ['educated'] = (1 - df_educ['education_pct_No Grade Completed']\n                           - df_educ['education_pct_Not Stated'])\n    df_educ['not_educated'] = df_educ['education_pct_No Grade Completed']\n    df_educ['unknown'] = df_educ['education_pct_Not Stated']\n    df_educ.drop(columns=['education_pct_Academic Degree Holder',\n                          'education_pct_College Undergraduate',\n                          'education_pct_Elementary',\n                          'education_pct_High School',\n                          'education_pct_No Grade Completed',\n                          'education_pct_Not Stated',\n                          'education_pct_Post Baccalaureate',\n                          'education_pct_Post-Secondary',\n                          'education_pct_Pre-School',\n                          'education_pct_Special Education'], inplace=True)\n    df_educ.set_index('region', inplace=True)\n    df_educ.reset_index(inplace=True)\n    return df_educ\n\n\n\n\nCode to run get_agg_education_percent()\ndf_educ = get_agg_education_percent()\ndf_educ\n\n\nThe output table shows the literacy table per region. The main columns shows the percentage that are educated, not_educated and unknown.\n\n\n\nregion\neducated\nnot_educated\nunknown\n\n\n\n\nBARMM\n0.85355\n0.14105\n0.00540\n\n\nCAR\n0.97376\n0.02622\n0.00002\n\n\nNCR\n0.99234\n0.00221\n0.00545\n\n\nNIR\n0.98085\n0.01882\n0.00033\n\n\nREGION I\n0.99321\n0.00679\n0.00000\n\n\nREGION II\n0.98610\n0.01389\n0.00001\n\n\nREGION III\n0.99355\n0.00615\n0.00030\n\n\nREGION IV-A\n0.99473\n0.00519\n0.00008\n\n\nREGION IV-B\n0.96573\n0.03417\n0.00010\n\n\nREGION IX\n0.96456\n0.03486\n0.00058\n\n\nREGION V\n0.99126\n0.00851\n0.00023\n\n\nREGION VI\n0.98721\n0.01257\n0.00022\n\n\nREGION VII\n0.98753\n0.01209\n0.00038\n\n\nREGION VIII\n0.97679\n0.02297\n0.00024\n\n\nREGION X\n0.98095\n0.01856\n0.00049\n\n\nREGION XI\n0.97818\n0.02050\n0.00132\n\n\nREGION XII\n0.94792\n0.05034\n0.00174\n\n\nREGION XIII\n0.98487\n0.01503\n0.00010\n\n\n\nWe then checked if the number of seats obtained by each coalition is correlated to the literacy rate of that region. First, we obtained the number of seats obtained by each coalition per region:\n\n\nFunction for get_coalition_rank()\ndef get_coalition_rank():\n    \"\"\"\n    Get number of seats obtained by each coalition\n    \"\"\"\n    coalition_seats = nle2019.query('position == 1')\n    coalition_seats = coalition_seats.groupby(\n        ['region', 'candidate_name', 'coalition'], as_index=False).agg({'votes_per_province': sum})\n    coalition_seats = coalition_seats.sort_values(\n        by=['region', 'votes_per_province'], ascending=[1, 0])\n    coalition_seats.set_index('region', inplace=True)\n\n    coalition_seats['rank'] = list(range(1, len(\n        coalition_seats.candidate_name.unique())+1)) * len(coalition_seats.index.unique())\n    coalition_seats['is_top_12'] = 1*(coalition_seats['rank'] &lt;= 12)\n    coalition_seats.reset_index(inplace=True)\n\n    coalition_seats = pd.pivot_table(\n        coalition_seats, index='region', columns='coalition', values='is_top_12', aggfunc=np.sum)\n    return coalition_seats\n\n\nWe then merge this with the education dataframe, then get the correlation by using python’s corr method:\n\n\nCode to run get_agg_education_percent() and create a a correlation graph.\ndf_coal_educ = get_agg_education_percent().set_index('region').join(get_coalition_rank())\ndf_coal_educ['Educated'] = df_coal_educ['educated']\ndf_coal_educ['Not Educated'] = df_coal_educ['not_educated']\ncorr = df_coal_educ.corr().loc[['Otso Diretso', 'HNP'],['Educated', 'Not Educated']]\ncolormap = sns.diverging_palette(100, 100, n = 10)\nsns.heatmap(corr, cmap=colormap, annot=True, vmin = -1, vmax = 1);\n\n\n\n\n\nFIGURE: Correlation Matrix between Education and Coalition\n\n\nThe Census and election data show that the voting preference of a region has no correlation with its literacy rate. We now look at religion to see if it has a correlation with the voting preference.\n\nStep 6.2: Is literacy rate related to voting preference?\n\nWe also looked into the religious affiliation per region, and checked if it is correlatd with voting preference.\nFirst, we obtained the distribution of religions per region from the Census data:\n\n\nFunction for get_religion_percent()\ndef get_religion_percent():\n    \"\"\"\n    Get percentages of religion per region\n    \"\"\"\n    religion = census_dict['religion'].groupby(\n        ['region', 'religion'], as_index=False).sum()\n    religion = pd.pivot_table(\n        religion, index='region', columns='religion', values='total')\n    religion.columns = ['religion_pct_' + rel for rel in religion.columns]\n    religion['religion_total'] = religion.sum(axis=1)\n    for rel in religion.columns[:-1]:\n        religion[rel] /= religion['religion_total']\n    religion.drop('religion_total', axis=1, inplace=True)\n    religion = religion.round(5)\n    return religion\n\n\ndf_rel = get_religion_percent()\n\n\nWe then merged the religion census data with the coalition ranking data to check if religion has correlation with the number of seats obtained by each coalition:\n\n\nJoining the coalition rank dataframe with the output from the get_religion_percent() function.\ndf_coal_rel = get_religion_percent().join(get_coalition_rank())\n\n\nFrom getting the correlation of the religion data with the number of seats per coalition, it is apparent that the Voting preference of a region has no correlation with its religious affiliation.\n\n\nCode to get a correlation graph between Religion and Top Coalition Party.\ndf_coal_rel['Roman Catholic'] = df_coal_rel['religion_pct_Roman Catholic, including Catholic Charismatic']\ndf_coal_rel['Islam'] = df_coal_rel['religion_pct_Islam']\ndf_coal_rel['Iglesia ni Cristo'] = df_coal_rel['religion_pct_Iglesia ni Cristo']\ncorr = df_coal_rel.corr().loc[\n    ['Roman Catholic','Islam','Iglesia ni Cristo'],['HNP','Otso Diretso']]\ncolormap = sns.diverging_palette(100, 100, n = 10)\nsns.heatmap(corr, cmap=colormap, annot=True, vmin = -1, vmax = 1);\n\n\n\n\n\nFIGURE: Correlation Matrix between Religion and Coalition\n\n\n\nConclusion\n\nUpon checking both the Comelec 2019 Election Results and the 2015 Philippine Census data, we found out that voting preference is characterized by high regionality.\nCandidates have a homecourt advantage, and voters tend to vote candidates or parties affiliated with their home region.\nAlso, literacy rate and religious affiliation is not correlated to voting preference.\n\nImages: https://primer.com.ph/blog/2016/02/04/philippine-elections-the-culture-the-drama-the-battle-2/"
  },
  {
    "objectID": "posts/2023-04-17-Chess/index.html",
    "href": "posts/2023-04-17-Chess/index.html",
    "title": "An analysis of the opening moves of the highest-ranked chess players",
    "section": "",
    "text": "Executive Summary\n\n\nOriginal creation and submission for this notebook was last May 29, 2019.\nThis jupyter notebook was created for my Data Mining and Wrangling class for my Masters in Data Science program in the Asian Institute of Management. In particular, this was done during the 2nd semester for a class - Data Mining and Wrangling - as one of the core requirements. In this report, we shall do an analysis on opening chess moves through exploratory data analysis. In this exercise, we extracted the data from ChessGames.com using the modules Beautifulsoup, requests, and re Python modules, then stored the data in an sqlite database. Results show that the Sicilian, Najdorf (ECO = B90) opening was used in nearly 2.6% of the total games won by the top players. However, looking at the individual players, there is no predominant opening move, with only 7 of the 30 players having the Sicilian, Najdorf move as their top winning opening move.\n\n\nAcknowledgements\n\nThis analysis was done together with my Lab partner, George Esleta. I would like to thank Prof Christian Alis and the ACCeSS Laboratory for the access to the high-performance computing facility.\n\nA. Introduction and the Problem Statement\n\nChess is a two player strategy game played on a checked 8 by 8 board. The 8 by 8 grid consists of 64 squares where each player is located at opposite ends of the checked board. At the beginning of the game, each player assigned a color of either Black or White. The color scheme indicates which player will move first, with the player with the white pieces moving first. Moves are alternating between players, with each player allowed to only move one chess piece each round.\nEach player is given six unique types of pieces with varying numbers, with each unique piece moving differently across the board. Pieces may move to squares occupied by another chess piece. If the player moves into a square occupied by an opponent’s piece, then the opponent’s piece is captured and taken off the chessboard. The purpose of the game is to capture the opponent’s king, signaling a checkmate. Each game is then concluded with either of three scenarios: “1–0” means White won, “0–1” means Black won, and “½–½” indicates a draw (also known as a stalemate).\nThe first moves in any chess game, also known as a player’s opening move, are the most crucial as it sets the tone, foundation and area of control of the players. The rationale behind this rests on the assumption that the first few moves will influence a player’s probability of winning. The objectives of the opening moves allow each player needs to gain dominance of the grid by: (1) getting most of their pieces out of their default positions as this allows more possible moves), and (2) getting control of the centre of the board (as this allows better dominance).\nWith the assumption the player’s opening moves may play influential role to the player’s grid control and outcome, the paper will explore the top ranked players opening moves. The purpose of this paper is to determine if there is a specific opening move that each of the top-ranking players use.\n\nB. Methodology\n\n\nPre-requisites: Load Requirement Package\n\nBefore anything else, let us first load all important modules for this exercise.\n\n\nLoading required modules\n# These are the standard imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n\n%matplotlib inline\n\n# This is to allow a code fold in jupyter notebook\nfrom IPython.display import HTML\nHTML('''&lt;script&gt;\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n\n  $( document ).ready(function(){\n    code_shown=false;\n    $('div.input').hide()\n  });\n&lt;/script&gt;\n&lt;form action=\"javascript:code_toggle()\"&gt;&lt;input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"&gt;&lt;/form&gt;''')\n\n\nAdditionally, we should create a sqlite3 database for where we will store the data what we will scrape. For that, we shall use the sqlite3 python module.\n\n\nImport the SQLite3 moule then create an empty database called chessgames.\nimport sqlite3\nconn = sqlite3.connect('chessgames.db')\n\n\nNext, since we will be doing some web scraping, which may want to set our proxy and headers. A proxy server can help a scraper avoid IP address blocking, access geographically restricted content, facilitate high-volume scraping, and avoid detection. Headers in web scraping are a part of the HTTP request that provides information about the client making the request. They are important because they can affect the response received from the server. Some websites may block or restrict access to content based on the header information. To avoid being detected as a bot or being blocked by the server, it is important to ensure that the headers used in web scraping are appropriate and mimic those of a real user.\n\n\nEdit our Proxy and Heading\n# Setting of proxy\nos.environ['HTTP_PROXY'] = 'http://13.115.147.132:8080'\nos.environ['HTTPS_PROXY'] = 'http://13.115.147.132:8080'\n\n# Setting of header\nheader = {'''accept: text/html,application/xhtml+xml,application/xml;q=0.9,\n            image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\n            accept-Encoding: gzip, deflate\n            accept-Language: en-US,en;q=0.9\n            cache-Control: max-age=0\n            connection: keep-alive\n            host: www.chessgames.com\n            referer: http://www.chessgames.com/perl/chess.pl?page=16&pid=14125\n            &playercomp=either&year=2010&yearcomp=ge\n            upgrade-Insecure-Requests: 1\n            user-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \n            (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'''}\n\n\n\nStep 1: Understand the Data and Conduct Web Scraping Tool\n\nTo analyze the top opening moves of the highest-rated chess players, web data extraction was performed on the pages of ChessGames.com. We focused on two entities, namely the chess players (players) and their games (games). Data for both entities were saved to an SQLlite database (chessgames.db).\nThe players table was obtained by scraping all web pages under the Chess Players Directory http://www.chessgames.com/directory/. Each page in this directory corresponds to a letter of the alphabet (e.g., http://www.chessgames.com/directory/A. Each player has the following information:\n\nTable 1: player table fields\n\n\n\n\n\n\n\n\n\n\nField\nDescription\nLong Description\nData Type\n\n\n\n\npid\nPlayer ID\nindentification number\ninteger\n\n\nlname\nLast Name\nplayer’s last name\nvarchar\n\n\nfname\nFirst Name\nplayer’s first name\nvarchar\n\n\nrating\nRating\nhighest rating achieved in the database\ninteger\n\n\nstart_year\nStart Year\nplayer’s starting year\ninteger\n\n\nend_year\nEnd Year\nplayer’s ending year\ninteger\n\n\ngame_count\nNumber of games\nnumber of games in database\ninteger\n\n\n\nThe get_player_info method was used to scrape the player information (seen below).\n\n\nFunction description for get_player_info()\ndef get_player_info(url):\n    '''\n    Scrapes player info from the specified URL.\n\n    Parameter\n    ---------\n    URL : URL of player page\n\n    Return\n    ------\n    list of tuples (pid, lname, fname, rating, start_year, end_year, \n    game_count)\n    '''\n    players_list = []\n    resp = requests.get(url, headers=headers)\n    time.sleep(1)\n    print(\"\\tStatus code: \", resp.status_code)\n    resp_soup = BeautifulSoup(resp.text, 'lxml')\n    players = resp_soup.select('tr[bgcolor=\"#FFFFFF\"],tr[bgcolor=\"#FFEEDD\"]')\n    for player in players:\n        data = player.select('td')\n        rating = data[0].text.strip()\n        name = data[2].text.split(',')\n        if len(name) == 2:\n            fname = name[1].strip()\n        else:\n            fname = None\n        lname = name[0].strip()\n        years = data[3].text.strip()\n        game_count = data[4].text.strip()\n        start_year = re.match('(\\d{4})?-?(\\d{4})', years).group(1)\n        end_year = re.match('(\\d{4})?-?(\\d{4})', years).group(2)\n\n        url = str(player.select('a')[-1])\n        pid = re.match('.*?pid=(\\d+)', url).group(1)\n\n        tup = (pid, lname, fname, rating, start_year, end_year, game_count)\n        print('\\t', tup)\n        players_list.append(tup)\n    return players_list\n\n\nFor the get_player_info function, the Requests module is a Python library used for making HTTP requests. We can use basic methods such as the GET, POST, PUT, DELETE, and others. The module also provides support for handling cookies, adding custom headers, and handling redirects. We also used the BeautifulSoup Python library, which is a typical package in parsing HTML and XML documents. The package parses the pased HTML source code into a parsed tree, which can be easily traversed. Finally, we used the re Python package to utilize Regular Expression for easier string matching.\n Next, the players data are then inserted to the players table using the insert_players method:\n\n\nFunction description for insert_players()\ndef insert_players(conn):\n    '''\n    Inserts players into the players table\n\n    Parameter\n    ---------\n    conn : sqlite connection\n    '''\n    cur = conn.cursor()\n    for char in string.ascii_uppercase:\n        url = \"http://www.chessgames.com/directory/\" + char + \".html\"\n        print(url)\n        players = get_player_info(url)\n        cur.executemany('''INSERT INTO players \n                            VALUES (?, ?, ?, ?, ?, ?, ?);''', players)\n        conn.commit()\n\n\nHere, we access the enter the created database, then Insert the players and associated metadata into the database.\n Next, this study will focus on the games of the thirty (30) highest-rated players. The ranking was based on the rating provided by the website. To extract the games of these players, we first obtained their player IDs (pid) by using the pandas method read_sql. Here we can pass a SQL statement:\n\n\nMethod to query the database\ndf_players = pd.read_sql(\"\"\"SELECT pid, fname || ' ' || lname, rating, \n                                    game_count \n                            FROM players\n                            WHERE rating != ''\n                            ORDER BY rating DESC\"\"\", conn)\ndf_players.columns = ['Player ID', 'Name', 'Rating', 'Number of Games']\ndf_players.head(30)\n\n\n\nTable 2: Top 30 chess players based on Rating\n\n\nPlayer ID\nName\nRating\nNumber of Games\n\n\n\n\n52948\nMagnus CARLSEN\n2882\n3,016\n\n\n15940\nGarry KASPAROV\n2851\n2,385\n\n\n76172\nFabiano CARUANA\n2844\n1,891\n\n\n17316\nLevon ARONIAN\n2830\n2,708\n\n\n95915\nWesley SO\n2822\n1,400\n\n\n56798\nMaxime VACHIER-LAGRAVE\n2819\n2,369\n\n\n12088\nViswanathan ANAND\n2817\n3,542\n\n\n12295\nVladimir KRAMNIK\n2817\n3,026\n\n\n12089\nVeselin TOPALOV\n2816\n2,278\n\n\n50065\nShakhriyar MAMEDYAROV\n2814\n2,254\n\n\n10084\nHikaru NAKAMURA\n2814\n2,424\n\n\n17279\nAlexander GRISCHUK\n2797\n2,586\n\n\n52629\nDing LIREN\n2797\n920\n\n\n107252\nAnish GIRI\n2793\n1,522\n\n\n49796\nTeimour RADJABOV\n2793\n1,746\n\n\n54535\nSergey KARJAKIN\n2788\n2,399\n\n\n11719\nAlexander MOROZEVICH\n2788\n1,847\n\n\n12183\nVassily IVANCHUK\n2787\n3,752\n\n\n19233\nRobert James FISCHER\n2785\n1,052\n\n\n20719\nAnatoly KARPOV\n2780\n3,609\n\n\n13847\nBoris GELFAND\n2777\n3,014\n\n\n79968\nPeter SVIDLER\n2769\n2,786\n\n\n49080\nLeinier Dominguez PEREZ\n2768\n1,342\n\n\n12109\nRuslan PONOMARIOV\n2768\n1,989\n\n\n54683\nIan NEPOMNIACHTCHI\n2767\n1,614\n\n\n49246\nPentala HARIKRISHNA\n2766\n1,442\n\n\n49456\nPavel ELJANOV\n2765\n1,409\n\n\n15874\nGata KAMSKY\n2763\n1,889\n\n\n12290\nPeter LEKO\n2763\n2,364\n\n\n112240\nYu YANGYI\n2762\n991\n\n\n\nThe games table was then populated by web scraping http://www.chessgames.com/perl/chessplayer?pid=&lt;pid&gt; and iterating over the top 30 player IDs. The following fields were extracted for each game:\n\nTable 3: games table fields\n\n\nField\nDescription\nData Type\n\n\n\n\ngid\nGame ID\ninteger\n\n\nwhite_pid\nWhite Player ID\nint\n\n\nblack_pid\nBlack Player ID\nint\n\n\nresult\nResult\nvarchar\n\n\nmoves\nNumber of moves\ninteger\n\n\nyear\nYear\ninteger\n\n\ntournament\nTournament Name\nvarchar\n\n\neco\nEncyclopaedia of Chess Openings\nvarchar\n\n\nopening_move\nOpening move\nvarchar\n\n\n\nThe get_players_games function was implemented to scrape the game data for a given Player ID pid and page number page_start. This writes the games data of the player to a CSV file (&lt;pid&gt;.csv):\n\n\nFunction description for the get_players_games()\ndef get_player_games(pid, page_start):\n    \"\"\"\n    Web scrapes the games list for a player and writes it to a CSV \n\n    Parameters:\n    -----------\n    pid : player ID\n    page_start : starting page\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    url = 'http://www.chessgames.com/perl/chessplayer?pid=' + str(pid)\n    resp = requests.get(url, headers=headers)\n    print('pid = ', pid, '\\turl = ', url, '\\tcode = ', resp.status_code)\n    time.sleep(np.random.randint(1, 2))\n    soup = BeautifulSoup(resp.text, 'lxml')\n    div_page_count = soup.select(\n        'td[background$=\"/chessimages/table_stripes.gif\"]')\n    page_count = int(re.findall('of (\\d+)\\;', div_page_count[0].text)[0])\n\n    with open(f'{pid}.csv', 'a') as file:\n        csv_writer = csv.writer(file, delimiter=',', quotechar='\"')\n        for page in range(page_start, page_count+1):\n            page_url = 'http://www.chessgames.com/perl/chess.pl?page=' + \\\n                str(page) + '&pid=' + str(pid)\n            page_resp = requests.get(page_url, headers=headers)\n            print('\\tpage = ', page, '\\turl = ', page_url,\n                  '\\tcode = ', page_resp.status_code)\n            time.sleep(np.random.randint(1, 2))\n            page_soup = BeautifulSoup(page_resp.text, 'lxml')\n            games = page_soup.select(\n                'tr[bgcolor=\"#FFFFFF\"],tr[bgcolor=\"#EEDDCC\"]')\n\n            for game in games:\n                data = game.select('td')\n                game_url = data[0].find(\"a\")['href']\n                game_id = re.findall('(\\d+)', game_url)[0]\n                result = data[2].text.strip()\n                moves = data[3].text.strip()\n                year = data[4].text.strip()\n                tournament = data[5].text.strip()\n                eco = data[6].select('a')[0].text.strip()\n                opening_move = re.findall(\n                    '^[A-E0-9][0-9]{2} (.*)', data[6].text.strip())[0]\n\n                game_resp = requests.get(\n                    'http://www.chessgames.com' + game_url, headers=headers)\n                time.sleep(np.random.randint(1, 2))\n                game_soup = BeautifulSoup(game_resp.text, 'lxml')\n                players = game_soup.select('center')[0].select('a')\n                try:\n                    white_id = re.findall('(\\d+)', players[0]['href'])[0]\n                except:\n                    white_id = None\n                try:\n                    black_id = re.findall('(\\d+)', players[1]['href'])[0]\n                except:\n                    black_id = None\n                tup = (game_id, white_id, black_id, result,\n                       moves, year, tournament, eco, opening_move)\n\n                try:\n                    csv_writer.writerow(tup)\n                except:\n                    print('\\t\\tgameID: ', game_id, '\\tWrite to CSV failed')\n\n\nAll player csv files were then inserted to the games table of the chessgames.db database using the read_csvs function:\n\n\nFunction description for read_csvs()\ndef read_csvs(conn):\n    \"\"\"\n    Read all player csvs and save them to the games table\n    \"\"\"\n    gid_failed = []\n    cur = conn.cursor()\n    for file_name in glob.glob('./games/*.csv'):\n        with open(file_name) as file:\n            print(file_name)\n            reader = csv.reader(file, delimiter = ',')\n            for line in reader:\n                try:\n                    cur.execute('''INSERT INTO games VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);''', line)\n                    print('\\t\\tgameID: ', line[0], '\\tInsert success!')\n                except Exception as e:\n                    gid_failed.append(line[0])\n                    print('\\t\\tgameID: ', line[0], '\\tInsert Failed!\\t', e)\n    return gid_failed\n\n\nNow, let us examine the games dataframe by calling it with the read_sql Pandas module.\n\n\nAccessing the database.\ndf_games = pd.read_sql(\"\"\"SELECT * FROM games\"\"\", conn)\ndf_games.columns = (['Game ID', 'White Player ID', 'Black Player ID',\n                     'Result', 'Number of Moves', 'Year', 'Tournament', 'ECO',\n                     'Opening Move'])\n\n\nBelow you can see that Table 4 and Table 5 shows the summary statistics for the players and games table. 8,574 players and 50,087 games were inserted into the chessgames database.\n\nTable 4: players table summary statistics\n\n\nStatistic\nRating\n\n\n\n\ncount\n8574.000000\n\n\nmean\n2361.279566\n\n\nstd\n184.344627\n\n\nmin\n1379.000000\n\n\n25%\n2268.000000\n\n\n50%\n2389.000000\n\n\n75%\n2479.000000\n\n\nmax\n2882.000000\n\n\n\n\nTable 5: games table summary statistics\n\n\nStatistic\nNumber of Moves\nYear\n\n\n\n\ncount\n50087.000000\n50087.000000\n\n\nmean\n42.893485\n2005.074950\n\n\nstd\n17.288031\n11.643902\n\n\nmin\n0.000000\n1953.000000\n\n\n25%\n31.000000\n2000.000000\n\n\n50%\n41.000000\n2008.000000\n\n\n75%\n52.000000\n2014.000000\n\n\nmax\n255.000000\n2019.000000\n\n\n\n\nResults\n\nFigure 1 below shows the top 10 opening moves used by the thirty highest-ranked chess players. 356 games, or nearly 2.6% of the games won by the top players, were opened using the Sicilian, Najdorf (ECO = B90) move.\nHowever, looking at the top opening moves per player as seen in Figure 3, the Sicilian, Najdorf (ECO = B90) move does not appear as the top move for most of the players. For instance, Magnus Carlsen, the highest-ranked player, has the Ruy Lopez, Berlin Defense as his top opening move. As shown in Table 7, Only 7 of the top 30 players have the Sicilian, Najdorf move as their top winning opening move. Also, if we look at the top 5 highest-rated players, 3 of 5 of them have the Ruy Lopez, Berlin Defense move as the top winning move.\n\n\nFunction to see top 10 opening moves and % won.\ndf_top_moves = pd.read_sql(\"\"\"SELECT g.white_pid, p.fname || ' ' || \n                            p.lname, p.rating, g.eco, g.opening_move, count(*) \n                            FROM games g\n                          INNER JOIN players p on g.white_pid = p.pid\n                          WHERE g.white_pid IN (SELECT pid FROM players \n                                              WHERE rating != ''\n                                              ORDER BY rating DESC limit 30)\n                          AND g.result = '1-0'\n                          GROUP BY g.white_pid, g.eco, g.opening_move\n                          ORDER BY p.rating DESC, count(*) DESC\"\"\", conn)\ndf_top_moves.columns = ['PID', 'Name', 'Rating',\n                        'ECO', 'Top Opening Move', 'Number of Games Won']\ndf_top_moves = df_top_moves.groupby(['ECO', 'Top Opening Move'])[\n    'Number of Games Won'].sum().to_frame()\ndf_top_moves = df_top_moves.sort_values('Number of Games Won', ascending=False)\ndf_top_moves.reset_index(inplace=True)\ndf_top_moves['% of Games Won'] = (\n    df_top_moves['Number of Games Won'] / df_top_moves['Number of Games Won'].sum() * 100)\ndf_top_moves.head(10)\n\n\n\nTable 6: Top Opening Moves of the 30 highest-rated players\n\n\n\n\n\n\n\n\nECO\nTop Opening Move\nNumber of Games Won\n% of Games Won\n\n\n\n\nB90\nSicilian, Najdorf\n356\n2.566321\n\n\nA07\nKing’s Indian Attack\n250\n1.802191\n\n\nB12\nCaro-Kann Defense\n227\n1.63639\n\n\nC78\nRuy Lopez\n206\n1.485006\n\n\nC42\nPetrov Defense\n205\n1.477797\n\n\nC11\nFrench\n204\n1.470588\n\n\nE15\nQueen’s Indian\n203\n1.463379\n\n\nA04\nReti Opening\n192\n1.384083\n\n\nD37\nQueen’s Gambit Declined\n186\n1.34083\n\n\nC65\nRuy Lopez, Berlin Defense\n176\n1.268743\n\n\n\n\n\nCode to generate the Top Opening Moves.\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(dpi=150)\ndf_top_moves[10::-1].plot.barh('Top Opening Move',\n                               '% of Games Won', ax=ax, color='#BF5209',\n                               legend=False)\nax.set_xlabel('% of Games Won')\n\n\n\n\n\nFIGURE 1:Top Opening Moves of the 30 highest-rated chess players\n\n\nNow, let us view the top opening moves per top player (top 10 players)\n\n\nCode to generate the Top Opening Moves for top players\ndf_top_moves = pd.read_sql(\"\"\"SELECT g.white_pid, p.fname || ' ' || \n                            p.lname, p.rating, g.eco, g.opening_move, count(*) \n                            FROM games g\n                          INNER JOIN players p on g.white_pid = p.pid\n                          WHERE g.white_pid IN (SELECT pid FROM players \n                                              WHERE rating != ''\n                                              ORDER BY rating DESC limit 30)\n                          AND g.result = '1-0'\n                          GROUP BY g.white_pid, g.eco\n                          ORDER BY p.rating DESC, count(*) DESC\"\"\", conn)\ndf_top_moves.columns = ['PID', 'Name', 'Rating', 'ECO', 'Top Opening Move', 'Number of Games Won']\ndf_top_moves = (df_top_moves\n                .groupby(['Rating', 'PID', 'Name', 'ECO', 'Top Opening Move'])\n                ['Number of Games Won'].sum().to_frame())\ndf_top_moves = (df_top_moves.reset_index()\n                .sort_values(['Rating', 'Number of Games Won'],\n                             ascending=[False, False])\n                .set_index(['PID', 'Rating', 'Name', 'ECO', 'Top Opening Move']))\ndf_top_moves = df_top_moves.groupby(level=0).head(10)\ndf_top_moves.reset_index(inplace=True)\ndf_top_moves\n\nnames = df_top_moves['Name'].unique()\n\nfor name in names:\n    fig, ax = plt.subplots()\n    df = df_top_moves[df_top_moves['Name'] == name]\n    df[10::-1].plot.barh('Top Opening Move', 'Number of Games Won',\n                         ax=ax, color='#BF5209', legend=False)\n    ax.set_title(name)\n    ax.set_xlabel('Number of Games Won')\n\n\n\n\n\n\n \n\n\n\n\n \n\n\n\n\n \n\n\n\n\n \n\n\n\n\n \n\n\nFigure 1: Top Winning Moves Per Top Player\n\n\n\nConclusions and Recommendations\n\nThe results indicate that the Sicilian, Najdorf is the most used opening move by the thirty top-ranked players, with 356 games or nearly 2.6% of the games were won using this move. However, looking at the individual opening moves of each top-ranked players, there is no predominant opening move, with only 7 of the 30 players have the Sicilian, Najdorf move as their top winning opening move. Additionally, looking at the top five players, the Ruy Lopez, Berlin Defense was the most frequent opening move.\nThe methodology of the study does not take into account the total number of games each player competed in. Certain top ranked players have higher number of competition appearances. This may affect the number of instances the Sicilian, Najdorf may appear due to playing style bias. Further research may be conducted to explore normalizing the data by the number of games each top-player appeared in and won. Also, we can explore why the top-ranked players have a preference towards the Ruy Lopez, Berlin Defense.\n\nReferences\n\n\n\n\nChessGames.com\n\nChess Strategy for Chess Openings and Chess Principles\n\nChess Tournament Rules Regulations Fide Approved\n\nTen Rules Openings"
  },
  {
    "objectID": "posts/2023-04-20-Reddit/index.html",
    "href": "posts/2023-04-20-Reddit/index.html",
    "title": "Unraveling Subreddits",
    "section": "",
    "text": "Executive Summary\n\n\nOriginal creation and submission for this notebook was last June 2019.\nReddit is an American discussion and aggregation website for user-generated content with more than 500 million monthly visitors. We uncovered the underlying categories or “subreddits” by performing representative-based clustering on a sample of Reddit titles. Results reveal that Reddit is a very American-centric platform and that the two main themes are (a) U.S. politics, and (b) seeking help from other Redditors on a wide range of topics including technology, food, and legal matters. For a small number of clusters (\\(n=10\\)), the high-level categories that emerged include the 2016 U.S. Presidential Elections (Donald Trump, Hillary Clinton, and Bernie Sanders), food, legal concerns, and “New year”-related subreddits. Increasing the number of clusters to \\(n=16\\) revealed more specific subreddits such as technical support, recipes, apartment/rental concerns, and reaction compilations.\n\n\nAcknowledgements\n\nThis analysis was done together with my Lab partner, George Esleta, and Cohortmates - Gilbert Chua, Nigel Silva and Oonre Advincula-Go.\n\nA. Introduction and the Problem Statement\n\nReddit is a discussion and content aggregation website. As of June 2019, it is ranked as the #5 most visited website in the United States and #15 worldwide. Registered members, also called “Redditors”, upload user-generated content (UGC) which are either voted up or voted down by other Redditors. Reddit posts are grouped into user-created boards or communities called “subreddits”, with each subreddit having a specific topic such as food, entertainment, and politics.\n\nB. Methodology\n\nReddit titles and authors were scraped from the Reddit website and stored in a text file. The file is tab-delimited and has two columns: author and title. Flow of this notebook is as follows:\n\n\nPre-requisites: Load Requirement Package\n\nReading the Reddit data\n\nInitial data cleaning\n\nExploratory data analysis\n\nVectorization (bag-of-words representation) using TFIDF Vectorizer\n\nDimensionality reduction using Latent Semantic Analysis (LSA)\n\nRepresentative-based clustering using k-means\n\n\nPre-requisites: Load Requirement Package\n\nBefore anything else, let us first load all important modules for this exercise.\n\n\nLoading required modules\n# These are standard imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom collections import Counter\nfrom IPython.display import display_html\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import calinski_harabaz_score, silhouette_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n    from sklearn.metrics import confusion_matrix\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist, RegexpTokenizer\n\nfrom IPython.display import HTML\nfrom scipy.spatial.distance import euclidean\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import euclidean, cityblock\n\nimport urllib\nimport requests\nimport pprint\nfrom PIL import Image\n\npp = pprint.PrettyPrinter(indent=4)\n\nHTML('''&lt;script&gt;\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n&lt;/script&gt;\n&lt;form action=\"javascript:code_toggle()\"&gt;&lt;input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"&gt;&lt;/form&gt;''')\n\n\n\nReading the Reddit data\n\nThe data was stored in the ACCESS lab cloud computer (JOJIE). So for this instance, the sample Reddit text file was read using the read_csv function of pandas. In reading the file, tab (\\t) separator was used, and the first line was skipped.\n\n\nLoad Reddit data\ndf = pd.read_csv('reddit-dmw-sample.txt', sep='\\t', skiprows=1, header=None, \n                 usecols = [1, 2], \n                 names = ['author', 'title'],)\n\n\nThe resulting dataframe has two columns: author and title. A sample of the Reddit topics are shown below.\n\nTale 1: Sample Reddits from the Data\n\n\nauthor\ntitle\n\n\n\n\nPrimotechInc\n7 Interesting Hidden Features of apple ios9\n\n\nxvagabondx\nNeed an advice on gaming laptop\n\n\nnkindustries\nSemi automatic ROPP Capping machine / ROPP Cap…\n\n\nPhilo1927\nMicrosoft Plumbs Ocean’s Depths to Test Underw…\n\n\ntuyetnt171\nOPPO F1 chính hãng - Fptshop.com.vn\n\n\n\n\nInitial data cleaning\n\nOnce the Reddit data is stored in a dataframe, initial data cleaning was performed by converting all titles to lowercase, and removing leading and trailing spaces. Duplicate titles were also dropped.\nThe unique titles are then stored in a list:\n\n\nViewing the unique titles\ndf['title'] = df['title'].str.lower()\ndf['title'] = df['title'].str.strip()\ndocs = list(df.title.unique())\npp.pprint('Number of titles: ' + str(len(docs)))\n\n\n'Number of titles: 5848'\nAfter removing duplicates, there are 5848 unique Reddit titles in the sample dataset.\nSample cleaned topics are shown below. Note that there still are punctuation marks and special characters. These will be removed during vectorization.\n[   '7 interesting hidden features of apple ios9',\n    'need an advice on gaming laptop',\n    'semi automatic ropp capping machine / ropp cap sealing machine',\n    'microsoft plumbs ocean’s depths to test underwater data center',\n    'oppo f1 chính hãng - fptshop.com.vn',\n    'stewed ramps w/ fixings in helvatia, wv',\n    '(california) is it a crime when a religious figure/ lecturer has '\n    'relations with one of his followers',\n    'being accused of public indecency, among other things. this is a '\n    'misunderstanding because i had health issues. [kansas, usa]',\n    '[us-ca/nv] my parents have been separated for almost 20 years, my mom '\n    'found out my dad divorced her without her knowledge a few years ago when '\n    'i was about 14--can she claim backdated child support?',\n    'skyrim ps4?']\n\nExploratory Data Analysis\n\nShown below are the top 15 most used words in the Reddit sample data. Note that the most common word is “TIL” (Today I Learned), followed by the names of prominent US politicians such as US President Donald Trump, 2016 presidential candidate Hillary Clinton, and Senator Bernie Sanders. All three of them figured prominently in the 2016 U.S. presidential elections, with Trump as the presidential nominee under the Republican party, while Clinton and Sanders as the Democratic party presidential candidates. Initial analysis of the data reveal that the 2016 U.S. Presidential Election is one of the primary themes in the sample Reddit data.\n\n\nViewing the unique titles\nmask = np.array(Image.open('Icon-Reddit1.png'))\ntokenizer = RegexpTokenizer(r'\\w{2,}')\nword_tokens = tokenizer.tokenize(' '.join(docs))\nstopword = set(stopwords.words('english') + list(stop_words.ENGLISH_STOP_WORDS))\ndocs_filtered = [w for w in word_tokens if not w in stopword] \ncounts = FreqDist(docs_filtered)\nlabels, values = zip(*counts.items())\nindSort = np.argsort(values)[::]\nlabels = np.array(labels)[indSort[-15:]]\nvalues = np.array(values)[indSort[-15:]]\nindexes = np.arange(len(labels))\nbar_width = 0.35\n\nfig, ax = plt.subplots(1, 2, dpi=100, figsize = (10,4))\nax[0].barh(indexes, values, color='#ff4500')\nax[0].set_xlabel('Word count')\nax[0].set_title('Most common words in the Reddit sample data')\nax[0].set_yticks(indexes + bar_width);\nax[0].set_yticklabels(labels);\n\nwordcloud = WordCloud(background_color=\"white\", max_words=100, min_font_size=8, mask=mask, \n                      prefer_horizontal=0.6, colormap='copper', relative_scaling=.5).generate(' '.join(docs_filtered))\n\nax[1].imshow(wordcloud, interpolation='bilinear')\nax[1].axis('off');\n\n\n\n\n\nFIGURE 1: (1) Frequency distribution of the top 15 words in the Reddit sample data, and (b) the corresponding wordcloud. Top words include TIL (Today I learned) and the names of 2016 US Presidential Aspirants (Donald Trump, Bernie Sanders, Hillary Clinton)\n\n\n\nVectorization/Bag-of-words representation\n\nTo uncover the other underlying themes apart from the 2016 U.S. Presidential Election, unsupervised clustering via k-means is performed on the collection of Reddit titles. To do this, the topics need to be vectorized or be converted to their bag-of-words representation. The TfidfVectorizer vectorizer was used to perform the vectorization.\nThe TFidfVectorizer also performs further cleaning of the data by removing both frequent words and rare words. Stopwords, or words that appear too frequently in the English language (e.g., the, a, an, and, or), were dropped from the corpus. Reddit-specific stopwords, or words that appear in more than 70% of the titles, were also ignored by setting the max_df parameter of TFidfVectorizer to \\(0.7\\).\nRare words, or words that appeared in less than 0.1% of the Reddit titles, were also excluded from the corpus. This was implemented by setting the min_df parameter of the TFidfVectorizer to \\(0.001\\).\nn-grams of word lengths between \\(n=1\\) and \\(n=3\\) were extracted as part of the vocabulary. This range of n-grams was chosen in order to preserve the context of certain phrases such as “Hillary Clinton” (\\(n=2\\)), “Donald Trump” (\\(n=2\\)), and “Happy new year” (\\(n=3\\))\n\n\nUsing a TFIDF Vectorizer\n# Creating a TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b\\w(?:\\w|\\-)+\\b', \n                                   stop_words=stopword, \n                                   ngram_range=(1,3), min_df=0.001, max_df=0.7)\n\n# Creating a Bag-Of-Words\nbow = tfidf_vectorizer.fit_transform(docs)\n\n# Identifying the vocabulary\nvocabulary = tfidf_vectorizer.get_feature_names()\n\n\nAfter vectorization, the sample of Reddit titles has 1,655 unique items in its vocabulary:\nSample vocabulary items are shown below:\n[   'years old',\n    'yesterday',\n    'york',\n    'york times',\n    'young',\n    'youtube',\n    'zealand',\n    'zelda',\n    'ăn',\n    'طريقة']\n\nDimensionality reduction using Latent Semantics Analysis (LSA)\n\nThe resulting bag of words is a 5848 by 1655 matrix:\nprint(\"\\t Dimensions: \", bow.shape)\nDimensions:  (5848, 1655)\nIt would be computationally expensive to perform clustering on a dataset this large. To minimize computational power, dimensionality reduction was performed on the vectorized data via latent semantic analysis (LSA). This was implemented using the TruncatedSVD class of sklearn.\nAs a general rule for LSA, fewer dimensions allow for broader comparisons of the themes contained in a collection of text, while a higher number of dimensions enable more specific comparisons of themes. A dimesionality between 50 and 1,000 are suitable depending on the size and nature of the document collection, with \\(300 \\pm 50\\) the optimum value. Unlike in other dimensionality reduction techniques such as principal component analysis (PCA), checking the proportion of variance retained to determine the optimal dimensionality is NOT applicable to LSA.\nIn this study, \\(n_{components} = 100\\) is used to extract the underlying themes.\n\nReduced dimensionality\n\nFor the purposes of this study, we reduced the dimensionality of the Reddit sample data to 100 components. The transformed Reddit dataset is shown below. Numerous subclusters are revealed, confirming that there are multiple categories or themes in the dataset.\n\n\nUsing a TruncatedSVD to reduce the number of components to 100 components, then getting bag of words, the using TSNE\ntsvd1 = TruncatedSVD(n_components=100)\nnormalizer = Normalizer(copy=False)\nlsa1 = make_pipeline(tsvd1, normalizer)\n\n# Getting the bag of words\nbow_tsvd1 = lsa1.fit_transform(bow)\n\n# Applying a TSNE model\ntsne = TSNE(n_components=2, random_state=1704, n_iter=1500, n_iter_without_progress=500, perplexity=50, learning_rate=10)\nbow_tsne1 = tsne.fit_transform(bow_tsvd1)\n\n\nNow, let’s make a graph on the TSNE model’s bag of words. Below you’ll see it flatted TSNE of the bag of words.\n\n\nCreating the plot of the TSNE.\nfig, ax = plt.subplots(dpi = 100)\nax.scatter(bow_tsne1[:,0], bow_tsne1[:,1], alpha=0.5, label=True, marker='o', s=8, c=\"#ff4500\");\n\n\n\n\n\nFIGURE 2: Reddit topics visualized in two dimensions using TSNE.\n\n\n\nRepresentative-based clustering using k-means\n\n\n\nSelecting the optimal number of clusters k\n\n\nTo determine the optimal number of clusters, we perform k-means clustering for different values of the cluster count \\(k\\) from \\(k=2\\) to \\(k=20\\). This range of values was chosen to keep the clustering parsimonious.\nSeveral internal validation measures were computed for each value of \\(k\\). The internal validation measures that were used are (a) intracluster to intercluster distance ratio, (b) Calinski-Harabasz score (c) Silhouette coefficient, and (d) sum of square distances to centroids coefficient.\nThe optimal value of \\(k\\) is then chosen based on the following criteria:\n\nSum-of-square distance to centroid is minimized\nCalinski-Harabasz index is maximized\nIntracluster to intercluster distance ratio is minimized\nSilhouette coefficient is maximized\n\nPlotted below are the values of the internal validation criteria for values of \\(k\\) between 2 and 20. \\(k = 10\\) and \\(k = 16\\) are good candidates for the number of clusters because for these values, the intracluster to intercluster distance ratio has a local minima, and the silhouette coefficient has a relatively high value.\n\n\nFunction for computing intracluster to intercluster distance ratio\ndef intra_to_inter(X, y, dist, r):\n    \"\"\"\n    Compute intracluster to intercluster distance ratio\n    \n    Parameters\n    ----------\n    X : array\n        Design matrix with each row corresponding to a point\n    y : array\n        Class label of each point\n    dist : callable\n        Distance between two points. It should accept two arrays, each \n        corresponding to the coordinates of each point\n    r : integer\n        Number of pairs to sample\n        \n    Returns\n    -------\n    ratio : float\n        Intracluster to intercluster distance ratio\n    \"\"\"\n    P = []\n    Q = []\n    p=0\n    q=0\n    np.random.seed(11)\n    d = np.random.choice(range(0,len(X)), (r,2), replace=True)\n    for a in range(0,len(d)):\n        if  d[a][0] == d[a][1]:\n            continue\n        elif y[d[a][0]] == y[d[a][1]]:\n            P.append(dist(X[d[a][0]],X[d[a][1]]))\n            p+=1\n        else:\n            Q.append(dist(X[d[a][0]],X[d[a][1]]))\n            q+=1\n    if len(P) == 0 or len(Q) == 0:\n        ratio = 0\n    else:\n        ratio = (np.sum(P)/len(P))/(np.sum(Q)/len(Q))\n    return ratio\n\n\n\n\nFunction to create the clusters, cluster labels (and scores of internal validation values)\ndef cluster_range(X, clusterer, k_start=2, k_stop=16, actual=None):\n    \"\"\"\n    Accepts the design matrix, the clustering object, the initial and final \n    values to step through, and, optionally, actual labels. It should return a\n    dictionary of the cluster labels, internal validation values and, if\n    actual labels is given, external validation values, for every k.\n\n    Parameters\n    ----------\n    X : design matrix\n    clusterer : clustering object\n    k_start : initial value of clusters\n    k_stop : final value of clusters\n    actual : actual labels\n\n    Returns\n    -------\n    dictionary\n    \"\"\"\n\n\n    validation_dict = {\"chs\": [],\n                       \"iidrs\": [],\n                       \"inertias\": [],\n                       \"scs\": [],\n                       \"ys\": []\n                       }\n    if actual is not None:\n        validation_dict = {\"chs\": [],\n                           \"iidrs\": [],\n                           \"inertias\": [],\n                           \"scs\": [],\n                           \"ys\": [],\n                           \"amis\": [],\n                           \"ars\": [],\n                           \"ps\": []\n                           }\n    for k in range(k_start, k_stop+1):\n        clusterer.n_clusters = k\n        np.random.seed(11)\n        y_predict = clusterer.fit_predict(X)\n        validation_dict[\"chs\"].append(calinski_harabaz_score(X, y_predict))\n        validation_dict[\"iidrs\"].append(\n            intra_to_inter(X, y_predict, euclidean, 50))\n        validation_dict[\"inertias\"].append(clusterer.inertia_)\n        validation_dict[\"scs\"].append(silhouette_score(X, y_predict))\n        validation_dict[\"ys\"].append(y_predict)\n        if actual is not None:\n            validation_dict[\"amis\"].append(\n                adjusted_mutual_info_score(actual, y_predict))\n            validation_dict[\"ars\"].append(\n                adjusted_rand_score(actual, y_predict))\n            validation_dict[\"ps\"].append(purity(actual, y_predict))\n    return validation_dict\n\n\nkmeans_dict1 = cluster_range(\n    # bag of words results\n    bow_tsvd1, \n    # using a k-means clusterer\n    KMeans(random_state=1704, n_init=20, max_iter=1000, tol=1e-6), \n    2,\n    20)\n\n\nFunction to plot internal validation values\ndef plot_internal(inertias, chs, iidrs, scs):\n    \"\"\"\n    Plot internal validation values\n    \"\"\"\n    fig, ax = plt.subplots(1,4,dpi=100, figsize=(10,2))\n    ks = np.arange(2, len(inertias)+2)\n    ax[0].plot(ks, inertias, '-o', label='SSE')\n    ax[1].plot(ks, chs, '-ro', label='CH')\n    ax[0].set_xlabel('$k$')\n    ax[1].set_xlabel('$k$')\n    ax[0].set_title('SSE')\n    ax[1].set_title('CH')\n    ax[2].plot(ks, iidrs, '-go', label='Inter-intra')\n    ax[3].plot(ks, scs, '-ko', label='Silhouette coefficient')\n    ax[2].set_xlabel('$k$')\n    ax[2].set_title('Inter-intra')\n    ax[3].set_xlabel('$k$')\n    ax[3].set_title('Silhouette coefficient')\n    return ax\n\n\nLet’s plot the interal validation values\nplot_internal(kmeans_dict1['inertias'], kmeans_dict1['chs'], \n    kmeans_dict1['iidrs'], kmeans_dict1['scs']);\n\n\n\nFIGURE 3: Plots of the various internal validation measures for \\(k=2\\) to \\(k=20\\). \\(k=10\\) and \\(k=16\\) were chosen because of the low intercluster-intracluster ration and the high silhouette coefficients for those values.\n\n\n\n\nk = 10\n\n\n\n\nCode of setting k-means k=10\nn1 = 10\nkmeans1 = KMeans(n_clusters=n1, random_state=1704, n_init=20, max_iter=1000, tol=1e-6)\ny_predict_kmeans1 = kmeans1.fit_predict(bow_tsvd1)\n\n\n\\(k\\)-means clustering was performed on the Reddit sample titles using 10 clusters. The resulting clusters and the cluster size distribution are shown below. The clusters are relatives balanced in terms of cluster size, with the exception of one cluster (cluster 5) which contains more than 2500 Reddit titles.\n\n\nCreating the plot and clusters\nfig, ax = plt.subplots(1,2, dpi = 100, figsize=(10,3))\nax[0].scatter(bow_tsne1[:,0], bow_tsne1[:,1], c=y_predict_kmeans1, alpha=0.5, label=True, marker='o', s=8);\n\ncounts = Counter(y_predict_kmeans1)\nlabels, values = zip(*counts.items())\nindSort = np.argsort(values)[::]\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\nindexes = np.arange(len(labels))\nbar_width = 0.35\nax[1].barh(indexes, values, color='#ff4500')\nax[1].set_xlabel('Cluster size')\nax[1].set_ylabel('Cluster labels')\nax[1].set_yticks(indexes + bar_width);\nax[1].set_yticklabels(labels);\n\n\n\n\n\nFIGURE 4: Resulting clustering for \\(n=10\\), and the cluster size distribution per cluster. The clusters are relatives balanced in terms of cluster size, with the exception of one cluster (cluster 5) which contains more than 2500 Reddit titles.\n\n\nTo understand the theme or category of each cluster, the top words or the most frequent words for each cluster was obtained. Shown below are the top 10 most occurring words per cluster, and the corresponding wordclouds.\n\n\nCode to see the top 10 most occuring words per cluster\nfeatures = tfidf_vectorizer.get_feature_names()\nweights = np.dot(kmeans1.cluster_centers_, tsvd1.components_)\nweights = np.abs(weights)\n\nfig, ax = plt.subplots(2,5, dpi=100, figsize=(16,5))\nplt.subplots_adjust(wspace=1.1, hspace=.8)\nfor i in range(kmeans1.n_clusters):\n    indices = np.argsort(weights[i])[-10:][::]\n    top_words = [features[index] for index in indices]\n    values = weights[i,indices]\n    indices = np.arange(len(top_words))\n    bar_width = 0.35\n    ax[i//5][i%5].barh(indices, values, color='#ff4500')\n    ax[i//5][i%5].set_xlabel('Weight')\n    ax[i//5][i%5].set_ylabel('Top words')\n    ax[i//5][i%5].set_yticks(indexes + bar_width);\n    ax[i//5][i%5].set_yticklabels(top_words);\n    ax[i//5][i%5].set_title('Cluster '+ str(i+1))\n\n\n\n\n\n\nFIGURE 5: Top 10 most occuring words per cluster (\\(k=10\\))\n\n\n\n\n\nCode of placing a Reddit mask wordcloud\nmask = np.array(Image.open('Icon-Reddit1.png'))\ntopic = ['TIL', 'Apartment/Rent', 'New Year', 'Iowa Caucus', 'Food', 'Technology', 'Donald Trump', 'Bernie Sanders', 'Advise-seeking', 'Hillary Clinton']\nfig, ax = plt.subplots(2,5,dpi=100, figsize=(16,4))\nplt.subplots_adjust(wspace=.2, hspace=0)\nfor ctr in range(kmeans1.n_clusters):\n    indices = [i for i, x in enumerate(y_predict_kmeans1) if x == ctr]\n    wordcloud = WordCloud(background_color=\"white\", max_words=100, min_font_size=8, mask=mask, \n                          prefer_horizontal=0.7, colormap='copper').generate(' '.join([docs[index] for index in indices]))\n    ax[ctr//5][ctr%5].imshow(wordcloud, interpolation='bilinear')\n    ax[ctr//5][ctr%5].axis('off');\n    ax[ctr//5][ctr%5].set_title(topic[ctr]);\n\n\n\n\n\n\nFIGURE 6: Wordcloud per cluster (\\(k=10\\))\n\n\n\nUsing \\(k = 10\\), the general categories listed below were revealed. Sample Reddit titles for each category are also shown.\nThe clustering results show that Reddit is a very American-centric platform. Four of the ten topics are related to the 2016 U.S. Presidential Election, specifically on the Republican nominee Donald Trump, Democrat candidates Hillary Clinton and Bernie Sanders, and the Iowa caucuses, a biennial electoral event that marks the first major contest of the United States presidential primary season.\nAlso, two of the topics are related to “advise”. This reveals that apart from discussing politics, Reddit users go to Reddit to ask help or seek advise from other Redditors, whethere it be technology-related or legal.\nCluster topics:\n\nTIL (Today I Learned)\n\n[   'til maternal kisses are not effective in alleviating minor childhood '\n    'injuries (boo-boos): a randomized, controlled and blinded study.',\n    'til goldman sachs did a study in 2009 that estimated a unified korea '\n    'could boast an economy larger than france, germany, and even japan by '\n    '2050 with a gdp of $6 trillion.',\n    \"til the justice league's martian manhunter was once addicted to cookies \"\n    'and hulked out when deprived of them.']\n\nApartment/rent-related topics\n\n[   'business telephone service',\n    'illegal towing from apartment complex md.',\n    'can a landlord place a security camera in a communal space']\n\n“Happy new year”\n\n[   'new poll shows how far hillary has fallen with democrats',\n    'new zealand - new tenants wanting to take over internet account, isp is '\n    'refusing.',\n    'watch \"best of maskedman - 2015 new year special.\" on youtube',\n    'happy new year reddit! :) i quit retirement, spent $250k in savings and '\n    '51 months (15,000 hours) to develop the new internet, owned by the '\n    'people, powered by humanity - the first company in history equally owned '\n    'by everybody alive! we are uniting the whole world into one!',\n    'til new yorkers at nye time square wear diapers and pads for lack of '\n    'porta potties']\n\n“Iowa caucuses”\n\n[   'the gop’s condemnation of ‘sanctuary cities’ is surprisingly awkward in '\n    'iowa',\n    \"jesse watters questions sanders' young supporters in iowa\",\n    '2016 iowa democratic caucus locations - get out there and caucus!!']\n\nFood\n\n[   'i have access to tons and tons of green onion tops,what can i make?',\n    'mexican chopped salad',\n    's&amp;m restaurant in beijing caters to millennial’s basic instincts by '\n    'serving food &amp; sex']\n\nDonald Trump\n\n[   \"donald trump says he'll attract democrats; polls say that's unlikely\",\n    'inside the clintons’ plan to defeat donald trump',\n    'hecklers disrupt trump rally, photographer shoved to the ground']\n\nBernie Sanders\n\n[   '“it appears that mr. trump is getting nervous that working families are '\n    'catching on that his policies represent the interests of the billionaire '\n    'class against almost everyone else.\" - bernie sanders',\n    'killer mike: bernie sanders is the only candidate for black voters',\n    'bernie sanders: the most fascist candidate of all.']\n\nFood\n\n[   'i have access to tons and tons of green onion tops,what can i make?',\n    'mexican chopped salad',\n    's&amp;m restaurant in beijing caters to millennial’s basic instincts by '\n    'serving food &amp; sex']\n\nAdvise-seeking\n\n[   'need an advice on gaming laptop',\n    'charged with dui 2 years and 4 months after i was involved in a single '\n    'car accident.',\n    \"i'd like to buy the rights of a post on reddit to recreate in another \"\n    'medium. how do i create a legal contract for this between strangers '\n    'online?',\n    'significant online food ordering trend',\n    'md - need advice evicting a suicidal/alcoholic tenant/roommate.']\n\nHillary Clinton\n\n[   'semi automatic ropp capping machine / ropp cap sealing machine',\n    'state: hundreds of old clinton emails newly classified',\n    'georgia poll: clinton 70, sanders 23']\n\n\nk = 16\n\n\nLet’s try with a k=16\n\n\nSetting k-means to k=16\nn2 = 16\n\nkmeans2 = KMeans(n_clusters=n2, random_state=1704)\ny_predict_kmeans2 = kmeans2.fit_predict(bow_tsvd1)\n\nfig, ax = plt.subplots(1,2, dpi = 100, figsize=(10,3))\nax[0].scatter(bow_tsne1[:,0], bow_tsne1[:,1], c=y_predict_kmeans2, alpha=0.5, label=True, marker='o', s=8);\n\ncounts = Counter(y_predict_kmeans2)\nlabels, values = zip(*counts.items())\nindSort = np.argsort(values)[::]\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\nindexes = np.arange(len(labels))\nbar_width = 0.35\nax[1].barh(indexes, values, color='#ff4500')\nax[1].set_xlabel('Cluster size')\nax[1].set_ylabel('Cluster label')\nax[1].set_yticks(indexes + bar_width);\nax[1].set_yticklabels(labels);\n\n\n\n\n\nFIGURE 7. Clustering and Cluster size distribution for \\(k=16\\))\n\n\nTo understand the theme or category of each cluster, the top words or the most frequent words for each cluster was obtained. Shown below are the top 10 most occurring words per cluster, and the corresponding wordclouds.\n\n\nCode to see the top words per 16 clusters\nfeatures = tfidf_vectorizer.get_feature_names()\nweights = np.dot(kmeans2.cluster_centers_, tsvd1.components_)\nweights = np.abs(weights)\nfig, ax = plt.subplots(4,4, dpi=100, figsize=(10,10))\nplt.subplots_adjust(wspace=1.1, hspace=.8)\nfor i in range(kmeans2.n_clusters):\n    indices = np.argsort(weights[i])[-10:][::]\n    top_words = [features[index] for index in indices]\n    values = weights[i,indices]\n    indices = np.arange(len(top_words))\n    bar_width = 0.35\n    ax[i//4][i%4].barh(top_words, values, color='#ff4500')\n    ax[i//4][i%4].set_xlabel('Weight')\n    ax[i//4][i%4].set_ylabel('Top words')\n#    ax[i//4][i%4].set_yticks(indexes + bar_width);\n    ax[i//4][i%4].set_yticklabels(top_words);\n    ax[i//4][i%4].set_title('Cluster '+ str(i+1))\n\n\n\n\n\n\nFIGURE 8. Top 10 most frequest words per cluster (\\(k=16\\))\n\n\n\n\n\nCode of placing a Reddit mask wordcloud for k=16\ntopics1 = ['Tech support', 'Clinton', 'Trump', 'Cluster 4', 'Recipes', 'TIL', 'Aparment/Rent', 'Technology'\n           , 'Sanders', 'Food', 'Cluster 11', 'Car-related queries', 'New Year', 'Reaction Compilation', 'Legal concerns', 'Cluster 15']\n\nfig, ax = plt.subplots(4,4,dpi=100, figsize=(16,12))\nplt.subplots_adjust(wspace=0.2, hspace=0)\nfor ctr in range(kmeans2.n_clusters):\n    indices = [i for i, x in enumerate(y_predict_kmeans2) if x == ctr]\n    wordcloud = WordCloud(background_color=\"white\", max_words=100, min_font_size=8, mask=mask, \n                          prefer_horizontal=0.6, colormap='copper').generate(' '.join([docs[index] for index in indices]))\n\n    ax[ctr//4][ctr%4].imshow(wordcloud, interpolation='bilinear')\n    ax[ctr//4][ctr%4].axis('off');\n    ax[ctr//4][ctr%4].set_title(topics1[ctr]);\n\n\n\n\n\n\nFIGURE 9. Wordcloud per cluster (\\(k=16\\))\n\n\n\nAs the number of clusters was increased, the more specific themes emerge. The following themes emerged:\n\nTechnical Support\n\n[   'smoothly solution for the canon printer 1-877-776-4348',\n    'follow these step and escan antivirus easily',\n    'norton antivirus technical support phone number',\n    'itech logik | uk microsoft help desk phone number',\n    'til tupak shakur became the first artist to have an album at number one '\n    'on the billboard 200 while serving a prison sentence.']\n\nHillary Clinton Emails\n\n[   'semi automatic ropp capping machine / ropp cap sealing machine',\n    'state: hundreds of old clinton emails newly classified',\n    'georgia poll: clinton 70, sanders 23',\n    'nsa whistleblower: clinton email server was ‘open to being hacked by '\n    'anybody in the world’',\n    \"hillary clinton's new hampshire boosters out in full force\"]\n\nDonald Trump\n\n[   \"donald trump says he'll attract democrats; polls say that's unlikely\",\n    'inside the clintons’ plan to defeat donald trump',\n    'hecklers disrupt trump rally, photographer shoved to the ground',\n    'a photographer covering a donald trump rally in virginia said a secret '\n    'service agent choked him and slammed him to the ground monday as he tried '\n    'to leave a media pen at the event where a protest erupted.',\n    'til that donald trump sold steaks at the sharper image']\n\nRecipes\n\n[   'i have access to tons and tons of green onion tops,what can i make?',\n    'mexican chopped salad',\n    'dublin, ireland has gone mad for spice bags from takeaways, i tried my '\n    'hand at my own. potato, shredded chicken, peppers and onions, cooked in '\n    'garlic, chillies and spices. apologies for photo quality!',\n    'tips for starting a healthy lifestyle!',\n    \"arroz carreteiro - brazilian wagoners' rice gourmetcentric\"]\n\nTIL (Today I learned) topics\n\n[   'til maternal kisses are not effective in alleviating minor childhood '\n    'injuries (boo-boos): a randomized, controlled and blinded study.',\n    'til goldman sachs did a study in 2009 that estimated a unified korea '\n    'could boast an economy larger than france, germany, and even japan by '\n    '2050 with a gdp of $6 trillion.',\n    \"til the justice league's martian manhunter was once addicted to cookies \"\n    'and hulked out when deprived of them.',\n    'til that the top 25 hedge fund managers in the us earn more than all us '\n    'kindergarten teachers (and pay a smaller tax rate)',\n    'til leonard howell (\"the first rasta\") preached that ras tafari, the '\n    'emperor of ethiopia in 1933, was the messiah, that black people were the '\n    'chosen of god and that they would soon be restored to their native '\n    'country of ethiopia.']\n\nApartment and Rent Concerns\n\n[   'my dad died yesterday and my mom is now being evicted because she is not '\n    'on the lease, what can she do?',\n    'landlord charged my girlfriend a damage fee of $5,000 for a broken door',\n    'is refusing to take a breathalyzer test an admission of guilt?',\n    'does an apartment with living/housing code violations against the city '\n    'equal a nullified lease?',\n    'had verbal agreement with room mate for payment of back monies to be '\n    'paid, to be formalized in writing, questions, etc.']\n\nBernie Sanders\n\n[   '“it appears that mr. trump is getting nervous that working families are '\n    'catching on that his policies represent the interests of the billionaire '\n    'class against almost everyone else.\" - bernie sanders',\n    'killer mike: bernie sanders is the only candidate for black voters',\n    'hillary clinton has already forgotten about bernie sanders | vice | '\n    'united states',\n    'bernie sanders ‘revolution’ threatens hillary clinton in iowa.',\n    \"jesse watters questions sanders' young supporters in iowa\"]\n\nFood\n\n[   'semi automatic ropp capping machine / ropp cap sealing machine',\n    'state: hundreds of old clinton emails newly classified',\n    'georgia poll: clinton 70, sanders 23']\n\nBernie Sanders\n\n[   '“it appears that mr. trump is getting nervous that working families are '\n    'catching on that his policies represent the interests of the billionaire '\n    'class against almost everyone else.\" - bernie sanders',\n    'killer mike: bernie sanders is the only candidate for black voters',\n    'bernie sanders: the most fascist candidate of all.']\n\nFood\n\n[   'significant online food ordering trend',\n    's&amp;m restaurant in beijing caters to millennial’s basic instincts by '\n    'serving food &amp; sex',\n    'food near me – jasper arkansas',\n    'as i am very polish and grew up eating a lot of good food, i decided to '\n    'cook it for myself tonight.',\n    'food in thailand - november 2015']\n\nCar-related topics\n\n[   'charged with dui 2 years and 4 months after i was involved in a single '\n    'car accident.',\n    \"[md] car impounded after repair order wasn't received in september, \"\n    'options?',\n    'tesla unveils the model 3, its lowest-priced car',\n    'lincolnshire county council hit by £1m malware demand',\n    'florida adverse possession law question and others regarding married but '\n    'separate couples.']\n\n“Happy new year” reddits\n\n[   'watch \"best of maskedman - 2015 new year special.\" on youtube',\n    'happy new year reddit! :) i quit retirement, spent $250k in savings and '\n    '51 months (15,000 hours) to develop the new internet, owned by the '\n    'people, powered by humanity - the first company in history equally owned '\n    'by everybody alive! we are uniting the whole world into one!',\n    'mobile game of the year 2015: horizon chase',\n    'til that j. r. r. tolkien hand wrote and illustrated letters to his '\n    'children from father christmas every year for 20 years.',\n    '[bellingham, wa] wife looking at buying property in bella coola, bc; '\n    'legal implications of living half the year here in bellingham and half '\n    'the year in bella coola']\n\nVines reaction compilation\n\n[   'happy 2016 and may all your games be mostly bug free',\n    'eight issues that could shape politics in 2016',\n    '2016 iowa democratic caucus locations - get out there and caucus!!',\n    '♥ dragonsden vines 🔒 - february 1, 2016 🔱 dragonsden reaction compilation '\n    '🎨',\n    'ark: surival evolved rings in 2016 with giant, rideable kangaroos']\n\nLegal concerns\n\n[   '(california) is it a crime when a religious figure/ lecturer has '\n    'relations with one of his followers',\n    'being accused of public indecency, among other things. this is a '\n    'misunderstanding because i had health issues. [kansas, usa]',\n    \"[philadelphia, pa] i'm concerned that something suspicious is going on at \"\n    \"my neighbor's house. i'd like to know if there's enough probable cause \"\n    'for the police to investigate.',\n    \"refugee's treated worse than dogs!\",\n    'i have a great idea for a trpg.']\n\nConclusions and Recommendations\n\nThe clustering results show that Reddit is a very American-centric platform. Majority of the topics are related to the 2016 U.S. Presidential Election, specifically on the Republican nominee Donald Trump, Democrat candidates Hillary Clinton, and the Iowa caucuses.\nAlso, the results show that the two primary motivations of users to post in Reddit is (a) to discuss politics, and (b) to seek advise/ask help from other Redditors, whether technological, legal, or culinary in nature.\nIncreasing the number of clusters also reveal more specific topics such as ‘technical support’ and ‘’Vines reaction compilation videos’.\nFor future studies, it is suggested to perform hierarchical clustering.\n\nReferences\n\n\n\n\nThumbnail Image\n\nAll the news\n\nClustering text documents using k-means\n\nLatent Semantic Analysis"
  },
  {
    "objectID": "posts/2023-04-23-UnwRappler/index.html",
    "href": "posts/2023-04-23-UnwRappler/index.html",
    "title": "UnwRappler",
    "section": "",
    "text": "Authors and Notes\n\nThis analysis was done together with my groupmates George Esleta, Carmelita Esclanda, and Elmer Robles.\nOriginal creation and submission of this report was last July 2019.\n\nExecutive Summary\n\n\nRappler, one of the leading online news publishers in the Philippines, seeks to inspire community engagement and create action for social change through citizen journalism. However, it has recently been under scrutiny by the Philippine government for “twisted” reporting. This study aims to uncover the underlying themes of Rappler news articles via unsupervised clustering techniques and see if there is an inherent concentration of news in a specific theme. A total of 11,079 articles were extracted from Rappler’s national news section published from January 2018 to May 2019. Features were extracted from the articles using a term frequency-inverse document frequency (TF-IDF) vectorizer and dimensions were reduced by implementing Latent Semantic Analysis (LSA). Lastly, unsupervised clustering via k-means algorithm was applied to group the articles and internal validation metrics were utilized to determine the optimal number of clusters. Ten clusters were uncovered, with Philippine President Rodrigo Duterte as the dominant cluster. The remaining themes touch on different branches of government, police and weather updates, and trending national issues. The insights gained from this research can aid Rappler in balancing its reporting by lessening bias towards specific topics.\n\n\n\n\n\nIntroduction\n\nRappler is one of the leading online news publishers in the Philippines. It was started in 2011 by former CNN journalist Maria Ressa. However, since 2018, Rappler has been under heavy scrutiny by the Philippine government for “twisted” and “biased reporting”. Philippine President Rodrigo Duterte even went so far as calling Rappler a “fake news” outlet that publishes articles that are “pregnant with falsity.” [1] Rappler has been on the receiving end of criminal cases from the Philippine governments. BIR filed several counts of tax evasion charges against Rappler CEO Maria Ressa. Cyberlibel cases were also filed against Ressa.\nThis begs the question “Why is Rappler being targeted by the Duterte Administration?” Is Rappler really “twisted” and “biased” in its reporting? To answer this question, we uncovered the underlying themes of Rappler’s news articles via unsupervised clustering, to see if there is indeed an inherent concentration of news in a specific topic or theme.\n\nBusiness Value\n\nExtracting themes from a set of articles programmatically has a widespread application in the digital publishing industry and can be used to deliver business value to readers, journalists, advertisers, aggregators, and researchers.\n\nReaders in the digital age have no patience. They are more likely to use a search engine to retrieve a set of ranked articles from multiple news sources and it becomes a challenge for online news platforms to engage a reader continuously. Automatic theme extraction and clustering of articles provide the ability to structure a set of articles in order to make it easier for readers to navigate to related articles. If articles can be presented in a logical hierarchy driven by patterns in the data, human intervention is minimized (saving on labor costs) and the reader is kept on the news platform, increasing engagement time on the site.\nNews Publishers and Journalists seeking to address topics of broad interest to their readership can benefit from a thematic clustering articles to determine the balance of news coverage.  Publishing organizations are always looking for gaps in news coverage in order to offer readers some perspective and insight into topics that are not heavily covered. By examining existing themes holistically, they can better gauge what is missing and what their next article should be about.\nAdvertisers that are considering online news platforms to reach specific customer segments can tailor their messages so that they are congruent with the themes that readers are interested in.  For instance, themes dealing with legislative or judicial proceedings may appeal to advertisers whose clients are lawyers or law schools. Advertisers of tourism and tourist agencies may be interested in weather and specific vacation destinations, such as Boracay.\nAggregators in the licensed publishing industry create packages of content for redistribution.  Their business model, which can be subscription or advertiser-based, is to target specific customers based on their geographic, demographic, and psychographic profile. For instance, if an investor wanted to understand market conditions for a particular industry, they would subscribe to a real-time news delivery service that was able to provide the relevant articles.\nResearchers make money by selling analytical reports.  By studying the themes presented on news platforms and combining the study with other metrics such as readership engagement, sentiment analysis, and political stance, they can make in-depth comparisons with other news delivery platforms to better inform advertisers and aggregators.\n\n\nMethodology\n\nTo uncover the underlying themes, a total of 11,079 news articles published from January 2018 to May 2019 were retrieved from Rappler’s Nation section (https://www.rappler.com/nation). The general workflow for clustering the Rappler articles as shown in Figure 1 involves the following steps:\n\nData Extraction\nData Storage\nData Preprocessing\nExploratory Data Analysis\nFeature Extraction via TFIDF Vectorization\nDimensionality Reduction using Latent Semantic Analysis (LSA)\nUnsupervised clustering using k-means algorithm\n\nEach step of the workflow will be discussed in the succeeding sections. \n\n\n\n\nFIGURE 1: Workflow for clustering the Rappler Articles\n\n\n\n\n\nData Extraction\n\nTo collect data for the analysis, web scraping tools were utilized, namely the Python’s requests and BeautifulSoup modules, to extract news articles from the Nation section of Rappler website between the period of January 2018 and May 2019. The extraction process included the extraction of the article’s content and relevant metadata while excluding non-essential information such as image captions, author and photographer details, location headers, hyperlinks, social media text, and general sign-offs. To ensure data accuracy and validity, an initial cleaning process was performed to eliminate irrelevant information.\n\nData Storage\n\nThe extracted articles were stored in a local sqlite database. A total of 11,079 news articles were stored in the database. Table 1 shows the data description:\nTABLE 1: Data Dictionary\n\n\n\nData\nData type\nDescription\n\n\n\n\narticle_id\nVARCHAR\nUnique ID identifier\n\n\nurl\nVARCHAR\nRelative URL\n\n\nheadline\nVARCHAR\nHeadline title\n\n\nmetadesc\nVARCHAR\nQuick synopsis of the article\n\n\nlabel\nVARCHAR\nAbsolute URL\n\n\nauthor\nVARCHAR\nAuthor name(s)\n\n\npublished_date\nVARCHAR\nPublished date\n\n\nupdated_date\nVARCHAR\nDate updated\n\n\narticle\nVARCHAR\nArticle text\n\n\nmetakey\nVARCHAR\nList of metatags associated with the article\n\n\n\n\nData Preprocessing\n\nData preprocessing was implemented on the acquired article text. The text preprocessing involves:\n\nNeutralizing text case-sensitivity by converting text to lowercase\nOmitting unnecessary whitespaces by removing leading and trailing whitespace\nConverting words to root form by performing stemming, via NLTK’s PorterStemmer.\n\n\nExploratory Data Analysis\n\nExploratory data analysis was conducted on the article corpus to identify any prominent themes or topics before clustering the data. Figure 2 illustrates the word cloud and word frequency graph for the Rappler news articles published from January 2018 to May 2019. The results demonstrate that the most frequently mentioned word in the dataset is President Rodrigo Duterte, indicating that he is a significant topic in Rappler’s news articles during that period. Furthermore, notable words such as government, Philippines, police, and justice were also identified. To confirm these findings, feature extraction and cluster analysis were performed on the dataset. Consequently, it can be inferred that at least one article cluster should have Duterte as the recurring theme.  \n\n\n\n\nFIGURE 2: (left) Word cloud of Rappler news articles from January 2018 to May 2019, (right) Word count of the top 15 most occurring words in the Rappler article corpus. Images show Former President Duterte was the top topic during the period.\n\n\n\n\nFeature Extraction\n\nTo vectorize the text, the term frequency-inverse document frequency (TF-IDF) vectorizer was implemented using scikit-learn’s Tfidfvectorizer. Unlike an equal weighting vectorizer, the TF-IDF statistic measures the significance of a word to both the document and the corpus. The implementation of the vectorizer also involved considering additional parameters, such as:\n\nthe removal of English stopwords\nthe inclusion of unigram, bigram, and trigram n-grams.\nwords that appeared less than 0.1% or more than 70% were discarded from the vectorization\n\nTABLE 2: Hyperparameter adjustments\n\n\n\n\n\n\n\n\nParameter\nDescription\nValue\n\n\n\n\nmin_df\nignore terms that have a document frequency strictly lower than the given threshold\n0.001\n\n\nmax_df\nignore terms that have a document frequency strictly higher than the given threshold\n0.7\n\n\nngram_range\nThe lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n will be used\n(1,3)\n\n\n\nThe TFidfVectorizer performs further cleaning of the data by removing both frequent words and rare words. Stopwords, or words that appear too frequently in the English and Filipino language (e.g., the, a, an, and, or), were dropped from the corpus. Rappler-specific stopwords, or words that appear in more than 70% of the titles, were also ignored by setting the max_df parameter of TFidfVectorizer to max_df = 0.7.\nRare words, or words that appeared in less than 0.1% of the Rappler articles, were also excluded from the corpus. This was implemented by setting the min_df parameter of the TFidfVectorizer to min_df = 0.001.\nUnigrams, bigrams, and trigrams were extracted as part of the vocabulary by setting the ngram_range parameter of the TFidfVectorizer to ngram_range = (1,3). This range of n-grams was chosen to preserve the context of certain phrases such as “Rodrigo Duterte”, “Supreme Court” (n=2), and “Philippine National Police” (n=3)\nAfter vectorization, the collection of Rappler articles has 36,254 unique items in its vocabulary.\n\nDimensionality Reduction\n\nThe resulting feature set was a 10,873 by 36,254 matrix, which was computationally expensive to cluster. To reduce computational power, the vectorized data underwent dimensionality reduction through latent semantic analysis (LSA) using the TruncatedSVD class of sklearn.\nIn general, a lower number of dimensions facilitates wider comparisons of themes in a collection of text, while a higher number of dimensions enables more specific comparisons. A dimensionality of 50 to 1000 is typically suitable depending on the document collection’s size and nature. Sensitivity analysis was conducted to determine the optimal number of components, starting with 50 components and incrementing by 50 until 1000 components. The analysis found that using 300 components (n=300) extracted the broader underlying themes.\n\nUnsupervised Clustering\n\nUnsupervised clustering using the k-means algorithm was utilized to extract the underlying themes present in the Rappler news articles. To determine the optimal number of clusters, sensitivity analysis was performed by testing different cluster counts ranging from k=2 to k=20. Internal validation were examined to select the optimal number of clusters. These validation metrics involved:\n\nminimizing the Intracluster to Intercluster distance ratio\nmaximizing the Calinski-Harabasz score\nmaximizing the Silhouette coefficient, and\nminimizing the sum of square distance to centroids coefficient\n\nThe next section will discuss the resulting article clusters and the optimal number of cluster counts.\n\nResults\n\nTo determine the optimal number of clusters, k-means clustering was implemented for different values of the cluster count k from k=2 to k=20. Shown in Figure 3 is the variation in the values of the different internal validation measures as the cluster count k is increased. The silhouette coefficient is monotonically increasing as the number of clusters is increased, suggesting increasing separation of the clusters. However, to keep the clustering parsimonious, the SSE elbow method was used instead to determine the optimal value of k. Plotted below are the values of the internal validation criteria for values of 𝑘 between 2 and 20. Using the elbow method, k=10 is a good candidate for the number of clusters.\n\n\n\n\nFIGURE 3: Plots of the various internal validation measures for k=2 to k=20. Using elbow method at k=10\n\n\n\n\nCluster Themes at k=10\n\nk-means clustering was performed on the Rappler articles using a cluster count of k=10. To infer the underlying theme of each article cluster, cluster word clouds were created as shown in Figure 5.\n\n\n\n\nFIGURE 5: Rappler Wordcloud for each cluster (k=10)\n\n\n\nForm these word clouds, the underlying theme of each article cluster was identified as follows:\n\nLegislative\nForeign Affairs (PH-China Relations)\nGeneral News\nJudiciary\nWeather\nPresident Rodrigo Duterte\n2019 Philippine Midterm Elections\nPolice\nBoracay Rehabilitation\nHealth\n\nFigure 6 displays the distribution of the number of articles across the identified clusters. The General News cluster accounts for over 25% of the articles, and the Duterte cluster is the second-largest with more than 15% of the news articles. This validates the initial findings of the exploratory data analysis, which showed that President Rodrigo Duterte is the most mentioned word in the dataset, and that Duterte is a prominent topic in Rappler’s news articles.\n\n\n\n\nFIGURE 6: Topic distibution of Rappler articles sorted descending.\n\n\n\n\nInsights\n\nThe application of the unsupervised clustering technique on a corpus of 11, 079 news articles shows consistency of topic clusters across different resolutions that include:\n\nDuterte Article Cluster\n\n\n\n\n\nFIGURE 7: (left) Word cloud and top words for the Duterte article cluster, (right) Top words mentioned in the Duterte corpus - it can be inferred that the subthemes of this cluster include human rights violations, the war on illegal drugs, and Senator Antonio Trillanes\n\n\n\nThe dominant theme of Rappler news articles is Philippine President Rodrigo Duterte, with nearly 16% of news articles in this cluster. Subthemes in this cluster include the war on drugs, human rights, Senator Antonio Trillanes, and Duterte’s ongoing war versus Rappler. This can imply that Rappler is particular with the President’s undertakings and that while this may be reasonable, Rappler has to review its focus given their strife with the President.\n\nPhilippine Politics Article Cluster (Legislative and Judiciary)\n\n\n\n\n\nFIGURE 8: (left) Word cloud and top words for the Judiciary article cluster, (right) From the top words, it can be inferred that the subthemes of this cluster include the impeachment of former Chief Justice Lourdes Sereno and the drug cases against Senator Leila De Lima.\n\n\n\n\n\n\n\nFIGURE 9: (left) Wordcloud and top words for the Legislative article cluster, (right) From the top words, it can be inferred that the subthemes of this cluster include the national budget and the proposed shift to a federal system\n\n\n\nThe other major themes are related to the other branches of the Philippine government, particularly the Legislative branch (House of Representatives, Senate) and the Judicial branch (Supreme Court).\nFigure 8 shows that the Judicial cluster contains subthemes such as the impeachment of former Chief Justice Maria Lourdes Sereno and the drug charges filed against Senator Leila de Lima by the DOJ. In the case of Sereno, the impeachment was due to various issues including failure to disclose assets in her SALN and misuse of public funds. On the other hand, De Lima was charged with drug-related offenses.\nhe Legislative article cluster (Figure 9) includes subthemes such as budget and proposed changes in the constitution to shift to a federal type of government. Similar to the dominant clusters, this cluster revolves around politics. It is evident that Rappler has a strong focus on the political landscape in the Philippines, particularly on current and significant issues.\n\nPolice and Weather Clusters\n\n\n\n\n\nFIGURE 10: (left) Wordcloud and (right) top words in the Police article cluster\n\n\n\n\n\n\n\nFIGURE 11: (left) Wordcloud and (right) top words in the Weather article cluster\n\n\n\nTwo recurring themes regardless of the number of clusters selected are police reports and weather reports. In line with the duterte cluster, it is sensible that the cluster about the police can be generated given the “oplan tokhang” implemented by the administration. Meanwhile, the weather cluster is sound considering the country’s geographic circumstances.\n\nTime-Specific Article Clusters\n\n\n\n\n\nFIGURE 12: (left) Wordcloud and (right) top words in the 2019 Midterm Elections article cluster\n\n\n\n\n\n\n\nFIGURE 13: (left) Wordcloud and (right) top words in the Boracay article cluster\n\n\n\n\n\n\n\nFIGURE 14: (left) Wordcloud and (right) top words in the Dengue Vaccine article cluster\n\n\n\nThe remaining themes are time-related and reflect major events during the timeframe selected (January 2019 to May 2019), such as the 2019 Philippine Midterm Elections, Boracay Island Rehabilitation (April 2018-October 2018), and the Dengvaxia (dengue vaccine) controversy.\n\nSensitivity Analysis\n\nWhile it was concluded that 10 is the optimal number of groupings for the subject dataset, sensitivity analysis has been performed to test the robustness of the results gathered. Shown below are the resulting article clusters for k=6 and k=16. It is noted that as the cluster count is increased, the most affected clusters are bigger ones, especially the Duterte cluster. As such, it can be deduced that said cluster has many subclusters which are differentiated when cluster count is increased. Nevertheless, the core clusters (e.g., General News, Duterte, 2019 Election, Police) created regardless of the number of clusters are constant.\n\n\n\n\nFIGURE 15: Clustering and Cluster size distribution for k=6\n\n\n\n\n\n\n\nFIGURE 16: Clustering and Cluster size distribution for k=16\n\n\n\n\nSummary & Conclusions\n\nGrouping of articles into clusters of related topics by unsupervised clustering method has significant value to communication researchers and media practitioners in studying news output at scale and its repercussions. This study was able to unwrap 10 major themes ranging from General News, Politics, Weather, Health and relevant events. As such, the result can be used as a starting point for a generalized theme extraction project from a national corpus in order to learn the general interest and sentiments of the people.\n\nRecommendations\n\nThe following points can be considered in future research related to this work:\n\nComparative cluster analysis with other Philippine news outfits (Inquirer, ABS-CBN News, GMA News) can be explored to validate Rappler’s focus on particular topics;\nSentiment analysis can explored to look into the subjective information or emotional states of the article. The value of combining clustering with sentiment analysis could be implemented in making better sense of Rappler’s public opinion on trending issues or the current administration; and,\nHistorical analysis can be explored to compare the rappler data during previous administrations with the current to recognize difference in rappler’s focus per administration. Administration changes can affect the political landscape and prioritization of complex issues. For instance, if an issue from one administration was solved in the next administration.\n\n\nReferences\n\n 1. Duterte says he banned Rappler due to ‘twisted’ reporting 2. Duterte calls Rappler ’fake news outlet” 3. All the news:Clustering 143,000 articles with KMeans 4. Story Discovery using K-Means Clustering on news articles 5. Clustering text documents using k-means 6. Latent semantic analysis"
  },
  {
    "objectID": "posts/2023-04-24-unwRappler2/index.html#sec-authors",
    "href": "posts/2023-04-24-unwRappler2/index.html#sec-authors",
    "title": "UnwRappler 2",
    "section": "1 Authors",
    "text": "1 Authors\nThis analysis was done together with my groupmates George Esleta, Carmelita Esclanda, and Elmer Robles.\nOriginal creation and submission of this report was last July 2019."
  },
  {
    "objectID": "posts/2023-04-24-unwRappler2/index.html#sec-executivesummary",
    "href": "posts/2023-04-24-unwRappler2/index.html#sec-executivesummary",
    "title": "UnwRappler 2",
    "section": "2 Executive Summary",
    "text": "2 Executive Summary\n\nRappler, one of the leading online news publishers in the Philippines, obtains audience sentiment through the Mood Meter widget embedded in its articles. The Mood Meter allows a reader to select an article’s mood from a set of predefined moods (happy, inspired, amused, sad, angry, afraid, annoyed, don’t care) with varying polarities (positive mood or negative mood). Using machine learning algorithms, we created two classification models to predict a Rappler article’s dominant mood and polarity. A total of 5,735 Rappler articles with metadata such as category, author, and mood ratings were used as the training set. The resulting models can predict the article polarity with 72% accuracy and the article’s dominant mood with 51% accuracy. These models can aid Rappler and other news publishers in content personalization, article engineering, and dynamic ad placement and pricing."
  },
  {
    "objectID": "posts/2023-04-24-unwRappler2/index.html#sec-introduction",
    "href": "posts/2023-04-24-unwRappler2/index.html#sec-introduction",
    "title": "UnwRappler 2",
    "section": "3 Introduction",
    "text": "3 Introduction\nWith the rapid growth of social networking and the internet in general, Rappler has been one of the top news providers in the Philippines by developing and distributing news via the web and social media channels. It is a social news network where stories inspire community engagement and digitally fueled actions for social change.\nRappler deployed “Mood Meter”, similar to Facebook Reactions, and embedded it on each article. Through the widget, Rappler allows readers to share their mood towards an article by answering the question, ”How does the story make you feel?”. Readers choose from a predefined set of emotions, namely: happy, sad, angry, don’t care, inspired, afraid, amused, and annoyed. By that, each article in Rappler will have multiple labeled emotions crowd-sourced from their readers. Figure 1 shows a sample Mood Meter report of an article.\n\n\n\nFigure 1: The Rappler Mood Meter. The reader can select from one of eight predefined moods (Happy, Sad, Angry, Don’t Care, Inspired, Afraid, Amused, and Annoyed)\n\n\nThis paper presents two models that implement Sentiment Analysis by predicting the mood and polarity of a given Rappler article. The paper is organized as follows: Section 4 describes related work on emotion classification using different language resources, features, and algorithms; Section 5 discusses the methodology; Section 6 discusses the model performance; lastly, Section 7 discusses the conclusions."
  },
  {
    "objectID": "posts/2023-04-24-unwRappler2/index.html#sec-relatedworks",
    "href": "posts/2023-04-24-unwRappler2/index.html#sec-relatedworks",
    "title": "UnwRappler 2",
    "section": "4 Related Works",
    "text": "4 Related Works\nStudies show that 80% of the way people make decisions is based on emotions, yet this fact has been disregarded by newsgroups. Given this, Rappler made its mark by implementing a ‘mood meter’ dashboard system that monitors the reader’s waves of emotion that kept them involved, thus amplified their social media presence.\nIn the advent of the knowledge economy, researchers such as Shivhare, Garg, and Mishra (2015) utilized emotion ontology and weighting algorithm that deals with the depth of ontology and parent-child relationship to quantify emotion class. The ontology is built with the definition of schemas and aspects such as entities and attributes, the relationship between the entities, and the domain vocabulary. Their research led to an average accuracy of 79.57% from six emotion classes. The emotion detection model was tested on 135 blog posts.\nAnother research classified a document’s polarity on a multi-way scale and expanded the task of classifying a movie review as either positive or negative to predicting star ratings on either 3 or 4 star scale. Pang and Lee (2005) checked human performance and, applied meta algorithm and metric labelling. The Meta algorithm gave the best performance over both multi-class and regression versions of SVMs when a novel similarity measure appropriate to the problem was employed.\nTaking the study further, sentiment analysis has been applied by researchers to get insights from user-generated feedback (see Medhat, Hassan, and Korashy 2014). Sentiment analysis is the computational study of opinions, sentiments, and emotions in text. Although traditional sentiment analysis focuses on the sentiment or emotion in the text, this study focuses on the emotions that are being elicited by a news article from readers.\nSeveral researchers,such as Siersdorfer et al. (2010), Sureka et al. (2010), Cheong and Cheong (2011), have performed sentiment analysis of social networks such as Twitter and YouTube. These works deal with comments, tweets and other metadata collected from the social network profiles of users or of public events that have been collected and analyzed to obtain significant and interesting insights about the usage of these social network websites by the general mass of people.\nThe work most closely related to ours is by Song et al. (Song et al. 2016) and Taj et al. (Taj, Shaikh, and Meghji 2019). The former built an emotion lexicon by developing a novel joint non-negative matrix factorization model which not only incorporates crowd-annotated emotion labels of articles but also generates the lexicon using the topic-specific matrices obtained from the factorization process while the latter explored sentiment analysis of news and blogs using a dataset from BBC comprising of new articles between the year 2004 and 2005. They observed that categories of business and sports had more positive articles, whereas entertainment and tech had a majority of negative articles."
  },
  {
    "objectID": "posts/2023-04-24-unwRappler2/index.html#sec-methodology",
    "href": "posts/2023-04-24-unwRappler2/index.html#sec-methodology",
    "title": "UnwRappler 2",
    "section": "5 Methodology",
    "text": "5 Methodology\n\n5.1 Data Collection\nA total of 5,735 articles were extracted from the Rappler website. As shown in Figure 2, the articles were taken from four categories: Nation (1,462 articles), World (1,322 articles), Entertainment-Movies (1,452 articles), and Entertainment-Music (1,499 articles. Apart from the full text of the article, metadata such as the article’s category, authors, and mood meter values were also extracted. The description of the Rappler data is shown in Table 1.\n\n\n\nFigure 2: Acticle count per category collected\n\n\n\n\n\nTable 1: Data description of the articles extracted from Rappler\n\n\n\n\n\n\n\nFeature\nData type\nDescription\n\n\n\n\ncategory\nVARCHAR\nArticle category (nation/world/entertainment)\n\n\npublished\nVARCHAR\nPublished date of the article\n\n\nmodified\nVARCHAR\nModified date of the article\n\n\nauthor\nVARCHAR\nAuthor of article\n\n\ntitle\nVARCHAR\nArticle title\n\n\nhappy\nINT\nPercentage of readers who selected “happy” as the article’s mood\n\n\nsad\nINT\nPercentage of readers who selected “sad” as the article’s mood\n\n\nangry\nINT\nPercentage of readers who selected “angry” as the article’s mood\n\n\ndon’t care\nINT\nPercentage of readers who selected “don’t care” as the article’s mood\n\n\ninspired\nINT\nPercentage of readers who selected “inspired” as the article’s mood\n\n\nafraid\nINT\nPercentage of readers who selected “afraid” as the article’s mood\n\n\namused\nINT\nPercentage of readers who selected “amused” as the article’s mood\n\n\nannoyed\nINT\nPercentage of readers who selected “annoyed” as the article’s mood\n\n\ntext\nVARCHAR\nArticle text\n\n\n\n\n\n\n5.2 Featue Extraction\n\n5.2.1 Text Processing\nThe article text was processed for data analysis prior to feature extraction. Common techniques like converting text to lowercase and removing leading/trailing whitespaces were applied to make text case-insensitive.\nAdvanced techniques such as stemming were also used to reduce derived words to their root form (e.g., playing, plays, and played were reduced to “play”). Stemming is a technique that reduces inflected or derived words to their base form, which can help to simplify text analysis by treating similar words as the same.\nTo further prepare the text for analysis, a bag-of-words vectorization technique was applied to the stemmed text. A Bag-of-words vectorization is a technique that represents text data as a sparse matrix of word frequencies, where each row corresponds to a document or text sample and each column corresponds to a unique word in the corpus. Table 2 shows the hyperparameters utilized.\nThe number of features was limited to 3,730 by performing hyperparameter tuning on the bag-of-words vectorization.\n\n\nTable 2: Hyperparameters utilized in bag-of-words representation\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nmin_df\n0.01\n\n\nmax_df\n0.7\n\n\nngram_range\n(1,3)\n\n\n\n\n\n\n5.2.2 Variable Engineering\nThe eight mood variables extracted from the Mood Meter are continuous variables and can have a value between 0 and 100. To convert the problem to a classification problem (as opposed to regression), four additional variables were computed from the original article metadata, namely the positivity score, negativity score, polarity, and the dominant mood. Table 3 provides a description of each computed variable. Of these four, the polarity and dominant mood were chosen as the target variables of our models.\n\n\nTable 3: Additional variables computed from the Rappler data\n\n\n\n\n\n\n\nComputed Variable\nData type\nDescription\n\n\n\n\npositive_score\nINT\nSum of positive moods (happy, inspired, amused)\n\n\nnegative_score\nINT\nSum of negative moods (angry, afraid, annoyed, sad)\n\n\npolarity\nINT\nArticle polarity, whether negative (0) or positive (1)Negative polarity (polarity=0) if positive_score &lt; negative_score  Positive polarity (polarity=1) if positive_score &gt; negative_score\n\n\ndominant_mood\nINT\nMood with the highest number of votes from readers0: happy1: sad2: angry3: dontcare4: inspired5: afraid6: amused7: annoyed\n\n\n\n\nThe polarity of an article was quantified by grouping the eight moods into two: the positive moods (happy, inspired, amused) and the negative moods (sad, angry, annoyed, afraid, don’t care). If the positivity score (sum of positive moods) is greater than the negativity score (sum of negative moods), the article’s polarity is tagged as positive; otherwise, the polarity is tagged as negative. The dominant mood of an article, on the other hand, was obtained by getting the mood with the highest value.\n\n\n\n5.3 Model Evaluation\nTwo (2) classification models were developed in this study. The first model is a binary classification model that predicts article polarity (negative or positive), while the second model is a multi-class classification model that predicts an article’s dominant mood from eight predefined moods (happy, inspired, amused, sad, angry, afraid, don’t care, annoyed).\nSeveral supervised machine learning algorithms were evaluated in terms of classification accuracy. For the models to be considered effective, they must achieve a classification accuracy greater than the proportional chance criteria (PCC), which is the highest possible random chance of classifying an article without an explicit mathematical model.\n\n5.3.1 Polarity Classification Model\nFigure 3 shows the resulting polarity distribution of the articles. In terms of article polarity, the dataset has a proportional chance criterion (PCC) of 54.4%, which means that there is a 54.4% chance of randomly predicting the correct polarity without any model. For our machine learning model to be considered successful, it must achieve a minimum classification accuracy of 1.25PCC or 68%.\n\n\n\nFigure 3: Polarity distribution of articles. Our model must achieve a classification accuracy higher than the proportional chance criteria (PCC = 0.545) for it to be considered adequate.\n\n\n\n\n5.3.2 Mood Classification Model\nThe extracted Rappler dataset is highly imbalanced in terms of mood distribution as seen in Figure 4. To address this, Synthetic Minority Oversampling Technique (SMOTE) was implemented. SMOTE is a technique for addressing class imbalance in a dataset by creating synthetic examples of the minority class through interpolation. It helps balance the class distribution and improve the performance of machine learning models. In this scenario, undersampling was utilized to equalize the class distribution among the eight multi-classes. After balancing using SMOTE, a proportional chance criteria (PCC) of 12.5% was achieved. For our model to be considered effective, a minimum classification accuracy of 1.25PCC or 16% must be achieved.\n\n\n\nFigure 4: Mood distribution of articles. Synthetic Minority Oversampling Technique (SMOTE) was implemented to balance the dataset."
  },
  {
    "objectID": "posts/2023-04-24-unwRappler2/index.html#sec-results",
    "href": "posts/2023-04-24-unwRappler2/index.html#sec-results",
    "title": "UnwRappler 2",
    "section": "6 Results",
    "text": "6 Results\n\n6.1 Polarity Classifier\nTable 4 shows the performance of the various machine learning algorithms used to predict the polarity of Rappler articles. The Random Forest classifier (with features of 300 trees, maximum depth=3 and, maximum features=30%) is the best-performing algorithm and can predict the polarity of an article with 72.0% accuracy. This is 32% higher than the proportional chance criterion of 54.5%, indicating that our model is successful in classifying Rappler articles based on polarity.\n\n\nTable 4: Performance of the various machine learning algorithms in predicting article polarity.\n\n\n\n\n\n\n\n\n\n\nMachine Learning Method\nTraining Accuracy\nTest Accuracy\nTraining Time (s)\nTest Time (s)\nBest Parameter\n\n\n\n\nKNN (minmax scaling)\n0.729\n0.710\n1.794\n1.316\nn_neighbors: 46\n\n\nLogistic Regression (l2 penalty, minmax scaling)\n0.739\n0.708\n0.250\n0.003\nC: 0.1\n\n\nLogistic Regression (l1 penalty, minmax scaling)\n0.720\n0.720\n0.199\n0.003\nC: 0.05\n\n\nLinear SVM (l2 penalty, minmax scaling)\n0.741\n0.709\n0.265\n0.003\nC: 0.02\n\n\nLinear SVM (l1 penalty, minmax scaling)\n0.720\n0.720\n0.173\n0.002\nC: 0.01\n\n\nNon-Linear SVM (rbf kernel, minmax scaling)\n0.720\n0.720\n13.375\n2.549\nC: 1, gamma: auto\n\n\nNon-Linear SVM (poly kernel, minmax scaling)\n0.733\n0.720\n13.585\n2.573\nC: 0.2, degree: 2, gamma: scale\n\n\nNaive Bayes\n0.684\n0.663\n0.047\n0.004\nalpha: 500\n\n\nDecision Tree\n0.722\n0.719\n0.179\n0.006\nmax_depth: 3\n\n\nRandom Forest\n0.721\n0.720\n5.052\n0.110\nmax_depth: 3, max_features: 0.3\n\n\nGradient Boosting\n0.858\n0.697\n9.643\n0.015\nlearning_rate: 0.1, max_depth: 3, max_features: 0.3\n\n\n\n\nTo further explain the output of our polarity classification model, a Shapley Additive Explanations (SHAP) summary plot of the model was created as shown in Figure 5.\nFor context, SHAP is a framework for explaining the predictions of machine learning models. It assigns importance values to each feature in a prediction using the concept of Shapley values from cooperative game theory. SHAP values provide a measure of feature importance and can be used to understand how the model is making its predictions.\nThe horizontal axis indicates the impact of each feature on the target variable, while the color signifies the magnitude of the feature. Among the features used in the model, the article category is the top predictor of article polarity. Being categorized under Nation has a significantly negative impact on the article’s polarity. Conversely, an article that is NOT categorized under Nation (e.g., Entertainment-Music, Entertainment-Movies) is more likely to get positive reactions from readers.\n\n\n\nFigure 5: Shapley Additive Explanations (SHAP) summary plot of the model for predicting article polarity. The horizontal axis indicates the impact of each feature on the target variable, while the color signifies the magnitude of the feature. The article category is the top predictor of polarity.\n\n\n\n\n6.2 Mood Classifier\nTable 5 shows the performance of the various machine algorithms used to predict article dominant mood. The Gradient Boosting classifier (learning rate=0.15, maximum depth=3 and maximum features=50%) is the best-performing algorithm and can classify the dominant polarity of an article with 51.0% accuracy.\n\n\nTable 5: Performance of the various machine learning algorithms in predicting the dominant mood.\n\n\n\n\n\n\n\n\n\n\nMachine Learning Method\nTraining Accuracy\nTest Accuracy\nTraining Time (s)\nTest Time (s)\nBest Parameter\n\n\n\n\nKNN (minmax scaling)\n1.000\n0.292\n1.315\n2.097\nn_neighbors: 1\n\n\nLogistic Regression (l2 penalty, minmax scaling)\n0.7333\n0.239\n43.838\n0.003\nC: 500\n\n\nLogistic Regression (l1 penalty, minmax scaling)\n0.735\n0.239\n223.993\n0.004\nC: 100\n\n\nLinear SVM (l2 penalty, minmax scaling)\n0.722\n0.219\n76.797\n0.003\nC: 1000\n\n\nLinear SVM (l1 penalty, minmax scaling)\n0.722\n0.217\n66.984\n0.004\nC: 100\n\n\nNon-Linear SVM (rbf kernel, minmax scaling)\n1.000\n0.463\n121.646\n5.137\nC: 500, gamma: ’auto’\n\n\nNon-Linear SVM (poly kernel, minmax scaling)\n0.999\n0.426\n128.128\n5.239\nC: 500, degree:2, gamma: ’auto’\n\n\nNaive Bayes\n0.365\n0.312\n0.281\n0.003\nalpha: 5\n\n\nDecision Tree\n0.421\n0.418\n1.312\n0.007\nmax_depth: 5\n\n\nRandom Forest\n0.608\n0.444\n17.425\n0.109\nmax_depth: 5, max_features: ‘auto’\n\n\nGradient Boosting\n1.000\n0.510\n1735.419\n0.088\nLearning_rate: 0.15, max_depth: 5, Max_features: 0.5"
  },
  {
    "objectID": "posts/2023-04-24-unwRappler2/index.html#sec-conclusions",
    "href": "posts/2023-04-24-unwRappler2/index.html#sec-conclusions",
    "title": "UnwRappler 2",
    "section": "7 Conclusions",
    "text": "7 Conclusions\nSentiment analysis of more than 5000 Rappler articles was conducted using several machine learning models. After collecting data, extracting relevant features and evaluating the models, it is found that Gradient Boosting Method has the best accuracy of 51% in predicting the mood of an article with reference to the baseline accuracy of 16%. On the other hand, Support Vector Machine (RBF) and Random Forest are superior in predicting the polarity with 72% accuracy against the 55% proportional chance criterion. Also, it is found that category is the top indicator for predicting both mood and polarity using superior models.\nWith said findings, ML is deemed to increase the precision of recommendations and provide a personalized experience to Rappler users. Rappler is recommended to implement a Recommender System to personalize the content of its website for each visitor based on the dominant mood or polarity of the articles being viewed. In the same way, Rappler can use the study in implementing dynamic placements and pricing in their display advertising inventory. With this, advertisers in the Rappler website can acquire huge increases in conversions and sales with a lower cost of acquisition. By understanding better what drives leads, Rappler can take advantage of ML by reengineering its contents to generate moods that attract more stakeholders. Whether the objective is page views, social shares, leads, or revenue, a ranked list of words-provoking sentiments can help Rappler plan resources, priorities, and budgets more effectively.\nFor future work, we recommend exploring other Rappler article categories and predicting the authorship of Rappler articles based on sentiments."
  },
  {
    "objectID": "posts/2023-04-24-unwRappler2/index.html#sec-references",
    "href": "posts/2023-04-24-unwRappler2/index.html#sec-references",
    "title": "UnwRappler 2",
    "section": "8 References",
    "text": "8 References\n\n\nCheong, France, and Christopher Cheong. 2011. “Social Media Data Mining: A Social Network Analysis of Tweets During the 2010-2011 Australian Floods.” In 15th Pacific Asia Conference on Information Systems (PACIS), 46.\n\n\nMedhat, Walaa, Ahmed Hassan, and Hoda Korashy. 2014. “Sentiment Analysis Algorithms and Applications: A Survey.” Ain Shams Engineering Journal 5 (4): 1093–113. https://doi.org/https://doi.org/10.1016/j.asej.2014.04.011.\n\n\nPang, Bo, and Lillian Lee. 2005. “Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales.” In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), 115–24. Ann Arbor, Michigan: Association for Computational Linguistics. https://doi.org/10.3115/1219840.1219855.\n\n\nShivhare, Shiv Naresh, Shakun Garg, and Anitesh Mishra. 2015. “EmotionFinder: Detecting Emotion from Blogs and Textual Documents.” In International Conference on Computing, Communication & Automation, 52–57. https://doi.org/10.1109/CCAA.2015.7148343.\n\n\nSiersdorfer, Stefan, Sergiu Chelaru, Wolfgang Nejdl, and Jose San Pedro. 2010. “How Useful Are Your Comments? Analyzing and Predicting YouTube Comments and Comment Ratings.” In Proceedings of the 19th International Conference on World Wide Web, WWW ’10, 891–900. https://doi.org/10.1145/1772690.1772781.\n\n\nSong, Kaisong, Wei Gao, Ling Chen, Shi Feng, Daling Wang, and Chengqi Zhang. 2016. “Build Emotion Lexicon from the Mood of Crowd via Topic-Assisted Joint Non-Negative Matrix Factorization.” In. https://doi.org/10.1145/2911451.2914759.\n\n\nSureka, Ashish, Ponnurangam Kumaraguru, Atul Goyal, and Sidharth Chhabra. 2010. “Mining YouTube to Discover Extremist Videos, Users and Hidden Communities.” In Information Retrieval Technology, edited by Pu-Jen Cheng, Min-Yen Kan, Wai Lam, and Preslav Nakov, 13–24. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nTaj, Soonh, Baby Bakhtawer Shaikh, and Areej Fatemah Meghji. 2019. “Sentiment Analysis of News Articles: A Lexicon Based Approach.” 2019 2nd International Conference on Computing, Mathematics and Engineering Technologies (iCoMET), 1–5."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-authors",
    "href": "posts/2023-04-25-Solar/index.html#sec-authors",
    "title": "Here Comes The Sun",
    "section": "1 Authors",
    "text": "1 Authors\nThis analysis was done together with my groupmate Jac Lin T. Yu\nOriginal creation and submission of this report was last Dec 2019."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-executivesummary",
    "href": "posts/2023-04-25-Solar/index.html#sec-executivesummary",
    "title": "Here Comes The Sun",
    "section": "2 Abstract",
    "text": "2 Abstract\n\nDue to the increasing focus towards sustainable global efforts against climate change, renewable energy generation, more specifically solar power generation, has experienced a significant growth in the past few years. However, solar power production by nature is still considered highly volatile and intermittent, due to its high dependency on various factors such as irridiation, cloud cover, temperature, and other weather parameters. As such, the inherent volatile nature of solar poses significant challenges in accurate forecasting techniques not only to solar power plant operators but also to power grid operators. This paper provides a deep learning model approach in forecasting solar power production with multiple weather variables\nIn the paper, four sites across the Philippines were considered, and four variations of the LSTM model were implemented. For two power plants, the best model had a single layer (32-nodes) LSTM, while the other two, utilized a two layer LSTM model with 64 and 32 nodes. The modifed Mean Absolute Error across all four sites reduced by at least 60% when compared to industry standard."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-introduction",
    "href": "posts/2023-04-25-Solar/index.html#sec-introduction",
    "title": "Here Comes The Sun",
    "section": "3 Introduction",
    "text": "3 Introduction\nWith the increasing devastating e ects of climate change, governments and industries have long been developing policies to mitigate its impact. In the COP21 or the 2015 Paris Climate Conference, all participating countries have agreed to reduce carbon emission in order to contain global warming to below 2°C. In addition, the United Nation’s 2030 Sustainable Development Goals which aims to address various societal issues, include provisions to produce affordable and clean energy (SDG 7) and provide climate action (SDG 13). Thus, the global energy industry has veered its focus away from traditional fuel sources to renewable and sustainable solutions, such as hydroelectric, solar and wind.\n\n3.1 Statement of the Problem\nThe shift towards renewable energy poses new threats and challenges to existing power grids across the world due to the inherent variability and intermittency of solar power. The problem then is “Using neural networks, can solar power generation be predicted better than what industry currently uses given weather data?”\n\n\n3.2 Significance\nAccurate forecasting of solar power can help solar power plant operators reduce the risk of unnecessary market penalties. In addition, forecasting accurate solar power generation may provide power grid operators the ability to balance and schedule the distribution of generated power, for not only renewable power operators, but also conventional (and rigid) power plants, such as coal and natural gas. The overall significance of this study is to ensure a more secure and stable power grid even with high solar power penetration.\n\n\n3.3 Scope and Limitation\nThe study only considers four solar power plants located in North Luzon, Greater Metro Manila, Visayas. No plants in the South Luzon and Mindanao regions were considered. Only 2 years and 8 months worth of weather data was included in the study. This data, however, does not include solar irridiance, an important indicator of solar power generation, due to the lack of availability."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-relatedworks",
    "href": "posts/2023-04-25-Solar/index.html#sec-relatedworks",
    "title": "Here Comes The Sun",
    "section": "4 Related Works",
    "text": "4 Related Works\nStudies show that 80% of the way people make decisions is based on emotions, yet this fact has been disregarded by newsgroups. Given this, Rappler made its mark by implementing a ‘mood meter’ dashboard system that monitors the reader’s waves of emotion that kept them involved, thus amplified their social media presence.\nIn the advent of the knowledge economy, researchers such as Shivhare, Garg, and Mishra (2015) utilized emotion ontology and weighting algorithm that deals with the depth of ontology and parent-child relationship to quantify emotion class. The ontology is built with the definition of schemas and aspects such as entities and attributes, the relationship between the entities, and the domain vocabulary. Their research led to an average accuracy of 79.57% from six emotion classes. The emotion detection model was tested on 135 blog posts.\nAnother research classified a document’s polarity on a multi-way scale and expanded the task of classifying a movie review as either positive or negative to predicting star ratings on either 3 or 4 star scale. Pang and Lee (2005) checked human performance and, applied meta algorithm and metric labelling. The Meta algorithm gave the best performance over both multi-class and regression versions of SVMs when a novel similarity measure appropriate to the problem was employed.\nTaking the study further, sentiment analysis has been applied by researchers to get insights from user-generated feedback (see Medhat, Hassan, and Korashy 2014). Sentiment analysis is the computational study of opinions, sentiments, and emotions in text. Although traditional sentiment analysis focuses on the sentiment or emotion in the text, this study focuses on the emotions that are being elicited by a news article from readers.\nSeveral researchers,such as Siersdorfer et al. (2010), Sureka et al. (2010), Cheong and Cheong (2011), have performed sentiment analysis of social networks such as Twitter and YouTube. These works deal with comments, tweets and other metadata collected from the social network profiles of users or of public events that have been collected and analyzed to obtain significant and interesting insights about the usage of these social network websites by the general mass of people.\nThe work most closely related to ours is by Song et al. (Song et al. 2016) and Taj et al. (Taj, Shaikh, and Meghji 2019). The former built an emotion lexicon by developing a novel joint non-negative matrix factorization model which not only incorporates crowd-annotated emotion labels of articles but also generates the lexicon using the topic-specific matrices obtained from the factorization process while the latter explored sentiment analysis of news and blogs using a dataset from BBC comprising of new articles between the year 2004 and 2005. They observed that categories of business and sports had more positive articles, whereas entertainment and tech had a majority of negative articles."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-methodology",
    "href": "posts/2023-04-25-Solar/index.html#sec-methodology",
    "title": "Here Comes The Sun",
    "section": "5 Methodology",
    "text": "5 Methodology\n\n5.1 Dataset\nFrom December 26, 2016 to August 25, 2019, energy generation data in megawatts (MW) was collected from the Philippines’ Wholesale Electricity Spot Market (WESM). To limit the scope of the study, only four solar power plants located across Luzon and Visayas were chosen. One power plant was selected per location/province. Table 1 shows these plants along with their location and rated dependable capacity, a metric that refers to the maximum amount of power that a power plant can produce over a specified period of time, while Table 2 shows a data dictionary of the collected data.\n\n\nTable 1: Summary of selected power plants, location, and capacity\n\n\nPower Plant\nLocation\nCapacity (in MW)\n\n\n\n\n\nClark Solar Power Plant\nMabalacat, Pampanga\n22 MW\n\n\n\nFirst Toledo Solar Power Plant\nToledo, Cebu\n60 MW\n\n\n\nSubic Solar Power Plant\nOlongapo, Zambales\n100 MW\n\n\n\nValenzuela Solar Power Plant\nValenzuela, NCR\n8.5 MW\n\n\n\n\n\n\n\nTable 2: Data Dictionary of the WESM data\n\n\nName\nDefinition\n\n\n\n\nDATETIME\nDelivery datetime (ex. 12/26/2016 12:00:00 AM)\n\n\nYEAR\nDatetimelike Property for the Specific Year in the Datetime\n\n\nMONTH\nDatetimelike Property for the Specific Month in the Datetime\n\n\nDAY\nDatetimelike Property for the Specific Day in the Datetime\n\n\nHOUR\nDatetimelike Property for the Hour/Interval in the Datetime\n\n\nRESOURCE_ID\nEach power plant and unit has a specific ID\n\n\nMW\nRegistered MW generated for the spefic hour\n\n\n\n\nCorresponding weather data, in each of these four locations, were collected separately from the Weather Underground website. The weather data contained 10 features. A data dictionary of the collected data can be seen in Table 3.\n\n\nTable 3: Data Dictionary of the Weather data from the Weather Underground Website\n\n\nName\nUnit of Measure\n\n\n\n\nTemperature\ndegrees Fahrenheit\n\n\nHumidity\npercentage\n\n\nDew Point\ndegrees Fahrenheit\n\n\nPressure\ninches\n\n\nPrecipitation\ninches\n\n\nCumulative Precipitation\ninches\n\n\nWind Speed\nmph\n\n\nWind Gust\nmph\n\n\nWind Direction\ndirection of wind\n\n\nWeather Condition\nconditional\n\n\n\n\nGranularity of the data is in 1-hr intervals, which results in a total of 23160 observations per plant. The experiment implemented a 80-10-10 training, validation and test split.\n\n\n5.2 Models\n\n5.2.1 Industry Benchmark\nFor the industry benchmark, \\(P_{pred}\\) is defined as each solar power plant’s day-ahead hourly projection submission to the market operator (in this case \\(T_{t-24}\\) hours), and \\(P_{act}\\) is defined as the actual scheduled dispatch by the solar plant considering the market operator’s dispatching schedule. The comparison between the \\(P_{pred}\\) and the \\(P_{act}\\) were examined.\n\n\n5.2.2 Proposed Models\nFour proposed models were utilized in this study all focusing on the LSTM neural network. Networks 1 and 2 considered a single LSTM layer with nodes, 64 and 32 respectively, while networks 3 and 4 considered a stacked two layer LSTM network with nodes 64-32 and 32-64 respectively.\nAn additional feed-forward hidden layer with a rectified linear unit activation function was utilized at the end of all four networks, where the number of hidden nodes corresponded to the number of hidden nodes of the last LSTM layer.\nNetwork 1 and 4 utilized 32 nodes, while networks 2 and 3 utilized 64 nodes. All four networks considered the optimizer=RMSProp, dropouts=0.2 and recurrent dropouts=0.2, batch size=128, and an epoch setting=50."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-results",
    "href": "posts/2023-04-25-Solar/index.html#sec-results",
    "title": "Here Comes The Sun",
    "section": "6 Results",
    "text": "6 Results\n\n\nTable 4: Summary of Benchmark vs Model Performances\n\n\n\n\n\n\n\n\nPower Plant\nBest Model\nIndustry Benchmark Model\nPerformance Improvement\n\n\n\n\nClark Solar Power Plant\nLSTM+LSTM with 32 & 64 nodes\n2.86%\n12.01%\n\n\nFirst Toledo Solar Power Plant\nLSTM with 32 nodes\n4.30%\n15.34%\n\n\nSubic Solar Power Plant\nLSTM with 32 nodes\n1.36%\n3.64%\n\n\nValenzuela Solar Power Plant\nLSTM+LSTM with 32 & 64 nodes\n2.58%\n12.48%\n\n\n\n\nTable 2 shows the results of for each location. The stacked 2 layer LSTM with 32 and 64 nodes is the best performing model for the Clark Solar Power Plant and the Valenzuela Solar Power Plant, whereas the single layer LSTM model with 32 nodes best ts the First Toledo Solar Power Plant and the Subic Solar Power Plant.\nFor the Clark Solar Power Plant, the initial industry benchmark error was computed at 12.01%, and the 2 layer LSTM with 32 and 64 nodes model error is at 2.86%, which translates to a 76.19% error reduction.\nFor the First Toledo Solar Power Plant, a 15.34% industry benchmark error was computed, and the single layer LSTM with 32 nodes model produced only a 4.30% model error, which translates to a 71.97% error reduction.\nFor the Subic Solar Power Plant, the initial industry benchmark error was computed at 3.64%, and the single layer LSTM with 32 nodes model error is at 1.36%, which translates to a 62.64% error reduction.\nFor the Valenzuela Solar Power Plant, a 12.48% industry benchmark error was computed, and the two layer LSTM with 32 and 64 nodes model produced only a 2.58% model error, which translates to a 71.33% error reduction."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-conclusions",
    "href": "posts/2023-04-25-Solar/index.html#sec-conclusions",
    "title": "Here Comes The Sun",
    "section": "7 Conclusions & Recommendations",
    "text": "7 Conclusions & Recommendations\nImprovements brought upon by the utilization of deep learning models can reduce costs for both suppliers and the national grid operators, and increase the effciency of power generation.\nFurther studies can be conducted using di erent neural network architectures, varying numbers of nodes, layers, and optimizers of the LSTM layer, adding dense layers with different nodes. Including data that are main drivers of solar power generation, such as quality and angle of solar panels and the hourly irridiance, can drastically improve results.\nA study that produces one model for all solar power plants may also be considered. Location and descriptors of the power plant can be included which is fed to a CNN network together with an LSTM model will consider both spatio-temporal features."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-references",
    "href": "posts/2023-04-25-Solar/index.html#sec-references",
    "title": "Here Comes The Sun",
    "section": "8 References",
    "text": "8 References\n\n\nAlmeida, Marcelo Pinho, Oscar Perpiñán, and Luis Narvarte. 2015. “PV Power Forecast Using a Nonparametric PV Model.” Solar Energy 115: 354–68. https://doi.org/https://doi.org/10.1016/j.solener.2015.03.006.\n\n\nQing, Xiangyun, and Yugang Niu. 2018. “Hourly Day-Ahead Solar Irradiance Prediction Using Weather Forecasts by LSTM.” Energy 148: 461–68. https://doi.org/https://doi.org/10.1016/j.energy.2018.01.177.\n\n\nRassem, Aliaa, Mohammed A. El-Beltagy, and Mohamed Saleh. 2017. “Cross-Country Skiing Gears Classification Using Deep Learning.” ArXiv abs/1706.08924.\n\n\nSobri, Sobrina, Sam Koohi-Kamali, and Nasrudin Abd. Rahim. 2018. “Solar Photovoltaic Generation Forecasting Methods: A Review.” Energy Conversion and Management 156: 459–97. https://doi.org/https://doi.org/10.1016/j.enconman.2017.11.019.\n\n\nWang, Kejun, Xiaoxia Qi, and Hongda Liu. 2019. “A Comparison of Day-Ahead Photovoltaic Power Forecasting Models Based on Deep Learning Neural Network.” Applied Energy 251: 113315. https://doi.org/https://doi.org/10.1016/j.apenergy.2019.113315.\n\n\nZhang, Jinxia, Yuanying Chi, and Linpeng Xiao. 2018. “Solar Power Generation Forecast Based on LSTM.” In 2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS), 869–72. https://doi.org/10.1109/ICSESS.2018.8663788."
  },
  {
    "objectID": "posts/2023-04-25-Solar/source-files/notebooks/[DL] Here Comes the Sun - CEBU.html",
    "href": "posts/2023-04-25-Solar/source-files/notebooks/[DL] Here Comes the Sun - CEBU.html",
    "title": "Sandro Luis R. Silva",
    "section": "",
    "text": "Here Comes The Sun: Cebu Solar Power Plant\n\n\nLT16 - Sandro Silva and Jac Lin Yu\n\n\nTable of Contents\n\n1. Problem Statement\n2. Methodology\n\n2.1. Loading Prerequisites\n2.2. Dataset and Preprocessing\n2.3. Train, Validation, Test Split and Batch Generation\n2.4. Determining the Benchmark\n2.5. Model Training and Selection\n\n2.5.1 LSTM with 64 nodes\n2.5.2 LSTM with 32 nodes\n2.5.3 Stacked LSTM with 64 and 32 nodes\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\n3. Results and Discussions * 3.1. Results * 3.2. Discussions and Recommendations\n\n\n\n1. Problem Statement\n\n\nIn recent years, the global energy industry has changed its focus away from traditional fuel sources, such as oil and coal, to more alternative and sustainable solutions, such as hydroelectric, solar and wind. However, the shift towards renewable energy poses new threats and challenges to existing power grids across the world. One major concern surrounding renewable energy generation is the inherent variability and intermittency of its fuel source, as this can cause disruptions in power grids. Essentially, renewable energy technologies threaten to overwhelm the grid operators.  Renewable energy forecasting, in particular solar generation supply, may provide power grid operators the ability to predict and balance energy generation and consumption. In addition, power grid operators will be able to balance and schedule the distribution of generated power for not only renewable power plants but also conventional (and rigid) power plants, such as coal and natural gas.\n\n\n\n2. Methodology\n\n\n\n2.1 Loading Prerequisites\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n\n2.2 Dataset and Preprocessing\n\n\n Here, we will load the already cleaned preprocessed dataset of solar generation per plant per hour. The cleaning was processed outside of this notebook, and followed a framework which follows the industry standard of imputing missing values.\nTo list a few processes: - using the average between the before(t-1) and after hour(t+1) - using a 1-week average of all similar intervals (ex, if empty during hour 6 of dec 20, take the moving average of all hour 6 from dec 13-dec19) - using a 4-week moving average of that specific hour and day of the week (ex, if empty during hour 6 of dec 20, we take average of hour 6(dec 13, 6, 29, 22) \n\n\ndf = pd.read_excel('solar_dataset.xlsx', \n                   sheet_name='cebu', \n                   parse_dates=['DATETIME'])\n\n\n Here let us observe the different datatypes, notice that some columns are objects, some of which will be dropped. \n\n\ndf.dtypes\n\nDATETIME         datetime64[ns]\nYEAR                      int64\nMONTH                     int64\nDAY                       int64\nHOUR                      int64\nRESOURCE_ID              object\nMW                      float64\nLocation                 object\nTemperature             float64\nDew Point               float64\nHumidity                float64\nWind Speed              float64\nWind Gust                 int64\nPressure                float64\nCondition                object\nLocation_Name            object\ndtype: object\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nDATETIME\nYEAR\nMONTH\nDAY\nHOUR\nRESOURCE_ID\nMW\nLocation\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nCondition\nLocation_Name\n\n\n\n\n0\n2016-12-26 00:00:00\n2016\n12\n26\n1\n5TOLSOL_G01\n0.0\nRPVM\n79.0\n77.0\n94.0\n1.0\n0\n29.7\nMostly Cloudy\nMACTAN-CEBU\n\n\n1\n2016-12-26 01:00:00\n2016\n12\n26\n2\n5TOLSOL_G01\n0.0\nRPVM\n82.0\n79.0\n89.0\n5.0\n0\n29.8\nMostly Cloudy\nMACTAN-CEBU\n\n\n2\n2016-12-26 02:00:00\n2016\n12\n26\n3\n5TOLSOL_G01\n0.0\nRPVM\n84.0\n79.0\n84.0\n6.0\n0\n29.8\nMostly Cloudy\nMACTAN-CEBU\n\n\n\n\n\n\n\n\n It is customary in traditional time-series regression to convert the month, day, hour into categorical data, which will be later used in one-hot encoding. In addition, this is the usual preprocessing used in the energy industry here in the Philippines. \n\n\ndf.MONTH = df.MONTH.astype('str')\ndf.DAY = df.DAY.astype('str')\ndf.HOUR = df.HOUR.astype('str')\n\n\n Let us drop the unimportant columns \n\n\ndf.drop(['Location_Name', 'Location', \"YEAR\"],axis=1,inplace=True)\n\n\n Let us append the main df with the one-hot encoded dataframe \n\n\ndf = df.join(pd.get_dummies(df[['MONTH','DAY','HOUR','Condition']]))\n\n\n Let us get the unique list of power plants in the region \n\n\nlocations = df.RESOURCE_ID.unique()\n\n\n For this study, let us consider the first power plant \n\n\nlocations[0]\n\n'5TOLSOL_G01'\n\n\n\ndf0 = df[df.RESOURCE_ID == locations[0]].copy()\n\n\n Further dropping of columns as well as reordering of columns \n\n\ndf0 = df0.drop(['RESOURCE_ID','DATETIME','MONTH','DAY','HOUR','Condition'],\n               axis=1)\n\n\ndf0 = df0[['Temperature','MW'] + df0.columns[2:].tolist()].copy()\n\n\ndf0 = df0.astype('float')\n\n\n Final DataFrame to be used \n\n\ndf0.head()\n\n\n\n\n\n\n\n\nTemperature\nMW\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nMONTH_1\nMONTH_10\nMONTH_11\n...\nCondition_Partly Cloudy\nCondition_Partly Cloudy / Windy\nCondition_Rain\nCondition_Rain / Windy\nCondition_Rain Shower\nCondition_Showers in the Vicinity\nCondition_T-Storm\nCondition_Thunder\nCondition_Thunder / Windy\nCondition_Thunder in the Vicinity\n\n\n\n\n0\n79.0\n0.0\n77.0\n94.0\n1.0\n0.0\n29.7\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n82.0\n0.0\n79.0\n89.0\n5.0\n0.0\n29.8\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n84.0\n0.0\n79.0\n84.0\n6.0\n0.0\n29.8\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n84.0\n0.0\n77.0\n79.0\n7.0\n0.0\n29.7\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n86.0\n0.0\n77.0\n74.0\n8.0\n0.0\n29.7\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 99 columns\n\n\n\n\ndf0.shape\n\n(23160, 99)\n\n\n\nplant_capacity = 60\n\n\n\n 2.3 Train,Validate,Test Split and Batch Generation\n\n\n For this analysis, a .8, .1, .1, train-validate-test split was implemented. Since we are dealing with a time-series, the train-validate-test split cannot be randomly selected from the dataset, splitting of dataset should be conducted in a chronogical order. Below we define the starting and ending index for each group \n\n\ntrain_validate_test_split = [.8, .1, .1]\n\ntrain_idx = math.floor(df0.shape[0] * train_validate_test_split[0])\nval_idx = math.floor(df0.shape[0] * train_validate_test_split[1]) + train_idx\n\n\n The generator function below was utilized to yield the appropriate batch. Significant parameters include the lookback, or how far back of the data to consider, delay, or how far into the future are we forecasting, batch_size, refers to the number of training examples utilized in one iteration. \n\n\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, \n              batch_size=128, step=1):\n    \n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, \n                                     max_index, size=batch_size)\n        else:\n            if i + batch_size &gt;= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets\n\n\n Let us mean-center the dataset \n\n\nfloat_data = df0.values\ntrain_mean = float_data[:train_idx].mean(axis=0)\nfloat_data -= train_mean\ntrain_std = float_data[:train_idx].std(axis=0)\nfloat_data /= train_std\ntrain_mw_std = train_std[1]\n\n\n Let us initialize the parameters which will be used for the generation An industry standard practic is to utilize a lookback of either 144 hours (1 week), 288 hours (2 weeks), or 720 hours (1[30-day] month). The initial run was to utilized a 288 hour lookback period due to group’s industry expert’s experience. However, outside consultation with industry experts said 1 week would suffice with this limited dataset. \n\n\nlookback = 144 # lookback(consider) the previous week data\nstep = 1 # in hourly granularity\ndelay = 24 # to forecast the next 24 hours\nbatch_size = 128\n\n\ntrain_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=train_idx,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size)\n\nval_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=train_idx+1,\n    max_index=val_idx,\n    step=step,\n    batch_size=batch_size)\n\ntest_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=val_idx,\n    max_index=None,\n    step=step,\n    batch_size=batch_size)\n\nval_steps = (val_idx - (train_idx+1) - lookback)\ntest_steps = (len(float_data) - (val_idx+1) - lookback)\n\n\n\n2.4 Benchmark\n\n\n The team used two metrics to establish a baseline error:\n(1) the naive method identified in a previous lecture, which we scaled to industry standard\n(2) a different framework suggested by industry\n\n\n\ndef evaluate_naive_method():\n\n    batch_maes = []\n    for step in range(val_steps):\n        if step % 1000 == 0:\n            print(step)\n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    \n    return np.mean(batch_maes)\n\nnaive_method = evaluate_naive_method()\n\n0\n1000\n2000\n\n\n\nprint('NAIVE METHOD 1:', f'{((naive_method * train_mw_std)/plant_capacity):0.2%}') #where 22 is the capacity of the plant\n\nNAIVE METHOD 1: 7.67%\n\n\n\nbenchmark1 = ((naive_method * train_mw_std)/plant_capacity) * 100\n\n\n For establishing the 2nd baseline error, the team consulted the industry to determine a baseline error. However, even the industry is still conflicted on how to establish a baseline measurement. As of now, the baseline is still being established by policy makers and grid operators. \nLuckily, consulting with industry experts gave the team a framework on how to establish a baseline error per plant. The resulting measure looked at the mean squared error of the day-ahead projection(DAP), also called scheduled (Ex-Ante or RTD), and the actual delivered (Ex-Post or RTX).\nThis was then scaled as a percentage to the capacity of the plant. This was with processed with an industry expert and not included in the notebook. The yielding error resulted with a baseline of 15.34% \n\n\nbenchmark2 = 15.34\n\n\n\n2.5 Model Training Selection\n\n\n The team utilized 4 models:\n- LSTM with 64 nodes\n- LSTM with 32 nodes\n- Stacked LSTM(2 layer) with 64 and 32 nodes\n- Stacked LSTM(2 layer) with 32 and 64 nodes\nDropout of 0.2 and Recurrent Dropout of 0.2 and a RMSProp optimized was implemented for all three.\nThe group decided to use a Epoch=50, with an EarlyStopping of patience=10 and min_delta=0.01 to avoid overfitting and to reduce runtime due to the limited time constraints. \n\n\nsteps_per_epoch_cnt = math.floor(train_idx/batch_size)\nval_steps_cnt = round(math.floor((val_idx-train_idx)/batch_size),-1)\n\n\n\n2.5.1 LSTM with 64\n\n\nfilepath_lstm64=\"Cebu_best_lstm64.hdf5\" \ncp_lstm64 = ModelCheckpoint(filepath_lstm64, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm64 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm64 = [cp_lstm64, es_lstm64]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm64 = Sequential()\nmodel_lstm64.add(layers.LSTM(64, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm64.add(layers.Dense(64,activation='relu'))\nmodel_lstm64.add(layers.Dense(1))\nmodel_lstm64.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm64 = model_lstm64.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm64, verbose=2) \n\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2050\n\nEpoch 00001: val_loss improved from inf to 0.20503, saving model to Cebu_best_lstm64.hdf5\n - 22s - loss: 0.3745 - val_loss: 0.2050\nEpoch 2/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.2099\n\nEpoch 00002: val_loss did not improve from 0.20503\n - 19s - loss: 0.2448 - val_loss: 0.2099\nEpoch 3/50\n20/20 [==============================] - 1s 44ms/step - loss: 0.1907\n\nEpoch 00003: val_loss improved from 0.20503 to 0.19068, saving model to Cebu_best_lstm64.hdf5\n - 19s - loss: 0.2185 - val_loss: 0.1907\nEpoch 4/50\n20/20 [==============================] - 1s 44ms/step - loss: 0.1947\n\nEpoch 00004: val_loss did not improve from 0.19068\n - 21s - loss: 0.1975 - val_loss: 0.1947\nEpoch 5/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1928\n\nEpoch 00005: val_loss did not improve from 0.19068\n - 20s - loss: 0.1910 - val_loss: 0.1928\nEpoch 6/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.1991\n\nEpoch 00006: val_loss did not improve from 0.19068\n - 20s - loss: 0.1823 - val_loss: 0.1991\nEpoch 7/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.2138\n\nEpoch 00007: val_loss did not improve from 0.19068\n - 23s - loss: 0.1713 - val_loss: 0.2138\nEpoch 8/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1954\n\nEpoch 00008: val_loss did not improve from 0.19068\n - 20s - loss: 0.1680 - val_loss: 0.1954\nEpoch 9/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.2211\n\nEpoch 00009: val_loss did not improve from 0.19068\n - 19s - loss: 0.1647 - val_loss: 0.2211\nEpoch 10/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.2042\n\nEpoch 00010: val_loss did not improve from 0.19068\n - 23s - loss: 0.1567 - val_loss: 0.2042\nEpoch 11/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.2127\n\nEpoch 00011: val_loss did not improve from 0.19068\n - 20s - loss: 0.1566 - val_loss: 0.2127\nEpoch 12/50\n20/20 [==============================] - 1s 56ms/step - loss: 0.2272\n\nEpoch 00012: val_loss did not improve from 0.19068\n - 22s - loss: 0.1541 - val_loss: 0.2272\nEpoch 13/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.2127\n\nEpoch 00013: val_loss did not improve from 0.19068\n - 21s - loss: 0.1491 - val_loss: 0.2127\nEpoch 00013: early stopping\n\n\n\nloss_lstm64 = history_lstm64.history['loss']\nval_loss_lstm64 = history_lstm64.history['val_loss']\n\nepochs = range(1, len(loss_lstm64) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm64, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm64, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm64) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 4.87%\n\n\n\nlstm64 = ((min(val_loss_lstm64) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.2 LSTM with 32 nodes\n\n\nfilepath_lstm32=\"Cebu_best_lstm32.hdf5\" \ncp_lstm32 = ModelCheckpoint(filepath_lstm32, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm32 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm32 = [cp_lstm32, es_lstm32]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm32 = Sequential()\nmodel_lstm32.add(layers.LSTM(32, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm32.add(layers.Dense(32, activation='relu'))\nmodel_lstm32.add(layers.Dense(1))\nmodel_lstm32.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm32 = model_lstm32.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm32, verbose=2) \n\nEpoch 1/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2389\n\nEpoch 00001: val_loss improved from inf to 0.23892, saving model to Cebu_best_lstm32.hdf5\n - 18s - loss: 0.4182 - val_loss: 0.2389\nEpoch 2/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1922\n\nEpoch 00002: val_loss improved from 0.23892 to 0.19216, saving model to Cebu_best_lstm32.hdf5\n - 16s - loss: 0.2575 - val_loss: 0.1922\nEpoch 3/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1897\n\nEpoch 00003: val_loss improved from 0.19216 to 0.18967, saving model to Cebu_best_lstm32.hdf5\n - 16s - loss: 0.2250 - val_loss: 0.1897\nEpoch 4/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1686\n\nEpoch 00004: val_loss improved from 0.18967 to 0.16857, saving model to Cebu_best_lstm32.hdf5\n - 17s - loss: 0.2017 - val_loss: 0.1686\nEpoch 5/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.1814\n\nEpoch 00005: val_loss did not improve from 0.16857\n - 17s - loss: 0.1978 - val_loss: 0.1814\nEpoch 6/50\n20/20 [==============================] - 1s 36ms/step - loss: 0.1943\n\nEpoch 00006: val_loss did not improve from 0.16857\n - 16s - loss: 0.1892 - val_loss: 0.1943\nEpoch 7/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1845\n\nEpoch 00007: val_loss did not improve from 0.16857\n - 17s - loss: 0.1793 - val_loss: 0.1845\nEpoch 8/50\n20/20 [==============================] - 1s 47ms/step - loss: 0.1899\n\nEpoch 00008: val_loss did not improve from 0.16857\n - 18s - loss: 0.1751 - val_loss: 0.1899\nEpoch 9/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.2050\n\nEpoch 00009: val_loss did not improve from 0.16857\n - 20s - loss: 0.1721 - val_loss: 0.2050\nEpoch 10/50\n20/20 [==============================] - 1s 32ms/step - loss: 0.1828\n\nEpoch 00010: val_loss did not improve from 0.16857\n - 17s - loss: 0.1644 - val_loss: 0.1828\nEpoch 11/50\n20/20 [==============================] - 1s 37ms/step - loss: 0.1875\n\nEpoch 00011: val_loss did not improve from 0.16857\n - 16s - loss: 0.1676 - val_loss: 0.1875\nEpoch 12/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1935\n\nEpoch 00012: val_loss did not improve from 0.16857\n - 17s - loss: 0.1646 - val_loss: 0.1935\nEpoch 13/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1911\n\nEpoch 00013: val_loss did not improve from 0.16857\n - 17s - loss: 0.1607 - val_loss: 0.1911\nEpoch 14/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1989\n\nEpoch 00014: val_loss did not improve from 0.16857\n - 17s - loss: 0.1528 - val_loss: 0.1989\nEpoch 00014: early stopping\n\n\n\nloss_lstm32 = history_lstm32.history['loss']\nval_loss_lstm32 = history_lstm32.history['val_loss']\n\nepochs = range(1, len(loss_lstm32) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm32, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm32, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm32) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 4.30%\n\n\n\nlstm32 = ((min(val_loss_lstm32) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.3 Stacked LSTM with 64 and 32 nodes\n\n\nfilepath_lstm6432=\"Cebu_best_lstm6432.hdf5\" \ncp_lstm6432 = ModelCheckpoint(filepath_lstm6432, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm6432 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm6432 = [cp_lstm6432, es_lstm6432]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm6432 = Sequential()\nmodel_lstm6432.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm6432.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm6432.add(layers.Dense(32, activation='relu'))\nmodel_lstm6432.add(layers.Dense(1))\nmodel_lstm6432.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm6432 = model_lstm6432.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm6432, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 93ms/step - loss: 0.2058\n\nEpoch 00001: val_loss improved from inf to 0.20581, saving model to Cebu_best_lstm6432.hdf5\n - 40s - loss: 0.3520 - val_loss: 0.2058\nEpoch 2/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2092\n\nEpoch 00002: val_loss did not improve from 0.20581\n - 39s - loss: 0.2220 - val_loss: 0.2092\nEpoch 3/50\n20/20 [==============================] - 2s 80ms/step - loss: 0.2094\n\nEpoch 00003: val_loss did not improve from 0.20581\n - 41s - loss: 0.2063 - val_loss: 0.2094\nEpoch 4/50\n20/20 [==============================] - 1s 61ms/step - loss: 0.1837\n\nEpoch 00004: val_loss improved from 0.20581 to 0.18374, saving model to Cebu_best_lstm6432.hdf5\n - 38s - loss: 0.1854 - val_loss: 0.1837\nEpoch 5/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1957\n\nEpoch 00005: val_loss did not improve from 0.18374\n - 37s - loss: 0.1823 - val_loss: 0.1957\nEpoch 6/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1878\n\nEpoch 00006: val_loss did not improve from 0.18374\n - 36s - loss: 0.1751 - val_loss: 0.1878\nEpoch 7/50\n20/20 [==============================] - 2s 76ms/step - loss: 0.1899\n\nEpoch 00007: val_loss did not improve from 0.18374\n - 39s - loss: 0.1630 - val_loss: 0.1899\nEpoch 8/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2210\n\nEpoch 00008: val_loss did not improve from 0.18374\n - 39s - loss: 0.1590 - val_loss: 0.2210\nEpoch 9/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.2153\n\nEpoch 00009: val_loss did not improve from 0.18374\n - 37s - loss: 0.1539 - val_loss: 0.2153\nEpoch 10/50\n20/20 [==============================] - 1s 72ms/step - loss: 0.2149\n\nEpoch 00010: val_loss did not improve from 0.18374\n - 38s - loss: 0.1487 - val_loss: 0.2149\nEpoch 11/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2119\n\nEpoch 00011: val_loss did not improve from 0.18374\n - 36s - loss: 0.1475 - val_loss: 0.2119\nEpoch 12/50\n20/20 [==============================] - 1s 75ms/step - loss: 0.2246\n\nEpoch 00012: val_loss did not improve from 0.18374\n - 37s - loss: 0.1471 - val_loss: 0.2246\nEpoch 13/50\n20/20 [==============================] - 1s 62ms/step - loss: 0.2063\n\nEpoch 00013: val_loss did not improve from 0.18374\n - 37s - loss: 0.1418 - val_loss: 0.2063\nEpoch 14/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2216\n\nEpoch 00014: val_loss did not improve from 0.18374\n - 38s - loss: 0.1352 - val_loss: 0.2216\nEpoch 00014: early stopping\n\n\n\nloss_lstm6432 = history_lstm6432.history['loss']\nval_loss_lstm6432 = history_lstm6432.history['val_loss']\n\nepochs = range(1, len(loss_lstm6432) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm6432, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm6432, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm6432) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 4.69%\n\n\n\nlstm6432 = ((min(val_loss_lstm6432) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\nfilepath_lstm3264 =\"Cebu_best_lstm3264.hdf5\" \ncp_lstm3264 = ModelCheckpoint(filepath_lstm3264, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm3264 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm3264 = [cp_lstm3264, es_lstm3264]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm3264 = Sequential()\nmodel_lstm3264.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm3264.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm3264.add(layers.Dense(64, activation='relu'))\nmodel_lstm3264.add(layers.Dense(1))\nmodel_lstm3264.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm3264 = model_lstm3264.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm3264, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 1s 71ms/step - loss: 0.1866\n\nEpoch 00001: val_loss improved from inf to 0.18658, saving model to Cebu_best_lstm3264.hdf5\n - 38s - loss: 0.3441 - val_loss: 0.1866\nEpoch 2/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.2028\n\nEpoch 00002: val_loss did not improve from 0.18658\n - 34s - loss: 0.2206 - val_loss: 0.2028\nEpoch 3/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2007\n\nEpoch 00003: val_loss did not improve from 0.18658\n - 36s - loss: 0.2065 - val_loss: 0.2007\nEpoch 4/50\n20/20 [==============================] - 1s 70ms/step - loss: 0.2070\n\nEpoch 00004: val_loss did not improve from 0.18658\n - 38s - loss: 0.1875 - val_loss: 0.2070\nEpoch 5/50\n20/20 [==============================] - 1s 74ms/step - loss: 0.2092\n\nEpoch 00005: val_loss did not improve from 0.18658\n - 37s - loss: 0.1841 - val_loss: 0.2092\nEpoch 6/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.1759\n\nEpoch 00006: val_loss improved from 0.18658 to 0.17589, saving model to Cebu_best_lstm3264.hdf5\n - 36s - loss: 0.1766 - val_loss: 0.1759\nEpoch 7/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1925\n\nEpoch 00007: val_loss did not improve from 0.17589\n - 37s - loss: 0.1656 - val_loss: 0.1925\nEpoch 8/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1958\n\nEpoch 00008: val_loss did not improve from 0.17589\n - 37s - loss: 0.1625 - val_loss: 0.1958\nEpoch 9/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.1980\n\nEpoch 00009: val_loss did not improve from 0.17589\n - 35s - loss: 0.1564 - val_loss: 0.1980\nEpoch 10/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.2024\n\nEpoch 00010: val_loss did not improve from 0.17589\n - 34s - loss: 0.1515 - val_loss: 0.2024\nEpoch 11/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1945\n\nEpoch 00011: val_loss did not improve from 0.17589\n - 35s - loss: 0.1507 - val_loss: 0.1945\nEpoch 12/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2235\n\nEpoch 00012: val_loss did not improve from 0.17589\n - 37s - loss: 0.1500 - val_loss: 0.2235\nEpoch 13/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2137\n\nEpoch 00013: val_loss did not improve from 0.17589\n - 38s - loss: 0.1436 - val_loss: 0.2137\nEpoch 14/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2093\n\nEpoch 00014: val_loss did not improve from 0.17589\n - 37s - loss: 0.1375 - val_loss: 0.2093\nEpoch 15/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2092\n\nEpoch 00015: val_loss did not improve from 0.17589\n - 40s - loss: 0.1401 - val_loss: 0.2092\nEpoch 16/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1887\n\nEpoch 00016: val_loss did not improve from 0.17589\n - 37s - loss: 0.1418 - val_loss: 0.1887\nEpoch 00016: early stopping\n\n\n\nloss_lstm3264 = history_lstm3264.history['loss']\nval_loss_lstm3264 = history_lstm3264.history['val_loss']\n\nepochs = range(1, len(loss_lstm3264) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm3264, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm3264, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm3264) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 4.49%\n\n\n\nlstm3264 = ((min(val_loss_lstm3264) * train_mw_std)/plant_capacity) * 100\n\n\nlstm3264\n\n0.0448781794602682\n\n\n\n\n3. Results and Discussions\n\n\n\n3.1 Results\n\n\nnames = ['Industry Agnostic Naive Method',\n         'Industry Standard Naive Method', \n         'LSTM with 64 nodes',\n         'LSTM with 32 nodes',\n         'LSTM+LSTM with 64 nodes and 32 nodes',\n         'LSTM+LSTM with 32 nodes and 64 nodes']\nresults = [benchmark1, \n           benchmark2,\n           lstm64,\n           lstm32,\n           lstm6432,\n           lstm3264]\n\n\nresults = pd.DataFrame(zip(names,results), columns=['Model', 'Error in Percentage'])\n\n\nresults['Error in Percentage'] = round(results['Error in Percentage'],2)\n\n\nresults['Improvement'] = round(1-(results['Error in Percentage']/benchmark2),4)*100\n\n\nresults\n\n\n\n\n\n\n\n\nModel\nError in Percentage\nImprovement\n\n\n\n\n0\nIndustry Agnostic Naive Method\n7.67\n50.00\n\n\n1\nIndustry Standard Naive Method\n15.34\n0.00\n\n\n2\nLSTM with 64 nodes\n4.87\n68.25\n\n\n3\nLSTM with 32 nodes\n4.30\n71.97\n\n\n4\nLSTM+LSTM with 64 nodes and 32 nodes\n4.69\n69.43\n\n\n5\nLSTM+LSTM with 32 nodes and 64 nodes\n4.49\n70.73"
  },
  {
    "objectID": "posts/2023-04-25-Solar/source-files/notebooks/[DL] Here Comes the Sun - METROMANILA.html",
    "href": "posts/2023-04-25-Solar/source-files/notebooks/[DL] Here Comes the Sun - METROMANILA.html",
    "title": "Sandro Luis R. Silva",
    "section": "",
    "text": "Here Comes The Sun: Clark Solar Power Plant\n\n\nLT16 - Sandro Silva and Jac Lin Yu\n\n\nTable of Contents\n\n1. Problem Statement\n2. Methodology\n\n2.1. Loading Prerequisites\n2.2. Dataset and Preprocessing\n2.3. Train, Validation, Test Split and Batch Generation\n2.4. Determining the Benchmark\n2.5. Model Training and Selection\n\n2.5.1 LSTM with 64 nodes\n2.5.2 LSTM with 32 nodes\n2.5.3 Stacked LSTM with 64 and 32 nodes\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\n3. Results and Discussions * 3.1. Results * 3.2. Discussions and Recommendations\n\n\n\n1. Problem Statement\n\n\nIn recent years, the global energy industry has changed its focus away from traditional fuel sources, such as oil and coal, to more alternative and sustainable solutions, such as hydroelectric, solar and wind. However, the shift towards renewable energy poses new threats and challenges to existing power grids across the world. One major concern surrounding renewable energy generation is the inherent variability and intermittency of its fuel source, as this can cause disruptions in power grids. Essentially, renewable energy technologies threaten to overwhelm the grid operators.  Renewable energy forecasting, in particular solar generation supply, may provide power grid operators the ability to predict and balance energy generation and consumption. In addition, power grid operators will be able to balance and schedule the distribution of generated power for not only renewable power plants but also conventional (and rigid) power plants, such as coal and natural gas.\n\n\n\n2. Methodology\n\n\n\n2.1 Loading Prerequisites\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n\n2.2 Dataset and Preprocessing\n\n\n Here, we will load the already cleaned preprocessed dataset of solar generation per plant per hour. The cleaning was processed outside of this notebook, and followed a framework which follows the industry standard of imputing missing values.\nTo list a few processes: - using the average between the before(t-1) and after hour(t+1) - using a 1-week average of all similar intervals (ex, if empty during hour 6 of dec 20, take the moving average of all hour 6 from dec 13-dec19) - using a 4-week moving average of that specific hour and day of the week (ex, if empty during hour 6 of dec 20, we take average of hour 6(dec 13, 6, 29, 22) \n\n\ndf = pd.read_excel('solar_dataset.xlsx', \n                   sheet_name='metro_manila', \n                   parse_dates=['DATETIME'])\n\n\n Here let us observe the different datatypes, notice that some columns are objects, some of which will be dropped. \n\n\ndf.dtypes\n\nDATETIME         datetime64[ns]\nYEAR                      int64\nMONTH                     int64\nDAY                       int64\nHOUR                      int64\nRESOURCE_ID              object\nMW                      float64\nLocation                 object\nTemperature             float64\nDew Point               float64\nHumidity                float64\nWind Speed                int64\nWind Gust                 int64\nPressure                float64\nCondition                object\nLocation_Name            object\ndtype: object\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nDATETIME\nYEAR\nMONTH\nDAY\nHOUR\nRESOURCE_ID\nMW\nLocation\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nCondition\nLocation_Name\n\n\n\n\n0\n2016-12-26 00:00:00\n2016\n12\n26\n1\n2VALSOL_G01\n0.0\nRPLL\n77.0\n73.0\n89.0\n20\n31\n29.6\nLight Rain\nNINOY AQUINO INT\n\n\n1\n2016-12-26 01:00:00\n2016\n12\n26\n2\n2VALSOL_G01\n0.0\nRPLL\n77.0\n75.0\n94.0\n15\n0\n29.6\nLight Rain\nNINOY AQUINO INT\n\n\n2\n2016-12-26 02:00:00\n2016\n12\n26\n3\n2VALSOL_G01\n0.0\nRPLL\n75.0\n75.0\n100.0\n15\n0\n29.6\nLight Rain\nNINOY AQUINO INT\n\n\n\n\n\n\n\n\n It is customary in traditional time-series regression to convert the month, day, hour into categorical data, which will be later used in one-hot encoding. In addition, this is the usual preprocessing used in the energy industry here in the Philippines. \n\n\ndf.MONTH = df.MONTH.astype('str')\ndf.DAY = df.DAY.astype('str')\ndf.HOUR = df.HOUR.astype('str')\n\n\n Let us drop the unimportant columns \n\n\ndf.drop(['Location_Name', 'Location', \"YEAR\"],axis=1,inplace=True)\n\n\n Let us append the main df with the one-hot encoded dataframe \n\n\ndf = df.join(pd.get_dummies(df[['MONTH','DAY','HOUR','Condition']]))\n\n\n Let us get the unique list of power plants in the region \n\n\nlocations = df.RESOURCE_ID.unique()\n\n\n For this study, let us consider the first power plant \n\n\nlocations[0]\n\n'2VALSOL_G01'\n\n\n\ndf0 = df[df.RESOURCE_ID == locations[0]].copy()\n\n\n Further dropping of columns as well as reordering of columns \n\n\ndf0 = df0.drop(['RESOURCE_ID','DATETIME','MONTH','DAY','HOUR','Condition'],\n               axis=1)\n\n\ndf0 = df0[['Temperature','MW'] + df0.columns[2:].tolist()].copy()\n\n\ndf0 = df0.astype('float')\n\n\n Final DataFrame to be used \n\n\ndf0.head()\n\n\n\n\n\n\n\n\nTemperature\nMW\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nMONTH_1\nMONTH_10\nMONTH_11\n...\nCondition_Mostly Cloudy / Windy\nCondition_Partly Cloudy\nCondition_Partly Cloudy / Windy\nCondition_Rain\nCondition_Rain / Windy\nCondition_Rain Shower\nCondition_Showers in the Vicinity\nCondition_T-Storm\nCondition_Thunder\nCondition_Thunder in the Vicinity\n\n\n\n\n0\n77.0\n0.0\n73.0\n89.0\n20.0\n31.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n77.0\n0.0\n75.0\n94.0\n15.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n75.0\n0.0\n75.0\n100.0\n15.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n75.0\n0.0\n75.0\n100.0\n16.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n77.0\n0.0\n77.0\n100.0\n13.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 99 columns\n\n\n\n\ndf0.shape\n\n(24021, 99)\n\n\n\nplant_capacity = 8.5\n\n\n\n 2.3 Train,Validate,Test Split and Batch Generation\n\n\n For this analysis, a .8, .1, .1, train-validate-test split was implemented. Since we are dealing with a time-series, the train-validate-test split cannot be randomly selected from the dataset, splitting of dataset should be conducted in a chronogical order. Below we define the starting and ending index for each group \n\n\ntrain_validate_test_split = [.8, .1, .1]\n\ntrain_idx = math.floor(df0.shape[0] * train_validate_test_split[0])\nval_idx = math.floor(df0.shape[0] * train_validate_test_split[1]) + train_idx\n\n\n The generator function below was utilized to yield the appropriate batch. Significant parameters include the lookback, or how far back of the data to consider, delay, or how far into the future are we forecasting, batch_size, refers to the number of training examples utilized in one iteration. \n\n\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, \n              batch_size=128, step=1):\n    \n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, \n                                     max_index, size=batch_size)\n        else:\n            if i + batch_size &gt;= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets\n\n\n Let us mean-center the dataset \n\n\nfloat_data = df0.values\ntrain_mean = float_data[:train_idx].mean(axis=0)\nfloat_data -= train_mean\ntrain_std = float_data[:train_idx].std(axis=0)\nfloat_data /= train_std\ntrain_mw_std = train_std[1]\n\n\n Let us initialize the parameters which will be used for the generation An industry standard practic is to utilize a lookback of either 144 hours (1 week), 288 hours (2 weeks), or 720 hours (1[30-day] month). The initial run was to utilized a 288 hour lookback period due to group’s industry expert’s experience. However, outside consultation with industry experts said 1 week would suffice with this limited dataset. \n\n\nlookback = 144 # lookback(consider) the previous week data\nstep = 1 # in hourly granularity\ndelay = 24 # to forecast the next 24 hours\nbatch_size = 128\n\n\ntrain_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=train_idx,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size)\n\nval_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=train_idx+1,\n    max_index=val_idx,\n    step=step,\n    batch_size=batch_size)\n\ntest_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=val_idx,\n    max_index=None,\n    step=step,\n    batch_size=batch_size)\n\nval_steps = (val_idx - (train_idx+1) - lookback)\ntest_steps = (len(float_data) - (val_idx+1) - lookback)\n\n\n\n2.4 Benchmark\n\n\n The team used two metrics to establish a baseline error:\n(1) the naive method identified in a previous lecture, which we scaled to industry standard\n(2) a different framework suggested by industry\n\n\n\ndef evaluate_naive_method():\n\n    batch_maes = []\n    for step in range(val_steps):\n        if step % 1000 == 0:\n            print(step)\n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    \n    return np.mean(batch_maes)\n\nnaive_method = evaluate_naive_method()\n\n0\n1000\n2000\n\n\n\nprint('NAIVE METHOD 1:', f'{((naive_method * train_mw_std)/plant_capacity):0.2%}') #where 22 is the capacity of the plant\n\nNAIVE METHOD 1: 5.84%\n\n\n\nbenchmark1 = ((naive_method * train_mw_std)/plant_capacity) * 100\n\n\n For establishing the 2nd baseline error, the team consulted the industry to determine a baseline error. However, even the industry is still conflicted on how to establish a baseline measurement. As of now, the baseline is still being established by policy makers and grid operators. \nLuckily, consulting with industry experts gave the team a framework on how to establish a baseline error per plant. The resulting measure looked at the mean squared error of the day-ahead projection(DAP), also called scheduled (Ex-Ante or RTD), and the actual delivered (Ex-Post or RTX).\nThis was then scaled as a percentage to the capacity of the plant. This was with processed with an industry expert and not included in the notebook. The yielding error resulted with a baseline of 12.48% \n\n\nbenchmark2 = 12.48\n\n\n\n2.5 Model Training Selection\n\n\n The team utilized 4 models:\n- LSTM with 64 nodes\n- LSTM with 32 nodes\n- Stacked LSTM(2 layer) with 64 and 32 nodes\n- Stacked LSTM(2 layer) with 32 and 64 nodes\nDropout of 0.2 and Recurrent Dropout of 0.2 and a RMSProp optimized was implemented for all three.\nThe group decided to use a Epoch=50, with an EarlyStopping of patience=10 and min_delta=0.01 to avoid overfitting and to reduce runtime due to the limited time constraints. \n\n\nsteps_per_epoch_cnt = math.floor(train_idx/batch_size)\nval_steps_cnt = round(math.floor((val_idx-train_idx)/batch_size),-1)\n\n\n\n2.5.1 LSTM with 64\n\n\nfilepath_lstm64=\"clark_best_lstm64.hdf5\" \ncp_lstm64 = ModelCheckpoint(filepath_lstm64, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm64 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm64 = [cp_lstm64, es_lstm64]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm64 = Sequential()\nmodel_lstm64.add(layers.LSTM(64, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm64.add(layers.Dense(64,activation='relu'))\nmodel_lstm64.add(layers.Dense(1))\nmodel_lstm64.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm64 = model_lstm64.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm64, verbose=2) \n\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n20/20 [==============================] - 1s 48ms/step - loss: 0.1711\n\nEpoch 00001: val_loss improved from inf to 0.17113, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.3853 - val_loss: 0.1711\nEpoch 2/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1504\n\nEpoch 00002: val_loss improved from 0.17113 to 0.15042, saving model to clark_best_lstm64.hdf5\n - 22s - loss: 0.2661 - val_loss: 0.1504\nEpoch 3/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.1373\n\nEpoch 00003: val_loss improved from 0.15042 to 0.13729, saving model to clark_best_lstm64.hdf5\n - 22s - loss: 0.2066 - val_loss: 0.1373\nEpoch 4/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.1295\n\nEpoch 00004: val_loss improved from 0.13729 to 0.12954, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.1742 - val_loss: 0.1295\nEpoch 5/50\n20/20 [==============================] - 1s 50ms/step - loss: 0.1284\n\nEpoch 00005: val_loss improved from 0.12954 to 0.12843, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.1633 - val_loss: 0.1284\nEpoch 6/50\n20/20 [==============================] - 1s 47ms/step - loss: 0.1436\n\nEpoch 00006: val_loss did not improve from 0.12843\n - 22s - loss: 0.1576 - val_loss: 0.1436\nEpoch 7/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.1292\n\nEpoch 00007: val_loss did not improve from 0.12843\n - 22s - loss: 0.1494 - val_loss: 0.1292\nEpoch 8/50\n20/20 [==============================] - 1s 49ms/step - loss: 0.1321\n\nEpoch 00008: val_loss did not improve from 0.12843\n - 21s - loss: 0.1475 - val_loss: 0.1321\nEpoch 9/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.1304\n\nEpoch 00009: val_loss did not improve from 0.12843\n - 22s - loss: 0.1404 - val_loss: 0.1304\nEpoch 10/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1405\n\nEpoch 00010: val_loss did not improve from 0.12843\n - 21s - loss: 0.1370 - val_loss: 0.1405\nEpoch 11/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1361\n\nEpoch 00011: val_loss did not improve from 0.12843\n - 21s - loss: 0.1375 - val_loss: 0.1361\nEpoch 12/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.1294\n\nEpoch 00012: val_loss did not improve from 0.12843\n - 20s - loss: 0.1328 - val_loss: 0.1294\nEpoch 13/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1366\n\nEpoch 00013: val_loss did not improve from 0.12843\n - 20s - loss: 0.1295 - val_loss: 0.1366\nEpoch 00013: early stopping\n\n\n\nloss_lstm64 = history_lstm64.history['loss']\nval_loss_lstm64 = history_lstm64.history['val_loss']\n\nepochs = range(1, len(loss_lstm64) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm64, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm64, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm64) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.82%\n\n\n\nlstm64 = ((min(val_loss_lstm64) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.2 LSTM with 32 nodes\n\n\nfilepath_lstm32=\"clark_best_lstm32.hdf5\" \ncp_lstm32 = ModelCheckpoint(filepath_lstm32, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm32 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm32 = [cp_lstm32, es_lstm32]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm32 = Sequential()\nmodel_lstm32.add(layers.LSTM(32, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm32.add(layers.Dense(32, activation='relu'))\nmodel_lstm32.add(layers.Dense(1))\nmodel_lstm32.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm32 = model_lstm32.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm32, verbose=2) \n\nEpoch 1/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1862\n\nEpoch 00001: val_loss improved from inf to 0.18618, saving model to clark_best_lstm32.hdf5\n - 18s - loss: 0.4808 - val_loss: 0.1862\nEpoch 2/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1496\n\nEpoch 00002: val_loss improved from 0.18618 to 0.14965, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.3122 - val_loss: 0.1496\nEpoch 3/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1462\n\nEpoch 00003: val_loss improved from 0.14965 to 0.14625, saving model to clark_best_lstm32.hdf5\n - 18s - loss: 0.2731 - val_loss: 0.1462\nEpoch 4/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.1438\n\nEpoch 00004: val_loss improved from 0.14625 to 0.14383, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.2481 - val_loss: 0.1438\nEpoch 5/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1364\n\nEpoch 00005: val_loss improved from 0.14383 to 0.13638, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.2247 - val_loss: 0.1364\nEpoch 6/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1361\n\nEpoch 00006: val_loss improved from 0.13638 to 0.13614, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.2101 - val_loss: 0.1361\nEpoch 7/50\n20/20 [==============================] - 1s 46ms/step - loss: 0.1266\n\nEpoch 00007: val_loss improved from 0.13614 to 0.12662, saving model to clark_best_lstm32.hdf5\n - 19s - loss: 0.1899 - val_loss: 0.1266\nEpoch 8/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1360\n\nEpoch 00008: val_loss did not improve from 0.12662\n - 17s - loss: 0.1716 - val_loss: 0.1360\nEpoch 9/50\n20/20 [==============================] - 1s 36ms/step - loss: 0.1264\n\nEpoch 00009: val_loss improved from 0.12662 to 0.12641, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.1583 - val_loss: 0.1264\nEpoch 10/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1375\n\nEpoch 00010: val_loss did not improve from 0.12641\n - 16s - loss: 0.1496 - val_loss: 0.1375\nEpoch 11/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1290\n\nEpoch 00011: val_loss did not improve from 0.12641\n - 17s - loss: 0.1506 - val_loss: 0.1290\nEpoch 12/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1341\n\nEpoch 00012: val_loss did not improve from 0.12641\n - 17s - loss: 0.1457 - val_loss: 0.1341\nEpoch 13/50\n20/20 [==============================] - 1s 54ms/step - loss: 0.1269\n\nEpoch 00013: val_loss did not improve from 0.12641\n - 19s - loss: 0.1442 - val_loss: 0.1269\nEpoch 14/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1369\n\nEpoch 00014: val_loss did not improve from 0.12641\n - 19s - loss: 0.1391 - val_loss: 0.1369\nEpoch 15/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1328\n\nEpoch 00015: val_loss did not improve from 0.12641\n - 18s - loss: 0.1361 - val_loss: 0.1328\nEpoch 00015: early stopping\n\n\n\nloss_lstm32 = history_lstm32.history['loss']\nval_loss_lstm32 = history_lstm32.history['val_loss']\n\nepochs = range(1, len(loss_lstm32) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm32, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm32, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm32) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.77%\n\n\n\nlstm32 = ((min(val_loss_lstm32) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.3 Stacked LSTM with 64 and 32 nodes\n\n\nfilepath_lstm6432=\"clark_best_lstm6432.hdf5\" \ncp_lstm6432 = ModelCheckpoint(filepath_lstm6432, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm6432 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm6432 = [cp_lstm6432, es_lstm6432]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm6432 = Sequential()\nmodel_lstm6432.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm6432.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm6432.add(layers.Dense(32, activation='relu'))\nmodel_lstm6432.add(layers.Dense(1))\nmodel_lstm6432.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm6432 = model_lstm6432.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm6432, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 84ms/step - loss: 0.1595\n\nEpoch 00001: val_loss improved from inf to 0.15946, saving model to clark_best_lstm6432.hdf5\n - 46s - loss: 0.4159 - val_loss: 0.1595\nEpoch 2/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1199\n\nEpoch 00002: val_loss improved from 0.15946 to 0.11991, saving model to clark_best_lstm6432.hdf5\n - 42s - loss: 0.2417 - val_loss: 0.1199\nEpoch 3/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1290\n\nEpoch 00003: val_loss did not improve from 0.11991\n - 40s - loss: 0.1874 - val_loss: 0.1290\nEpoch 4/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1210\n\nEpoch 00004: val_loss did not improve from 0.11991\n - 40s - loss: 0.1660 - val_loss: 0.1210\nEpoch 5/50\n20/20 [==============================] - 2s 77ms/step - loss: 0.1203\n\nEpoch 00005: val_loss did not improve from 0.11991\n - 41s - loss: 0.1551 - val_loss: 0.1203\nEpoch 6/50\n20/20 [==============================] - 2s 83ms/step - loss: 0.1340\n\nEpoch 00006: val_loss did not improve from 0.11991\n - 41s - loss: 0.1528 - val_loss: 0.1340\nEpoch 7/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1215\n\nEpoch 00007: val_loss did not improve from 0.11991\n - 39s - loss: 0.1419 - val_loss: 0.1215\nEpoch 8/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1299\n\nEpoch 00008: val_loss did not improve from 0.11991\n - 38s - loss: 0.1433 - val_loss: 0.1299\nEpoch 9/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1234\n\nEpoch 00009: val_loss did not improve from 0.11991\n - 38s - loss: 0.1355 - val_loss: 0.1234\nEpoch 10/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1421\n\nEpoch 00010: val_loss did not improve from 0.11991\n - 38s - loss: 0.1325 - val_loss: 0.1421\nEpoch 11/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1251\n\nEpoch 00011: val_loss did not improve from 0.11991\n - 39s - loss: 0.1322 - val_loss: 0.1251\nEpoch 12/50\n20/20 [==============================] - 1s 72ms/step - loss: 0.1347\n\nEpoch 00012: val_loss did not improve from 0.11991\n - 40s - loss: 0.1292 - val_loss: 0.1347\nEpoch 00012: early stopping\n\n\n\nloss_lstm6432 = history_lstm6432.history['loss']\nval_loss_lstm6432 = history_lstm6432.history['val_loss']\n\nepochs = range(1, len(loss_lstm6432) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm6432, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm6432, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm6432) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.63%\n\n\n\nlstm6432 = ((min(val_loss_lstm6432) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\nfilepath_lstm3264 =\"clark_best_lstm3264.hdf5\" \ncp_lstm3264 = ModelCheckpoint(filepath_lstm3264, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm3264 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm3264 = [cp_lstm3264, es_lstm3264]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm3264 = Sequential()\nmodel_lstm3264.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm3264.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm3264.add(layers.Dense(64, activation='relu'))\nmodel_lstm3264.add(layers.Dense(1))\nmodel_lstm3264.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm3264 = model_lstm3264.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm3264, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 86ms/step - loss: 0.1717\n\nEpoch 00001: val_loss improved from inf to 0.17167, saving model to clark_best_lstm3264.hdf5\n - 50s - loss: 0.3827 - val_loss: 0.1717\nEpoch 2/50\n20/20 [==============================] - 1s 71ms/step - loss: 0.1447\n\nEpoch 00002: val_loss improved from 0.17167 to 0.14468, saving model to clark_best_lstm3264.hdf5\n - 38s - loss: 0.2317 - val_loss: 0.1447\nEpoch 3/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1177\n\nEpoch 00003: val_loss improved from 0.14468 to 0.11771, saving model to clark_best_lstm3264.hdf5\n - 39s - loss: 0.1797 - val_loss: 0.1177\nEpoch 4/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1282\n\nEpoch 00004: val_loss did not improve from 0.11771\n - 37s - loss: 0.1620 - val_loss: 0.1282\nEpoch 5/50\n20/20 [==============================] - 2s 90ms/step - loss: 0.1210\n\nEpoch 00005: val_loss did not improve from 0.11771\n - 42s - loss: 0.1546 - val_loss: 0.1210\nEpoch 6/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1249\n\nEpoch 00006: val_loss did not improve from 0.11771\n - 49s - loss: 0.1525 - val_loss: 0.1249\nEpoch 7/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1245\n\nEpoch 00007: val_loss did not improve from 0.11771\n - 36s - loss: 0.1434 - val_loss: 0.1245\nEpoch 8/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1350\n\nEpoch 00008: val_loss did not improve from 0.11771\n - 37s - loss: 0.1449 - val_loss: 0.1350\nEpoch 9/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1203\n\nEpoch 00009: val_loss did not improve from 0.11771\n - 37s - loss: 0.1356 - val_loss: 0.1203\nEpoch 10/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1250\n\nEpoch 00010: val_loss did not improve from 0.11771\n - 38s - loss: 0.1336 - val_loss: 0.1250\nEpoch 11/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1447\n\nEpoch 00011: val_loss did not improve from 0.11771\n - 38s - loss: 0.1332 - val_loss: 0.1447\nEpoch 12/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1234\n\nEpoch 00012: val_loss did not improve from 0.11771\n - 37s - loss: 0.1302 - val_loss: 0.1234\nEpoch 13/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1325\n\nEpoch 00013: val_loss did not improve from 0.11771\n - 38s - loss: 0.1275 - val_loss: 0.1325\nEpoch 00013: early stopping\n\n\n\nloss_lstm3264 = history_lstm3264.history['loss']\nval_loss_lstm3264 = history_lstm3264.history['val_loss']\n\nepochs = range(1, len(loss_lstm3264) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm3264, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm3264, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm3264) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.58%\n\n\n\nlstm3264 = ((min(val_loss_lstm3264) * train_mw_std)/plant_capacity)  * 100\n\n\n\n3. Results and Discussions\n\n\n\n3.1 Results\n\n\nnames = ['Industry Agnostic Naive Method',\n         'Industry Standard Naive Method', \n         'LSTM with 64 nodes',\n         'LSTM with 32 nodes',\n         'LSTM+LSTM with 64 nodes and 32 nodes',\n         'LSTM+LSTM with 32 nodes and 64 nodes']\nresults = [benchmark1, \n           benchmark2,\n           lstm64,\n           lstm32,\n           lstm6432,\n           lstm3264]\n\n\nresults = pd.DataFrame(zip(names,results), columns=['Model', 'Error in Percentage'])\n\n\nresults['Error in Percentage'] = round(results['Error in Percentage'],2)\n\n\nresults['Improvement'] = round(1-(results['Error in Percentage']/benchmark2),4)*100\n\n\nresults\n\n\n\n\n\n\n\n\nModel\nError in Percentage\nImprovement\n\n\n\n\n0\nIndustry Agnostic Naive Method\n5.84\n53.21\n\n\n1\nIndustry Standard Naive Method\n12.48\n0.00\n\n\n2\nLSTM with 64 nodes\n2.82\n77.40\n\n\n3\nLSTM with 32 nodes\n2.77\n77.80\n\n\n4\nLSTM+LSTM with 64 nodes and 32 nodes\n2.63\n78.93\n\n\n5\nLSTM+LSTM with 32 nodes and 64 nodes\n2.58\n79.33"
  },
  {
    "objectID": "posts/2023-04-25-Solar/source-files/notebooks/[DL] HERE Comes the Sun-SUBIC (Indiv - Yu,J).html",
    "href": "posts/2023-04-25-Solar/source-files/notebooks/[DL] HERE Comes the Sun-SUBIC (Indiv - Yu,J).html",
    "title": "Sandro Luis R. Silva",
    "section": "",
    "text": "Here Comes The Sun: Clark Solar Power Plant\n\n\nLT16 - Sandro Silva and Jac Lin Yu\n\n\nIndividual Project of Jac Lin Yu\n\n\n\nTable of Contents\n\n1. Problem Statement\n2. Methodology\n\n2.1. Loading Prerequisites\n2.2. Dataset and Preprocessing\n2.3. Train, Validation, Test Split and Batch Generation\n2.4. Determining the Benchmark\n2.5. Model Training and Selection\n\n2.5.1 LSTM with 64 nodes\n2.5.2 LSTM with 32 nodes\n2.5.3 Stacked LSTM with 64 and 32 nodes\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\n3. Results and Discussions * 3.1. Results * 3.2. Discussions and Recommendations\n4. Summary and Main Learning Points * 4.1. Summary * 4.1. Main Learning Points \n\n\n1. Problem Statement\n\n\nWith the threat of climate change, governments and industries needed to make adjustments as a response to the changes in the priorities of the people. Being one of the biggest contributors to greenhouse gases, the energy industry has shifted its focus away from traditional fuel sources, such as oil and coal, to more alternative and sustainable solutions, such as hydroelectric, solar, and wind. This movement, however, poses new threats and challenges to existing power grids across the world due to the inherent variability and intermittency of renewable fuel source. The disruptions in the power grids and the possibility of an imbalanced load threaten to overwhelm the grid operators.  Renewable energy forecasting, in particular for solar generation supply as this is the most volatile renewable power source, may provide grid operators the ability to predict and balance energy generation and consumption. In addition, power grid operators will be able to balance and schedule the distribution of generated power for not only renewable power plants but also conventional (and rigid) power plants, such as coal and natural gas. This is currently important to the Philippines, as the Department of Energy has given priority to solar power plants. The government has promised to buy all power these plants produce at a fixed rate. This encourages production which increases supply, but demand remains constant. Therefore, conventional plants need to adjust their production or incur loses and cause instability to the grid.\n\n\n\n2. Methodology\n\n\n\n2.1 Loading Prerequisites\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n\n2.2 Dataset and Preprocessing\n\n\n Here, we will load the already cleaned preprocessed dataset of solar generation per plant per hour. The cleaning was processed outside of this notebook, and followed a framework which follows the industry standard of imputing missing values.\nTo list a few processes: - using the average between the before(t-1) and after hour(t+1) - using a 1-week average of all similar intervals (ex, if empty during hour 6 of dec 20, take the moving average of all hour 6 from dec 13-dec19) - using a 4-week moving average of that specific hour and day of the week (ex, if empty during hour 6 of dec 20, we take average of hour 6(dec 13, 6, 29, 22) \n\n\ndf = pd.read_excel('solar_dataset.xlsx', \n                   sheet_name='subic', \n                   parse_dates=['DATETIME'])\n\n\n Listed below are the columns and the corresponding datatype. \n\n\ndf.dtypes\n\nDATETIME         datetime64[ns]\nYEAR                      int64\nMONTH                     int64\nDAY                       int64\nHOUR                      int64\nRESOURCE_ID              object\nMW                      float64\nLocation                 object\nTemperature             float64\nDew Point               float64\nHumidity                float64\nWind Speed                int64\nWind Gust                 int64\nPressure                float64\nCondition                object\nLocation_Name            object\ndtype: object\n\n\n\n Drop some columns such as location, resource ID, and year. The information regarding the specific plant is unimportant since the scope has already been specified, therefore, there is only one value for each of the specified column. The year is not important since weather has no seasonality in terms of year. \n\n\ndf.drop(['Location_Name', 'Location', 'YEAR', 'RESOURCE_ID'],axis=1,inplace=True)\n\n\n It is customary in traditional time-series regression to convert the month, day, hour into categorical data, which will be later used in one-hot encoding. In addition, this is the usual preprocessing used in the energy industry here in the Philippines. \n\n\ndf.MONTH = df.MONTH.astype('str')\ndf.DAY = df.DAY.astype('str')\ndf.HOUR = df.HOUR.astype('str')\n\n\n Let us append the main df with the one-hot encoded dataframe. These columns need to be further processed because keeping them as they are will give the model the wrong idea that months, day, hour, and conditions that have higher values are more significant. \n\n\ndf0 = df.join(pd.get_dummies(df[['MONTH','DAY','HOUR','Condition']]))\n\n\n Further dropping of columns as well as reordering of columns \n\n\ndf0 = df0.drop(['DATETIME','MONTH','DAY','HOUR','Condition'],\n               axis=1)\n\n\ndf0 = df0[['Temperature','MW'] + df0.columns[2:].tolist()].copy()\n\n\ndf0 = df0.astype('float')\n\n\n Final DataFrame to be used \n\n\ndf0.head()\n\n\n\n\n\n\n\n\nTemperature\nMW\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nMONTH_1\nMONTH_10\nMONTH_11\n...\nCondition_Partly Cloudy\nCondition_Partly Cloudy / Windy\nCondition_Rain\nCondition_Rain / Windy\nCondition_Rain Shower\nCondition_Showers in the Vicinity\nCondition_T-Storm\nCondition_T-Storm / Windy\nCondition_Thunder\nCondition_Thunder in the Vicinity\n\n\n\n\n0\n72.0\n0.0\n68.0\n88.0\n8.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n73.0\n0.0\n68.0\n83.0\n10.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n75.0\n0.0\n70.0\n83.0\n9.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n75.0\n0.0\n70.0\n83.0\n0.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n73.0\n0.0\n70.0\n88.0\n12.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 103 columns\n\n\n\n\ndf0.shape\n\n(24018, 103)\n\n\n\nplant_capacity = 100\n\n\n\n 2.3 Train,Validate,Test Split and Batch Generation\n\n\n For this analysis, a .8, .1, .1, train-validate-test split was implemented. Since we are dealing with a time-series, the train-validate-test split cannot be randomly selected from the dataset, splitting of dataset should be conducted in a chronogical order. Below we define the starting and ending index for each group \n\n\ntrain_validate_test_split = [.8, .1, .1]\n\ntrain_idx = math.floor(df0.shape[0] * train_validate_test_split[0])\nval_idx = math.floor(df0.shape[0] * train_validate_test_split[1]) + train_idx\n\n\n The generator function below was utilized to yield the appropriate batch. Significant parameters include the lookback, or how far back of the data to consider, delay, or how far into the future are we forecasting, batch_size, refers to the number of training examples utilized in one iteration. \n\n\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, \n              batch_size=128, step=1):\n    \n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, \n                                     max_index, size=batch_size)\n        else:\n            if i + batch_size &gt;= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets\n\n\n Let us mean-center the dataset \n\n\nfloat_data = df0.values\ntrain_mean = float_data[:train_idx].mean(axis=0)\nfloat_data -= train_mean\ntrain_std = float_data[:train_idx].std(axis=0)\nfloat_data /= train_std\ntrain_mw_std = train_std[1]\n\n\n An industry standard is to utilize a lookback of either 144 hours (1 week), 288 hours (2 weeks), or 720 hours (1[30-day] month). The initial run was to utilized a 288 hour lookback period due to group’s industry expert’s experience. However, outside consultation with industry experts said 1 week would suffice with this limited dataset. \n\n\nlookback = 144 # lookback(consider) the previous week data\nstep = 1 # in hourly granularity\ndelay = 24 # to forecast the next 24 hours\nbatch_size = 128\n\n\ntrain_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=train_idx,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size)\n\nval_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=train_idx+1,\n    max_index=val_idx,\n    step=step,\n    batch_size=batch_size)\n\ntest_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=val_idx,\n    max_index=None,\n    step=step,\n    batch_size=batch_size)\n\nval_steps = (val_idx - (train_idx+1) - lookback)\ntest_steps = (len(float_data) - (val_idx+1) - lookback)\n\n\n\n2.4 Benchmark\n\n\n Two metrics were used to establish a baseline error:\n(1) the naive method identified in a previous lecture, which we scaled to industry standard\n(2) a different framework suggested by industry\n\n\n\ndef evaluate_naive_method():\n\n    batch_maes = []\n    for step in range(val_steps):\n        if step % 1000 == 0:\n            print(step)\n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    \n    return np.mean(batch_maes)\n\nnaive_method = evaluate_naive_method()\n\n0\n1000\n2000\n\n\n\nprint('NAIVE METHOD 1:', f'{((naive_method * train_mw_std)/22):0.2%}') #where 22 is the capacity of the plant\n\nNAIVE METHOD 1: 8.87%\n\n\n\nbenchmark1 = ((naive_method * train_mw_std)/22) * 100\n\n\n For establishing the 2nd baseline error, the team consulted the industry to determine a baseline error. However, even the industry is still conflicted on how to establish a baseline measurement. As of now, the baseline is still being established by policy makers and grid operators. \nLuckily, consulting with industry experts gave the team a framework on how to establish a baseline error per plant. The resulting measure looked at the mean squared error of the day-ahead projection(DAP), also called scheduled (Ex-Ante or RTD), and the actual delivered (Ex-Post or RTX).\nThis was then scaled as a percentage to the capacity of the plant. This was with processed with an industry expert and not included in the notebook. The yielding error resulted with a baseline of 3.64% \n\n\nbenchmark2 = 3.64\n\n\n\n2.5 Model Training Selection\n\n\n Four models were trained:\n- LSTM with 64 nodes\n- LSTM with 32 nodes\n- Stacked LSTM(2 layer) with 64 and 32 nodes\n- Stacked LSTM(2 layer) with 32 and 64 nodes\nDropout of 0.2 and Recurrent Dropout of 0.2 and a RMSProp optimized was implemented for all three.\nThe group decided to use a Epoch=50, with an EarlyStopping of patience=10 and min_delta=0.01 to avoid overfitting and to reduce runtime due to the limited time constraints. \n\n\nsteps_per_epoch_cnt = math.floor(train_idx/batch_size)\nval_steps_cnt = round(math.floor((val_idx-train_idx)/batch_size),-1)\n\n\n\n2.5.1 LSTM with 64\n\n\nfilepath_lstm64=\"clark_best_lstm64.hdf5\" \ncp_lstm64 = ModelCheckpoint(filepath_lstm64, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm64 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm64 = [cp_lstm64, es_lstm64]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm64 = Sequential()\nmodel_lstm64.add(layers.LSTM(64, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm64.add(layers.Dense(64,activation='relu'))\nmodel_lstm64.add(layers.Dense(1))\nmodel_lstm64.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm64 = model_lstm64.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm64, verbose=2) \n\nWARNING:tensorflow:From /home/jaclin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/jaclin/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/jaclin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.2421\n\nEpoch 00001: val_loss improved from inf to 0.24213, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.4244 - val_loss: 0.2421\nEpoch 2/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.2260\n\nEpoch 00002: val_loss improved from 0.24213 to 0.22596, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.2940 - val_loss: 0.2260\nEpoch 3/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2035\n\nEpoch 00003: val_loss improved from 0.22596 to 0.20355, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.2379 - val_loss: 0.2035\nEpoch 4/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.2291\n\nEpoch 00004: val_loss did not improve from 0.20355\n - 21s - loss: 0.2035 - val_loss: 0.2291\nEpoch 5/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.2306\n\nEpoch 00005: val_loss did not improve from 0.20355\n - 20s - loss: 0.1880 - val_loss: 0.2306\nEpoch 6/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2080\n\nEpoch 00006: val_loss did not improve from 0.20355\n - 21s - loss: 0.1816 - val_loss: 0.2080\nEpoch 7/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.2186\n\nEpoch 00007: val_loss did not improve from 0.20355\n - 21s - loss: 0.1706 - val_loss: 0.2186\nEpoch 8/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2124\n\nEpoch 00008: val_loss did not improve from 0.20355\n - 21s - loss: 0.1624 - val_loss: 0.2124\nEpoch 9/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.2054\n\nEpoch 00009: val_loss did not improve from 0.20355\n - 21s - loss: 0.1548 - val_loss: 0.2054\nEpoch 10/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.2036\n\nEpoch 00010: val_loss did not improve from 0.20355\n - 20s - loss: 0.1517 - val_loss: 0.2036\nEpoch 11/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2477\n\nEpoch 00011: val_loss did not improve from 0.20355\n - 20s - loss: 0.1469 - val_loss: 0.2477\nEpoch 12/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.1994\n\nEpoch 00012: val_loss improved from 0.20355 to 0.19939, saving model to clark_best_lstm64.hdf5\n - 22s - loss: 0.1445 - val_loss: 0.1994\nEpoch 13/50\n20/20 [==============================] - 1s 44ms/step - loss: 0.2224\n\nEpoch 00013: val_loss did not improve from 0.19939\n - 21s - loss: 0.1363 - val_loss: 0.2224\nEpoch 00013: early stopping\n\n\n\nloss_lstm64 = history_lstm64.history['loss']\nval_loss_lstm64 = history_lstm64.history['val_loss']\n\nepochs = range(1, len(loss_lstm64) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm64, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm64, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm64) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 1.44%\n\n\n\nlstm64 = ((min(val_loss_lstm64) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.2 LSTM with 32 nodes\n\n\nfilepath_lstm32=\"clark_best_lstm32.hdf5\" \ncp_lstm32 = ModelCheckpoint(filepath_lstm32, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm32 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm32 = [cp_lstm32, es_lstm32]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm32 = Sequential()\nmodel_lstm32.add(layers.LSTM(32, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm32.add(layers.Dense(32, activation='relu'))\nmodel_lstm32.add(layers.Dense(1))\nmodel_lstm32.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm32 = model_lstm32.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm32, verbose=2) \n\nEpoch 1/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.2359\n\nEpoch 00001: val_loss improved from inf to 0.23593, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.4811 - val_loss: 0.2359\nEpoch 2/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2314\n\nEpoch 00002: val_loss improved from 0.23593 to 0.23142, saving model to clark_best_lstm32.hdf5\n - 15s - loss: 0.3305 - val_loss: 0.2314\nEpoch 3/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.2202\n\nEpoch 00003: val_loss improved from 0.23142 to 0.22023, saving model to clark_best_lstm32.hdf5\n - 15s - loss: 0.2887 - val_loss: 0.2202\nEpoch 4/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2305\n\nEpoch 00004: val_loss did not improve from 0.22023\n - 15s - loss: 0.2572 - val_loss: 0.2305\nEpoch 5/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2144\n\nEpoch 00005: val_loss improved from 0.22023 to 0.21437, saving model to clark_best_lstm32.hdf5\n - 15s - loss: 0.2366 - val_loss: 0.2144\nEpoch 6/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2146\n\nEpoch 00006: val_loss did not improve from 0.21437\n - 15s - loss: 0.2151 - val_loss: 0.2146\nEpoch 7/50\n20/20 [==============================] - 1s 29ms/step - loss: 0.1948\n\nEpoch 00007: val_loss improved from 0.21437 to 0.19485, saving model to clark_best_lstm32.hdf5\n - 15s - loss: 0.2005 - val_loss: 0.1948\nEpoch 8/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2154\n\nEpoch 00008: val_loss did not improve from 0.19485\n - 16s - loss: 0.1869 - val_loss: 0.2154\nEpoch 9/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2095\n\nEpoch 00009: val_loss did not improve from 0.19485\n - 15s - loss: 0.1743 - val_loss: 0.2095\nEpoch 10/50\n20/20 [==============================] - 1s 36ms/step - loss: 0.2087\n\nEpoch 00010: val_loss did not improve from 0.19485\n - 17s - loss: 0.1674 - val_loss: 0.2087\nEpoch 11/50\n20/20 [==============================] - 1s 54ms/step - loss: 0.2268\n\nEpoch 00011: val_loss did not improve from 0.19485\n - 22s - loss: 0.1650 - val_loss: 0.2268\nEpoch 12/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1882\n\nEpoch 00012: val_loss improved from 0.19485 to 0.18820, saving model to clark_best_lstm32.hdf5\n - 22s - loss: 0.1633 - val_loss: 0.1882\nEpoch 13/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.2057\n\nEpoch 00013: val_loss did not improve from 0.18820\n - 19s - loss: 0.1543 - val_loss: 0.2057\nEpoch 14/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.2006\n\nEpoch 00014: val_loss did not improve from 0.18820\n - 19s - loss: 0.1561 - val_loss: 0.2006\nEpoch 15/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.2172\n\nEpoch 00015: val_loss did not improve from 0.18820\n - 19s - loss: 0.1506 - val_loss: 0.2172\nEpoch 16/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1941\n\nEpoch 00016: val_loss did not improve from 0.18820\n - 19s - loss: 0.1504 - val_loss: 0.1941\nEpoch 17/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.2072\n\nEpoch 00017: val_loss did not improve from 0.18820\n - 18s - loss: 0.1448 - val_loss: 0.2072\nEpoch 00017: early stopping\n\n\n\nloss_lstm32 = history_lstm32.history['loss']\nval_loss_lstm32 = history_lstm32.history['val_loss']\n\nepochs = range(1, len(loss_lstm32) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm32, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm32, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm32) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 1.36%\n\n\n\nlstm32 = ((min(val_loss_lstm32) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.3 Stacked LSTM with 64 and 32 nodes\n\n\nfilepath_lstm6432=\"clark_best_lstm6432.hdf5\" \ncp_lstm6432 = ModelCheckpoint(filepath_lstm6432, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm6432 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm6432 = [cp_lstm6432, es_lstm6432]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm6432 = Sequential()\nmodel_lstm6432.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm6432.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm6432.add(layers.Dense(32, activation='relu'))\nmodel_lstm6432.add(layers.Dense(1))\nmodel_lstm6432.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm6432 = model_lstm6432.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm6432, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 83ms/step - loss: 0.2134\n\nEpoch 00001: val_loss improved from inf to 0.21343, saving model to clark_best_lstm6432.hdf5\n - 52s - loss: 0.4166 - val_loss: 0.2134\nEpoch 2/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2394\n\nEpoch 00002: val_loss did not improve from 0.21343\n - 38s - loss: 0.2592 - val_loss: 0.2394\nEpoch 3/50\n20/20 [==============================] - 1s 73ms/step - loss: 0.2154\n\nEpoch 00003: val_loss did not improve from 0.21343\n - 38s - loss: 0.2145 - val_loss: 0.2154\nEpoch 4/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.2356\n\nEpoch 00004: val_loss did not improve from 0.21343\n - 37s - loss: 0.1888 - val_loss: 0.2356\nEpoch 5/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2106\n\nEpoch 00005: val_loss improved from 0.21343 to 0.21064, saving model to clark_best_lstm6432.hdf5\n - 37s - loss: 0.1734 - val_loss: 0.2106\nEpoch 6/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2121\n\nEpoch 00006: val_loss did not improve from 0.21064\n - 37s - loss: 0.1678 - val_loss: 0.2121\nEpoch 7/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1950\n\nEpoch 00007: val_loss improved from 0.21064 to 0.19502, saving model to clark_best_lstm6432.hdf5\n - 37s - loss: 0.1602 - val_loss: 0.1950\nEpoch 8/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2112\n\nEpoch 00008: val_loss did not improve from 0.19502\n - 38s - loss: 0.1505 - val_loss: 0.2112\nEpoch 9/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2117\n\nEpoch 00009: val_loss did not improve from 0.19502\n - 38s - loss: 0.1454 - val_loss: 0.2117\nEpoch 10/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2128\n\nEpoch 00010: val_loss did not improve from 0.19502\n - 37s - loss: 0.1427 - val_loss: 0.2128\nEpoch 11/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2147\n\nEpoch 00011: val_loss did not improve from 0.19502\n - 39s - loss: 0.1397 - val_loss: 0.2147\nEpoch 12/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1968\n\nEpoch 00012: val_loss did not improve from 0.19502\n - 38s - loss: 0.1353 - val_loss: 0.1968\nEpoch 13/50\n20/20 [==============================] - 1s 71ms/step - loss: 0.2072\n\nEpoch 00013: val_loss did not improve from 0.19502\n - 38s - loss: 0.1296 - val_loss: 0.2072\nEpoch 14/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1979\n\nEpoch 00014: val_loss did not improve from 0.19502\n - 39s - loss: 0.1283 - val_loss: 0.1979\nEpoch 15/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2203\n\nEpoch 00015: val_loss did not improve from 0.19502\n - 39s - loss: 0.1231 - val_loss: 0.2203\nEpoch 16/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1974\n\nEpoch 00016: val_loss did not improve from 0.19502\n - 39s - loss: 0.1256 - val_loss: 0.1974\nEpoch 17/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2095\n\nEpoch 00017: val_loss did not improve from 0.19502\n - 39s - loss: 0.1221 - val_loss: 0.2095\nEpoch 00017: early stopping\n\n\n\nloss_lstm6432 = history_lstm6432.history['loss']\nval_loss_lstm6432 = history_lstm6432.history['val_loss']\n\nepochs = range(1, len(loss_lstm6432) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm6432, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm6432, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm6432) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 1.41%\n\n\n\nlstm6432 = ((min(val_loss_lstm6432) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\nfilepath_lstm3264 =\"clark_best_lstm3264.hdf5\" \ncp_lstm3264 = ModelCheckpoint(filepath_lstm3264, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm3264 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm3264 = [cp_lstm3264, es_lstm3264]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm3264 = Sequential()\nmodel_lstm3264.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm3264.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm3264.add(layers.Dense(64, activation='relu'))\nmodel_lstm3264.add(layers.Dense(1))\nmodel_lstm3264.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm3264 = model_lstm3264.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm3264, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 78ms/step - loss: 0.2272\n\nEpoch 00001: val_loss improved from inf to 0.22722, saving model to clark_best_lstm3264.hdf5\n - 41s - loss: 0.4076 - val_loss: 0.2272\nEpoch 2/50\n20/20 [==============================] - 2s 75ms/step - loss: 0.2298\n\nEpoch 00002: val_loss did not improve from 0.22722\n - 39s - loss: 0.2578 - val_loss: 0.2298\nEpoch 3/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2099\n\nEpoch 00003: val_loss improved from 0.22722 to 0.20986, saving model to clark_best_lstm3264.hdf5\n - 41s - loss: 0.2111 - val_loss: 0.2099\nEpoch 4/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2387\n\nEpoch 00004: val_loss did not improve from 0.20986\n - 37s - loss: 0.1871 - val_loss: 0.2387\nEpoch 5/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2111\n\nEpoch 00005: val_loss did not improve from 0.20986\n - 37s - loss: 0.1708 - val_loss: 0.2111\nEpoch 6/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2193\n\nEpoch 00006: val_loss did not improve from 0.20986\n - 37s - loss: 0.1655 - val_loss: 0.2193\nEpoch 7/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1926\n\nEpoch 00007: val_loss improved from 0.20986 to 0.19259, saving model to clark_best_lstm3264.hdf5\n - 38s - loss: 0.1579 - val_loss: 0.1926\nEpoch 8/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2260\n\nEpoch 00008: val_loss did not improve from 0.19259\n - 38s - loss: 0.1513 - val_loss: 0.2260\nEpoch 9/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2203\n\nEpoch 00009: val_loss did not improve from 0.19259\n - 39s - loss: 0.1417 - val_loss: 0.2203\nEpoch 10/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2130\n\nEpoch 00010: val_loss did not improve from 0.19259\n - 37s - loss: 0.1421 - val_loss: 0.2130\nEpoch 11/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2420\n\nEpoch 00011: val_loss did not improve from 0.19259\n - 37s - loss: 0.1376 - val_loss: 0.2420\nEpoch 12/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1928\n\nEpoch 00012: val_loss did not improve from 0.19259\n - 38s - loss: 0.1358 - val_loss: 0.1928\nEpoch 13/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2139\n\nEpoch 00013: val_loss did not improve from 0.19259\n - 38s - loss: 0.1290 - val_loss: 0.2139\nEpoch 14/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1976\n\nEpoch 00014: val_loss did not improve from 0.19259\n - 38s - loss: 0.1270 - val_loss: 0.1976\nEpoch 15/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2421\n\nEpoch 00015: val_loss did not improve from 0.19259\n - 38s - loss: 0.1227 - val_loss: 0.2421\nEpoch 16/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2069\n\nEpoch 00016: val_loss did not improve from 0.19259\n - 38s - loss: 0.1243 - val_loss: 0.2069\nEpoch 17/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2055\n\nEpoch 00017: val_loss did not improve from 0.19259\n - 37s - loss: 0.1201 - val_loss: 0.2055\nEpoch 00017: early stopping\n\n\n\nloss_lstm3264 = history_lstm3264.history['loss']\nval_loss_lstm3264 = history_lstm3264.history['val_loss']\n\nepochs = range(1, len(loss_lstm3264) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm3264, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm3264, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm3264) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 1.40%\n\n\n\nlstm3264 = ((min(val_loss_lstm3264) * train_mw_std)/plant_capacity)  * 100\n\n\n\n3. Results and Discussions\n\n\n\n3.1 Results\n\n\nnames = ['Industry Agnostic Naive Method',\n         'Industry Standard Naive Method', \n         'LSTM with 64 nodes',\n         'LSTM with 32 nodes',\n         'LSTM+LSTM with 64 nodes and 32 nodes',\n         'LSTM+LSTM with 32 nodes and 64 nodes']\nresults = [benchmark1, \n           benchmark2,\n           lstm64,\n           lstm32,\n           lstm6432,\n           lstm3264]\n\n\nresults = pd.DataFrame(zip(names,results), columns=['Model', 'Error in Percentage'])\n\n\nresults['Error in Percentage'] = round(results['Error in Percentage'],2)\n\n\nresults['Improvement'] = round(1-(results['Error in Percentage']/3.64),4)*100\n\n\nresults\n\n\n\n\n\n\n\n\nModel\nError in Percentage\nImprovement\n\n\n\n\n0\nIndustry Agnostic Naive Method\n8.87\n143.68\n\n\n1\nIndustry Standard Naive Method\n3.64\n0.00\n\n\n2\nLSTM with 64 nodes\n1.44\n60.44\n\n\n3\nLSTM with 32 nodes\n1.36\n62.64\n\n\n4\nLSTM+LSTM with 64 nodes and 32 nodes\n1.41\n61.26\n\n\n5\nLSTM+LSTM with 32 nodes and 64 nodes\n1.40\n61.54\n\n\n\n\n\n\n\n\n\n3.2 Discussion and Recommendation\n\nDeep learning models, particularly Long-Short Term Memory, works very well with time series data that all neural networks performed more than 60% better than current industry standards. This improvements will save companies and the national grid operators money and make our power system more efficient. Companies will pay less paid due to miscalculations of projected generation and waste from generating too much power that are not bought by the market. Grid operators will save on repairs and downtimes caused by fluctuations in the power supply.\nFurther improvements in the model can be achieved further manipulation of the architecture of the network such as trying different numbers of nodes, layers, and optimizers in the LSTM layer, adding dense layers with different nodes, or even adding a CNN layer after the LSTM layer, as some have done. Another avenue for improvement is to obtain more data that are more indicative of solar power generation such as quality of solar panels, the angle they are placed, the irridiance per hour, altitude of the area, among others.\n\n\n4. Main Learning Points\n\nIn this age of the 4th industrial revolution, technologies are making everything, even ones that require specialized technical knowledge, such as programming, available to all. Various programs enable anyone, who has a computer, internet connection, and interest, to create their own machine learning models. All these parameter tuning, model selection, and model evaluation are getting simpler and simpler to execute, but harder to explain. This is where a formal education in data science with good mathematical foundation brings an edge. Real self-respecting data scientists will be able to explain not just how to execute/program the pipline, but also how the actual algorithm works and how it was able to provide the results it did. There is a developed intuition from having understand the fundamentals. This, together with industry expertise, will determine the success of a data scientist in the field.\nIndustry expertise is still important, as there are various intricacies in an industry that are not known to outsiders. These insights might be the determining factor in the increased performance of a model, or even its acceptance."
  },
  {
    "objectID": "posts/2023-04-25-Solar/source-files/notebooks/[DL] Here Comes the Sun - CLARK (Indiv- Silva, S).html",
    "href": "posts/2023-04-25-Solar/source-files/notebooks/[DL] Here Comes the Sun - CLARK (Indiv- Silva, S).html",
    "title": "Sandro Luis R. Silva",
    "section": "",
    "text": "Here Comes The Sun: Clark Solar Power Plant\n\n\nLT16 - Sandro Silva and Jac Lin Yu\n\n\nIndividual Project of Sandro Silva\n\n\n\nTable of Contents\n\n1. Problem Statement\n2. Methodology\n\n2.1. Loading Prerequisites\n2.2. Dataset and Preprocessing\n2.3. Train, Validation, Test Split and Batch Generation\n2.4. Determining the Benchmark\n2.5. Model Training and Selection\n\n2.5.1 LSTM with 64 nodes\n2.5.2 LSTM with 32 nodes\n2.5.3 Stacked LSTM with 64 and 32 nodes\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\n3. Results and Discussions * 3.1. Results * 3.2. Discussions and Recommendations\n4. Summary and Main Learning Points * 4.1. Summary * 4.1. Main Learning Points \n\n\n1. Problem Statement\n\n\nIn recent years, the global energy industry has changed its focus away from traditional fuel sources, such as oil and coal, to more alternative and sustainable solutions, such as hydroelectric, solar and wind. However, the shift towards renewable energy poses new threats and challenges to existing power grids across the world. One major concern surrounding renewable energy generation is the inherent variability and intermittency of its fuel source, as this can cause disruptions in power grids. Essentially, renewable energy technologies threaten to overwhelm the grid operators.  Renewable energy forecasting, in particular solar generation supply, may provide power grid operators the ability to predict and balance energy generation and consumption. In addition, power grid operators will be able to balance and schedule the distribution of generated power for not only renewable power plants but also conventional (and rigid) power plants, such as coal and natural gas.\n\n\n\n2. Methodology\n\n\n\n2.1 Loading Prerequisites\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n\n2.2 Dataset and Preprocessing\n\n\n Here, we will load the already cleaned preprocessed dataset of solar generation per plant per hour. The cleaning was processed outside of this notebook, and followed a framework which follows the industry standard of imputing missing values.\nTo list a few processes: - using the average between the before(t-1) and after hour(t+1) - using a 1-week average of all similar intervals (ex, if empty during hour 6 of dec 20, take the moving average of all hour 6 from dec 13-dec19) - using a 4-week moving average of that specific hour and day of the week (ex, if empty during hour 6 of dec 20, we take average of hour 6(dec 13, 6, 29, 22) \n\n\ndf = pd.read_excel('solar_dataset.xlsx', \n                   sheet_name='clark', \n                   parse_dates=['DATETIME'])\n\n\n Here let us observe the different datatypes, notice that some columns are objects, some of which will be dropped. \n\n\ndf.dtypes\n\nDATETIME         datetime64[ns]\nYEAR                      int64\nMONTH                     int64\nDAY                       int64\nHOUR                      int64\nRESOURCE_ID              object\nMW                      float64\nLocation                 object\nTemperature             float64\nDew Point               float64\nHumidity                float64\nWind Speed              float64\nWind Gust                 int64\nPressure                float64\nCondition                object\nLocation_Name            object\ndtype: object\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nDATETIME\nYEAR\nMONTH\nDAY\nHOUR\nRESOURCE_ID\nMW\nLocation\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nCondition\nLocation_Name\n\n\n\n\n0\n2016-12-26 00:00:00\n2016\n12\n26\n1\n1CLASOL_G01\n0.0\nRPLC\n75.0\n70.0\n83.0\n5.0\n0\n29.3\nLight Rain Shower\nANGELES/PAMPANGA\n\n\n1\n2016-12-26 01:00:00\n2016\n12\n26\n2\n1CLASOL_G01\n0.0\nRPLC\n77.0\n68.0\n74.0\n9.0\n0\n29.2\nCloudy\nANGELES/PAMPANGA\n\n\n2\n2016-12-26 02:00:00\n2016\n12\n26\n3\n1CLASOL_G01\n0.0\nRPLC\n77.0\n70.0\n78.0\n9.0\n0\n29.2\nLight Rain Shower\nANGELES/PAMPANGA\n\n\n\n\n\n\n\n\n It is customary in traditional time-series regression to convert the month, day, hour into categorical data, which will be later used in one-hot encoding. In addition, this is the usual preprocessing used in the energy industry here in the Philippines. \n\n\ndf.MONTH = df.MONTH.astype('str')\ndf.DAY = df.DAY.astype('str')\ndf.HOUR = df.HOUR.astype('str')\n\n\n Let us drop the unimportant columns \n\n\ndf.drop(['Location_Name', 'Location', \"YEAR\"],axis=1,inplace=True)\n\n\n Let us append the main df with the one-hot encoded dataframe \n\n\ndf = df.join(pd.get_dummies(df[['MONTH','DAY','HOUR','Condition']]))\n\n\n Let us get the unique list of power plants in the region \n\n\nlocations = df.RESOURCE_ID.unique()\n\n\n For this study, let us consider the first power plant \n\n\nlocations[0]\n\n'1CLASOL_G01'\n\n\n\ndf0 = df[df.RESOURCE_ID == locations[0]].copy()\n\n\n Further dropping of columns as well as reordering of columns \n\n\ndf0 = df0.drop(['RESOURCE_ID','DATETIME','MONTH','DAY','HOUR','Condition'],\n               axis=1)\n\n\ndf0 = df0[['Temperature','MW'] + df0.columns[2:].tolist()].copy()\n\n\ndf0 = df0.astype('float')\n\n\n Final DataFrame to be used \n\n\ndf0.head()\n\n\n\n\n\n\n\n\nTemperature\nMW\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nMONTH_1\nMONTH_10\nMONTH_11\n...\nCondition_Mostly Cloudy / Windy\nCondition_Partly Cloudy\nCondition_Partly Cloudy / Windy\nCondition_Patches of Fog\nCondition_Rain\nCondition_Rain / Windy\nCondition_Rain Shower\nCondition_Showers in the Vicinity\nCondition_T-Storm\nCondition_Thunder\n\n\n\n\n0\n75.0\n0.0\n70.0\n83.0\n5.0\n0.0\n29.3\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n77.0\n0.0\n68.0\n74.0\n9.0\n0.0\n29.2\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n77.0\n0.0\n70.0\n78.0\n9.0\n0.0\n29.2\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n79.0\n0.0\n70.0\n74.0\n12.0\n0.0\n29.2\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n79.0\n0.0\n70.0\n74.0\n12.0\n0.0\n29.2\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 101 columns\n\n\n\n\ndf0.shape\n\n(24031, 101)\n\n\n\nplant_capacity = 22\n\n\n\n 2.3 Train,Validate,Test Split and Batch Generation\n\n\n For this analysis, a .8, .1, .1, train-validate-test split was implemented. Since we are dealing with a time-series, the train-validate-test split cannot be randomly selected from the dataset, splitting of dataset should be conducted in a chronogical order. Below we define the starting and ending index for each group \n\n\ntrain_validate_test_split = [.8, .1, .1]\n\ntrain_idx = math.floor(df0.shape[0] * train_validate_test_split[0])\nval_idx = math.floor(df0.shape[0] * train_validate_test_split[1]) + train_idx\n\n\n The generator function below was utilized to yield the appropriate batch. Significant parameters include the lookback, or how far back of the data to consider, delay, or how far into the future are we forecasting, batch_size, refers to the number of training examples utilized in one iteration. \n\n\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, \n              batch_size=128, step=1):\n    \n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, \n                                     max_index, size=batch_size)\n        else:\n            if i + batch_size &gt;= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets\n\n\n Let us mean-center the dataset \n\n\nfloat_data = df0.values\ntrain_mean = float_data[:train_idx].mean(axis=0)\nfloat_data -= train_mean\ntrain_std = float_data[:train_idx].std(axis=0)\nfloat_data /= train_std\ntrain_mw_std = train_std[1]\n\n\n Let us initialize the parameters which will be used for the generation An industry standard practic is to utilize a lookback of either 144 hours (1 week), 288 hours (2 weeks), or 720 hours (1[30-day] month). The initial run was to utilized a 288 hour lookback period due to group’s industry expert’s experience. However, outside consultation with industry experts said 1 week would suffice with this limited dataset. \n\n\nlookback = 144 # lookback(consider) the previous week data\nstep = 1 # in hourly granularity\ndelay = 24 # to forecast the next 24 hours\nbatch_size = 128\n\n\ntrain_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=train_idx,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size)\n\nval_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=train_idx+1,\n    max_index=val_idx,\n    step=step,\n    batch_size=batch_size)\n\ntest_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=val_idx,\n    max_index=None,\n    step=step,\n    batch_size=batch_size)\n\nval_steps = (val_idx - (train_idx+1) - lookback)\ntest_steps = (len(float_data) - (val_idx+1) - lookback)\n\n\n\n2.4 Benchmark\n\n\n The team used two metrics to establish a baseline error:\n(1) the naive method identified in a previous lecture, which we scaled to industry standard\n(2) a different framework suggested by industry\n\n\n\ndef evaluate_naive_method():\n\n    batch_maes = []\n    for step in range(val_steps):\n        if step % 1000 == 0:\n            print(step)\n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    \n    return np.mean(batch_maes)\n\nnaive_method = evaluate_naive_method()\n\n0\n1000\n2000\n\n\n\nprint('NAIVE METHOD 1:', f'{((naive_method * train_mw_std)/22):0.2%}') #where 22 is the capacity of the plant\n\nNAIVE METHOD 1: 6.58%\n\n\n\nbenchmark1 = ((naive_method * train_mw_std)/22) * 100\n\n\n For establishing the 2nd baseline error, the team consulted the industry to determine a baseline error. However, even the industry is still conflicted on how to establish a baseline measurement. As of now, the baseline is still being established by policy makers and grid operators. \nLuckily, consulting with industry experts gave the team a framework on how to establish a baseline error per plant. The resulting measure looked at the mean squared error of the day-ahead projection(DAP), also called scheduled (Ex-Ante or RTD), and the actual delivered (Ex-Post or RTX).\nThis was then scaled as a percentage to the capacity of the plant. This was with processed with an industry expert and not included in the notebook. The yielding error resulted with a baseline of 12.01% \n\n\nbenchmark2 = 12.01\n\n\n\n2.5 Model Training Selection\n\n\n The team utilized 4 models:\n- LSTM with 64 nodes\n- LSTM with 32 nodes\n- Stacked LSTM(2 layer) with 64 and 32 nodes\n- Stacked LSTM(2 layer) with 32 and 64 nodes\nDropout of 0.2 and Recurrent Dropout of 0.2 and a RMSProp optimized was implemented for all three.\nThe group decided to use a Epoch=50, with an EarlyStopping of patience=10 and min_delta=0.01 to avoid overfitting and to reduce runtime due to the limited time constraints. Lastly, the ModelCheckpoint was utilized to extract the best model. \n\n\nsteps_per_epoch_cnt = math.floor(train_idx/batch_size)\nval_steps_cnt = round(math.floor((val_idx-train_idx)/batch_size),-1)\n\n\n\n2.5.1 LSTM with 64\n\n\nfilepath_lstm64=\"clark_best_lstm64.hdf5\" \ncp_lstm64 = ModelCheckpoint(filepath_lstm64, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm64 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm64 = [cp_lstm64, es_lstm64]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm64 = Sequential()\nmodel_lstm64.add(layers.LSTM(64, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm64.add(layers.Dense(64,activation='relu'))\nmodel_lstm64.add(layers.Dense(1))\nmodel_lstm64.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm64 = model_lstm64.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm64, verbose=2) \n\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n20/20 [==============================] - 1s 47ms/step - loss: 0.1993\n\nEpoch 00001: val_loss improved from inf to 0.19933, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.4025 - val_loss: 0.1993\nEpoch 2/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1724\n\nEpoch 00002: val_loss improved from 0.19933 to 0.17236, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.2765 - val_loss: 0.1724\nEpoch 3/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1564\n\nEpoch 00003: val_loss improved from 0.17236 to 0.15639, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.2005 - val_loss: 0.1564\nEpoch 4/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1634\n\nEpoch 00004: val_loss did not improve from 0.15639\n - 19s - loss: 0.1664 - val_loss: 0.1634\nEpoch 5/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1585\n\nEpoch 00005: val_loss did not improve from 0.15639\n - 19s - loss: 0.1598 - val_loss: 0.1585\nEpoch 6/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1697\n\nEpoch 00006: val_loss did not improve from 0.15639\n - 19s - loss: 0.1500 - val_loss: 0.1697\nEpoch 7/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1465\n\nEpoch 00007: val_loss improved from 0.15639 to 0.14650, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.1445 - val_loss: 0.1465\nEpoch 8/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1521\n\nEpoch 00008: val_loss did not improve from 0.14650\n - 19s - loss: 0.1431 - val_loss: 0.1521\nEpoch 9/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1565\n\nEpoch 00009: val_loss did not improve from 0.14650\n - 19s - loss: 0.1339 - val_loss: 0.1565\nEpoch 10/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1584\n\nEpoch 00010: val_loss did not improve from 0.14650\n - 19s - loss: 0.1301 - val_loss: 0.1584\nEpoch 11/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1515\n\nEpoch 00011: val_loss did not improve from 0.14650\n - 19s - loss: 0.1297 - val_loss: 0.1515\nEpoch 12/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1624\n\nEpoch 00012: val_loss did not improve from 0.14650\n - 19s - loss: 0.1285 - val_loss: 0.1624\nEpoch 13/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1430\n\nEpoch 00013: val_loss improved from 0.14650 to 0.14299, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.1255 - val_loss: 0.1430\nEpoch 14/50\n20/20 [==============================] - 1s 53ms/step - loss: 0.1586\n\nEpoch 00014: val_loss did not improve from 0.14299\n - 23s - loss: 0.1221 - val_loss: 0.1586\nEpoch 15/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1457\n\nEpoch 00015: val_loss did not improve from 0.14299\n - 19s - loss: 0.1162 - val_loss: 0.1457\nEpoch 16/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1452\n\nEpoch 00016: val_loss did not improve from 0.14299\n - 18s - loss: 0.1186 - val_loss: 0.1452\nEpoch 17/50\n20/20 [==============================] - 1s 39ms/step - loss: 0.1591\n\nEpoch 00017: val_loss did not improve from 0.14299\n - 19s - loss: 0.1152 - val_loss: 0.1591\nEpoch 18/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1563\n\nEpoch 00018: val_loss did not improve from 0.14299\n - 19s - loss: 0.1138 - val_loss: 0.1563\nEpoch 19/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1538\n\nEpoch 00019: val_loss did not improve from 0.14299\n - 18s - loss: 0.1129 - val_loss: 0.1538\nEpoch 20/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1551\n\nEpoch 00020: val_loss did not improve from 0.14299\n - 18s - loss: 0.1128 - val_loss: 0.1551\nEpoch 21/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1505\n\nEpoch 00021: val_loss did not improve from 0.14299\n - 19s - loss: 0.1118 - val_loss: 0.1505\nEpoch 22/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1461\n\nEpoch 00022: val_loss did not improve from 0.14299\n - 19s - loss: 0.1109 - val_loss: 0.1461\nEpoch 23/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.1543\n\nEpoch 00023: val_loss did not improve from 0.14299\n - 23s - loss: 0.1093 - val_loss: 0.1543\nEpoch 00023: early stopping\n\n\n\nloss_lstm64 = history_lstm64.history['loss']\nval_loss_lstm64 = history_lstm64.history['val_loss']\n\nepochs = range(1, len(loss_lstm64) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm64, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm64, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm64) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 3.21%\n\n\n\nlstm64 = ((min(val_loss_lstm64) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.2 LSTM with 32 nodes\n\n\nfilepath_lstm32=\"clark_best_lstm32.hdf5\" \ncp_lstm32 = ModelCheckpoint(filepath_lstm32, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm32 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm32 = [cp_lstm32, es_lstm32]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm32 = Sequential()\nmodel_lstm32.add(layers.LSTM(32, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm32.add(layers.Dense(32, activation='relu'))\nmodel_lstm32.add(layers.Dense(1))\nmodel_lstm32.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm32 = model_lstm32.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm32, verbose=2) \n\nEpoch 1/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.1916\n\nEpoch 00001: val_loss improved from inf to 0.19160, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.4936 - val_loss: 0.1916\nEpoch 2/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1696\n\nEpoch 00002: val_loss improved from 0.19160 to 0.16960, saving model to clark_best_lstm32.hdf5\n - 19s - loss: 0.3050 - val_loss: 0.1696\nEpoch 3/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1460\n\nEpoch 00003: val_loss improved from 0.16960 to 0.14600, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.2672 - val_loss: 0.1460\nEpoch 4/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1610\n\nEpoch 00004: val_loss did not improve from 0.14600\n - 17s - loss: 0.2259 - val_loss: 0.1610\nEpoch 5/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1525\n\nEpoch 00005: val_loss did not improve from 0.14600\n - 17s - loss: 0.2027 - val_loss: 0.1525\nEpoch 6/50\n20/20 [==============================] - 1s 36ms/step - loss: 0.1681\n\nEpoch 00006: val_loss did not improve from 0.14600\n - 16s - loss: 0.1687 - val_loss: 0.1681\nEpoch 7/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.1471\n\nEpoch 00007: val_loss did not improve from 0.14600\n - 17s - loss: 0.1564 - val_loss: 0.1471\nEpoch 8/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1418\n\nEpoch 00008: val_loss improved from 0.14600 to 0.14184, saving model to clark_best_lstm32.hdf5\n - 16s - loss: 0.1522 - val_loss: 0.1418\nEpoch 9/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1502\n\nEpoch 00009: val_loss did not improve from 0.14184\n - 17s - loss: 0.1418 - val_loss: 0.1502\nEpoch 10/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1595\n\nEpoch 00010: val_loss did not improve from 0.14184\n - 19s - loss: 0.1390 - val_loss: 0.1595\nEpoch 11/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.1491\n\nEpoch 00011: val_loss did not improve from 0.14184\n - 17s - loss: 0.1370 - val_loss: 0.1491\nEpoch 12/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1505\n\nEpoch 00012: val_loss did not improve from 0.14184\n - 17s - loss: 0.1363 - val_loss: 0.1505\nEpoch 13/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1394\n\nEpoch 00013: val_loss improved from 0.14184 to 0.13940, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.1334 - val_loss: 0.1394\nEpoch 00013: early stopping\n\n\n\nloss_lstm32 = history_lstm32.history['loss']\nval_loss_lstm32 = history_lstm32.history['val_loss']\n\nepochs = range(1, len(loss_lstm32) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm32, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm32, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm32) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 3.13%\n\n\n\nlstm32 = ((min(val_loss_lstm32) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.3 Stacked LSTM with 64 and 32 nodes\n\n\nfilepath_lstm6432=\"clark_best_lstm6432.hdf5\" \ncp_lstm6432 = ModelCheckpoint(filepath_lstm6432, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm6432 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm6432 = [cp_lstm6432, es_lstm6432]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm6432 = Sequential()\nmodel_lstm6432.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm6432.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm6432.add(layers.Dense(32, activation='relu'))\nmodel_lstm6432.add(layers.Dense(1))\nmodel_lstm6432.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm6432 = model_lstm6432.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm6432, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 77ms/step - loss: 0.1807\n\nEpoch 00001: val_loss improved from inf to 0.18073, saving model to clark_best_lstm6432.hdf5\n - 35s - loss: 0.4037 - val_loss: 0.1807\nEpoch 2/50\n20/20 [==============================] - 2s 80ms/step - loss: 0.1452\n\nEpoch 00002: val_loss improved from 0.18073 to 0.14524, saving model to clark_best_lstm6432.hdf5\n - 34s - loss: 0.2290 - val_loss: 0.1452\nEpoch 3/50\n20/20 [==============================] - 1s 60ms/step - loss: 0.1595\n\nEpoch 00003: val_loss did not improve from 0.14524\n - 41s - loss: 0.1754 - val_loss: 0.1595\nEpoch 4/50\n20/20 [==============================] - 1s 60ms/step - loss: 0.1460\n\nEpoch 00004: val_loss did not improve from 0.14524\n - 34s - loss: 0.1528 - val_loss: 0.1460\nEpoch 5/50\n20/20 [==============================] - 1s 70ms/step - loss: 0.1535\n\nEpoch 00005: val_loss did not improve from 0.14524\n - 37s - loss: 0.1483 - val_loss: 0.1535\nEpoch 6/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1510\n\nEpoch 00006: val_loss did not improve from 0.14524\n - 39s - loss: 0.1385 - val_loss: 0.1510\nEpoch 7/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1482\n\nEpoch 00007: val_loss did not improve from 0.14524\n - 35s - loss: 0.1345 - val_loss: 0.1482\nEpoch 8/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1382\n\nEpoch 00008: val_loss improved from 0.14524 to 0.13823, saving model to clark_best_lstm6432.hdf5\n - 34s - loss: 0.1336 - val_loss: 0.1382\nEpoch 9/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1606\n\nEpoch 00009: val_loss did not improve from 0.13823\n - 34s - loss: 0.1249 - val_loss: 0.1606\nEpoch 10/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1529\n\nEpoch 00010: val_loss did not improve from 0.13823\n - 34s - loss: 0.1214 - val_loss: 0.1529\nEpoch 11/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1430\n\nEpoch 00011: val_loss did not improve from 0.13823\n - 34s - loss: 0.1213 - val_loss: 0.1430\nEpoch 12/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1400\n\nEpoch 00012: val_loss did not improve from 0.13823\n - 34s - loss: 0.1209 - val_loss: 0.1400\nEpoch 00012: early stopping\n\n\n\nloss_lstm6432 = history_lstm6432.history['loss']\nval_loss_lstm6432 = history_lstm6432.history['val_loss']\n\nepochs = range(1, len(loss_lstm6432) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm6432, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm6432, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm6432) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 3.10%\n\n\n\nlstm6432 = ((min(val_loss_lstm6432) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\nfilepath_lstm3264 =\"clark_best_lstm3264.hdf5\" \ncp_lstm3264 = ModelCheckpoint(filepath_lstm3264, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm3264 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm3264 = [cp_lstm3264, es_lstm3264]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm3264 = Sequential()\nmodel_lstm3264.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm3264.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm3264.add(layers.Dense(64, activation='relu'))\nmodel_lstm3264.add(layers.Dense(1))\nmodel_lstm3264.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm3264 = model_lstm3264.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm3264, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 77ms/step - loss: 0.1807\n\nEpoch 00001: val_loss improved from inf to 0.18071, saving model to clark_best_lstm3264.hdf5\n - 36s - loss: 0.4016 - val_loss: 0.1807\nEpoch 2/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1537\n\nEpoch 00002: val_loss improved from 0.18071 to 0.15374, saving model to clark_best_lstm3264.hdf5\n - 33s - loss: 0.2138 - val_loss: 0.1537\nEpoch 3/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1639\n\nEpoch 00003: val_loss did not improve from 0.15374\n - 33s - loss: 0.1709 - val_loss: 0.1639\nEpoch 4/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1513\n\nEpoch 00004: val_loss improved from 0.15374 to 0.15125, saving model to clark_best_lstm3264.hdf5\n - 33s - loss: 0.1528 - val_loss: 0.1513\nEpoch 5/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1401\n\nEpoch 00005: val_loss improved from 0.15125 to 0.14008, saving model to clark_best_lstm3264.hdf5\n - 33s - loss: 0.1501 - val_loss: 0.1401\nEpoch 6/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1481\n\nEpoch 00006: val_loss did not improve from 0.14008\n - 33s - loss: 0.1413 - val_loss: 0.1481\nEpoch 7/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1498\n\nEpoch 00007: val_loss did not improve from 0.14008\n - 33s - loss: 0.1372 - val_loss: 0.1498\nEpoch 8/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1588\n\nEpoch 00008: val_loss did not improve from 0.14008\n - 33s - loss: 0.1362 - val_loss: 0.1588\nEpoch 9/50\n20/20 [==============================] - 1s 71ms/step - loss: 0.1470\n\nEpoch 00009: val_loss did not improve from 0.14008\n - 33s - loss: 0.1273 - val_loss: 0.1470\nEpoch 10/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1598\n\nEpoch 00010: val_loss did not improve from 0.14008\n - 36s - loss: 0.1242 - val_loss: 0.1598\nEpoch 11/50\n20/20 [==============================] - 1s 59ms/step - loss: 0.1428\n\nEpoch 00011: val_loss did not improve from 0.14008\n - 34s - loss: 0.1246 - val_loss: 0.1428\nEpoch 12/50\n20/20 [==============================] - 1s 59ms/step - loss: 0.1389\n\nEpoch 00012: val_loss improved from 0.14008 to 0.13890, saving model to clark_best_lstm3264.hdf5\n - 33s - loss: 0.1239 - val_loss: 0.1389\nEpoch 13/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1276\n\nEpoch 00013: val_loss improved from 0.13890 to 0.12760, saving model to clark_best_lstm3264.hdf5\n - 39s - loss: 0.1203 - val_loss: 0.1276\nEpoch 14/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1609\n\nEpoch 00014: val_loss did not improve from 0.12760\n - 37s - loss: 0.1178 - val_loss: 0.1609\nEpoch 15/50\n20/20 [==============================] - 1s 70ms/step - loss: 0.1446\n\nEpoch 00015: val_loss did not improve from 0.12760\n - 36s - loss: 0.1108 - val_loss: 0.1446\nEpoch 16/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1417\n\nEpoch 00016: val_loss did not improve from 0.12760\n - 36s - loss: 0.1140 - val_loss: 0.1417\nEpoch 17/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1453\n\nEpoch 00017: val_loss did not improve from 0.12760\n - 37s - loss: 0.1111 - val_loss: 0.1453\nEpoch 18/50\n20/20 [==============================] - 1s 72ms/step - loss: 0.1500\n\nEpoch 00018: val_loss did not improve from 0.12760\n - 38s - loss: 0.1105 - val_loss: 0.1500\nEpoch 19/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1578\n\nEpoch 00019: val_loss did not improve from 0.12760\n - 36s - loss: 0.1090 - val_loss: 0.1578\nEpoch 20/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1500\n\nEpoch 00020: val_loss did not improve from 0.12760\n - 37s - loss: 0.1094 - val_loss: 0.1500\nEpoch 21/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1490\n\nEpoch 00021: val_loss did not improve from 0.12760\n - 36s - loss: 0.1088 - val_loss: 0.1490\nEpoch 22/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1482\n\nEpoch 00022: val_loss did not improve from 0.12760\n - 36s - loss: 0.1063 - val_loss: 0.1482\nEpoch 23/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1551\n\nEpoch 00023: val_loss did not improve from 0.12760\n - 36s - loss: 0.1036 - val_loss: 0.1551\nEpoch 00023: early stopping\n\n\n\nloss_lstm3264 = history_lstm3264.history['loss']\nval_loss_lstm3264 = history_lstm3264.history['val_loss']\n\nepochs = range(1, len(loss_lstm3264) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm3264, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm3264, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm3264) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.86%\n\n\n\nlstm3264 = ((min(val_loss_lstm3264) * train_mw_std)/plant_capacity) *100\n\n\n\n3. Results and Discussions\n\n\nnames = ['Industry Agnostic Naive Method',\n         'Industry Standard Naive Method', \n         'LSTM with 64 nodes',\n         'LSTM with 32 nodes',\n         'LSTM+LSTM with 64 nodes and 32 nodes',\n         'LSTM+LSTM with 32 nodes and 64 nodes']\nresults = [benchmark1, \n           benchmark2,\n           lstm64,\n           lstm32,\n           lstm6432,\n           lstm3264]\n\n\nresults = pd.DataFrame(zip(names,results), columns=['Model', 'Error in Percentage'])\n\n\nresults['Error in Percentage'] = round(results['Error in Percentage'],2)\n\n\nresults['Improvement'] = round(1-(results['Error in Percentage']/12.01),4)*100\n\n\nresults\n\n\n\n\n\n\n\n\nModel\nError in Percentage\nImprovement\n\n\n\n\n0\nIndustry Agnostic Naive Method\n6.58\n45.21\n\n\n1\nIndustry Standard Naive Method\n12.01\n0.00\n\n\n2\nLSTM with 64 nodes\n3.21\n73.27\n\n\n3\nLSTM with 32 nodes\n3.13\n73.94\n\n\n4\nLSTM+LSTM with 64 nodes and 32 nodes\n3.10\n74.19\n\n\n5\nLSTM+LSTM with 32 nodes and 64 nodes\n2.86\n76.19\n\n\n\n\n\n\n\n\n\n3.1 Results\n\n\n Results are indicated in the table above. All four models appear to have performed better than the naive models by quite a margin, with the Two layer LSTM with 64 nodes and 32 nodes having the lowest error in terms of percentage. This translates to an improvement by 73.27% (LSTM with 64 nodes), 73.94% (LSTM with 32 nodes), 74.19% (LSTM+LSTM with 64 nodes and 32 nodes) and 76.19% (LSTM+LSTM with 32 nodes and 64 nodes). \n\n\n\n3.2 Discussions and Recommendations\n\n\n\n\nAdding more epochs and allowing the model train even longer would have some benefits. Results could have improved on all four models if the patience and the min_delta were relaxed.\nMy recommendation is to include more technical data in the dataset. Based on industry knowledge, irridiance is one of the main drivers of forecasting solar generation. I would expect that including this as a feature would drastically improve the already impressive model results. Similarly, adding power grid data such as power line capacity and degradation, solar panel angle, may improve in each models accuracy.\n\n\nMoving forward, I would want to explore different types of architectures. Similar journal articles suggested using an LSTM-CNN and/or a CNN-LSTM architectures. These two types of models capture not only the temporal data (LSTM) but also the spatial element (CNN). In addition, using a CNN to include images of the panels may improve the models accuracy. as this may capture the overall health of the panel.\n\n\n\n\n\n4. Summary and Main Learning Points\n\n\n\n4.1 Summary\n\n\n The main objective of the study was to apply deep learning models to forecast solar power supply generation. The data collected was from two online sources, the Wholesale Electrcity Spot Market and Weather Underground. Actual plant generation was obtained from the Wholesale Electricity Spot Market and 13 locational temperature data (Clark International Airport) was collected from Weather Underground. Next, domain knowledge was utilized in the preprocessing data to impute missing data. Here, two forms of benchmarks were idenfied, one was an industry agnostic standard while another error was sourced through consultation of industry experts. Afterwhich, a total of 4 different LSTM architecures were tested and all seem to have beaten both the industry agnostic and industry standard errors. Dropouts and Recurrent Dropouts were utilized during training as fine tuning of the model. In addition, ModelCheckpoints and EarlyStopping was implemented during the training to monitor the output and reduce the runtime. For the results, the 2 layer LSTM with 32 nodes and 64 nodes had the best improvement of 76.19%, from an 12.01% error to 2.86% error. \n\n\n\n4.2 Main Learning Points\n\n\n The main role of domain knowledge and expertise can be augmented with data science and machine learning.\n\n\nThe class has taught me the fundamentals of machine learning and deep learning algorithms, such as MLP, Convolutional, to Recurrent neural networks. However, what I discovered during this process is that domain knowledge influences and augments the data science flow. Yes, we may have developed the fundamentals of understanding deep learning techniques but the true potential of an algorithm can be truly harnessed only when some form of domain knowledge is utilized.\n\n\n\n\nIndustry heuristics can improve the data science pipeline from the feature engineering, benchmarking, model selection and hyperparameter turing. That with prior knowledge on the topic at hand can improve on the accuracy of the resulting model.\n\n\n\n\nOn the contrary, having industry knowledge can allow me to think more critical with regards to other peoples models (whether in journals or at the office), both from the machine learning and technical perspective.\n\n\n\n\nLastly, one may know the inner workings of the model and know how to implement the model, however with domain knowledge, one will also know how to ask the right questions. Questions relating to (1) business value, on how to extract more value from a model, and/or (2) how can we improve or build on the model. Essentially asking what are the next steps, and how can we improve on this.\n\n\nNot really data science related but: tap the industry the topic is related about. You do not know everything in an industry so leverage on the network to figure out things\n\n\nEven if I was already in the energy industry, I was not particularly familiar with renewable energy forecasting and the standard protocols. If my groupmate and I were not familiar with the industry, it would have been very difficult for us to verify protocols. However, since I am blessed with very talented people from the industry, a quick 5 minute call was all we needed to verify certain aspect of the industry.\n\n\n\nFind a field/aspect of data science that you’re interested in, stick to it, build on it and fix it!\n\n\n\nFrom the numerous of talks I have listened in this class, it was already ingrained to me that data science is applicable to almost every industry, and as aspiring data scientists it may be difficult to hone in into a certain field or aspect. For me, I find joy when I see the application of data science to the energy sector as I have experienced and seen where the holes in the analytics are. Now, I have the fundamental skillset to cover up and fix these holes."
  },
  {
    "objectID": "notebooks/2023-04-24-Solar/[DL] Here Comes the Sun - CEBU.html",
    "href": "notebooks/2023-04-24-Solar/[DL] Here Comes the Sun - CEBU.html",
    "title": "Sandro Luis R. Silva",
    "section": "",
    "text": "Here Comes The Sun: Cebu Solar Power Plant\n\n\nLT16 - Sandro Silva and Jac Lin Yu\n\n\nTable of Contents\n\n1. Problem Statement\n2. Methodology\n\n2.1. Loading Prerequisites\n2.2. Dataset and Preprocessing\n2.3. Train, Validation, Test Split and Batch Generation\n2.4. Determining the Benchmark\n2.5. Model Training and Selection\n\n2.5.1 LSTM with 64 nodes\n2.5.2 LSTM with 32 nodes\n2.5.3 Stacked LSTM with 64 and 32 nodes\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\n3. Results and Discussions * 3.1. Results * 3.2. Discussions and Recommendations\n\n\n\n1. Problem Statement\n\n\nIn recent years, the global energy industry has changed its focus away from traditional fuel sources, such as oil and coal, to more alternative and sustainable solutions, such as hydroelectric, solar and wind. However, the shift towards renewable energy poses new threats and challenges to existing power grids across the world. One major concern surrounding renewable energy generation is the inherent variability and intermittency of its fuel source, as this can cause disruptions in power grids. Essentially, renewable energy technologies threaten to overwhelm the grid operators.  Renewable energy forecasting, in particular solar generation supply, may provide power grid operators the ability to predict and balance energy generation and consumption. In addition, power grid operators will be able to balance and schedule the distribution of generated power for not only renewable power plants but also conventional (and rigid) power plants, such as coal and natural gas.\n\n\n\n2. Methodology\n\n\n\n2.1 Loading Prerequisites\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n\n2.2 Dataset and Preprocessing\n\n\n Here, we will load the already cleaned preprocessed dataset of solar generation per plant per hour. The cleaning was processed outside of this notebook, and followed a framework which follows the industry standard of imputing missing values.\nTo list a few processes: - using the average between the before(t-1) and after hour(t+1) - using a 1-week average of all similar intervals (ex, if empty during hour 6 of dec 20, take the moving average of all hour 6 from dec 13-dec19) - using a 4-week moving average of that specific hour and day of the week (ex, if empty during hour 6 of dec 20, we take average of hour 6(dec 13, 6, 29, 22) \n\n\ndf = pd.read_excel('solar_dataset.xlsx', \n                   sheet_name='cebu', \n                   parse_dates=['DATETIME'])\n\n\n Here let us observe the different datatypes, notice that some columns are objects, some of which will be dropped. \n\n\ndf.dtypes\n\nDATETIME         datetime64[ns]\nYEAR                      int64\nMONTH                     int64\nDAY                       int64\nHOUR                      int64\nRESOURCE_ID              object\nMW                      float64\nLocation                 object\nTemperature             float64\nDew Point               float64\nHumidity                float64\nWind Speed              float64\nWind Gust                 int64\nPressure                float64\nCondition                object\nLocation_Name            object\ndtype: object\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nDATETIME\nYEAR\nMONTH\nDAY\nHOUR\nRESOURCE_ID\nMW\nLocation\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nCondition\nLocation_Name\n\n\n\n\n0\n2016-12-26 00:00:00\n2016\n12\n26\n1\n5TOLSOL_G01\n0.0\nRPVM\n79.0\n77.0\n94.0\n1.0\n0\n29.7\nMostly Cloudy\nMACTAN-CEBU\n\n\n1\n2016-12-26 01:00:00\n2016\n12\n26\n2\n5TOLSOL_G01\n0.0\nRPVM\n82.0\n79.0\n89.0\n5.0\n0\n29.8\nMostly Cloudy\nMACTAN-CEBU\n\n\n2\n2016-12-26 02:00:00\n2016\n12\n26\n3\n5TOLSOL_G01\n0.0\nRPVM\n84.0\n79.0\n84.0\n6.0\n0\n29.8\nMostly Cloudy\nMACTAN-CEBU\n\n\n\n\n\n\n\n\n It is customary in traditional time-series regression to convert the month, day, hour into categorical data, which will be later used in one-hot encoding. In addition, this is the usual preprocessing used in the energy industry here in the Philippines. \n\n\ndf.MONTH = df.MONTH.astype('str')\ndf.DAY = df.DAY.astype('str')\ndf.HOUR = df.HOUR.astype('str')\n\n\n Let us drop the unimportant columns \n\n\ndf.drop(['Location_Name', 'Location', \"YEAR\"],axis=1,inplace=True)\n\n\n Let us append the main df with the one-hot encoded dataframe \n\n\ndf = df.join(pd.get_dummies(df[['MONTH','DAY','HOUR','Condition']]))\n\n\n Let us get the unique list of power plants in the region \n\n\nlocations = df.RESOURCE_ID.unique()\n\n\n For this study, let us consider the first power plant \n\n\nlocations[0]\n\n'5TOLSOL_G01'\n\n\n\ndf0 = df[df.RESOURCE_ID == locations[0]].copy()\n\n\n Further dropping of columns as well as reordering of columns \n\n\ndf0 = df0.drop(['RESOURCE_ID','DATETIME','MONTH','DAY','HOUR','Condition'],\n               axis=1)\n\n\ndf0 = df0[['Temperature','MW'] + df0.columns[2:].tolist()].copy()\n\n\ndf0 = df0.astype('float')\n\n\n Final DataFrame to be used \n\n\ndf0.head()\n\n\n\n\n\n\n\n\nTemperature\nMW\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nMONTH_1\nMONTH_10\nMONTH_11\n...\nCondition_Partly Cloudy\nCondition_Partly Cloudy / Windy\nCondition_Rain\nCondition_Rain / Windy\nCondition_Rain Shower\nCondition_Showers in the Vicinity\nCondition_T-Storm\nCondition_Thunder\nCondition_Thunder / Windy\nCondition_Thunder in the Vicinity\n\n\n\n\n0\n79.0\n0.0\n77.0\n94.0\n1.0\n0.0\n29.7\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n82.0\n0.0\n79.0\n89.0\n5.0\n0.0\n29.8\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n84.0\n0.0\n79.0\n84.0\n6.0\n0.0\n29.8\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n84.0\n0.0\n77.0\n79.0\n7.0\n0.0\n29.7\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n86.0\n0.0\n77.0\n74.0\n8.0\n0.0\n29.7\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 99 columns\n\n\n\n\ndf0.shape\n\n(23160, 99)\n\n\n\nplant_capacity = 60\n\n\n\n 2.3 Train,Validate,Test Split and Batch Generation\n\n\n For this analysis, a .8, .1, .1, train-validate-test split was implemented. Since we are dealing with a time-series, the train-validate-test split cannot be randomly selected from the dataset, splitting of dataset should be conducted in a chronogical order. Below we define the starting and ending index for each group \n\n\ntrain_validate_test_split = [.8, .1, .1]\n\ntrain_idx = math.floor(df0.shape[0] * train_validate_test_split[0])\nval_idx = math.floor(df0.shape[0] * train_validate_test_split[1]) + train_idx\n\n\n The generator function below was utilized to yield the appropriate batch. Significant parameters include the lookback, or how far back of the data to consider, delay, or how far into the future are we forecasting, batch_size, refers to the number of training examples utilized in one iteration. \n\n\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, \n              batch_size=128, step=1):\n    \n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, \n                                     max_index, size=batch_size)\n        else:\n            if i + batch_size &gt;= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets\n\n\n Let us mean-center the dataset \n\n\nfloat_data = df0.values\ntrain_mean = float_data[:train_idx].mean(axis=0)\nfloat_data -= train_mean\ntrain_std = float_data[:train_idx].std(axis=0)\nfloat_data /= train_std\ntrain_mw_std = train_std[1]\n\n\n Let us initialize the parameters which will be used for the generation An industry standard practic is to utilize a lookback of either 144 hours (1 week), 288 hours (2 weeks), or 720 hours (1[30-day] month). The initial run was to utilized a 288 hour lookback period due to group’s industry expert’s experience. However, outside consultation with industry experts said 1 week would suffice with this limited dataset. \n\n\nlookback = 144 # lookback(consider) the previous week data\nstep = 1 # in hourly granularity\ndelay = 24 # to forecast the next 24 hours\nbatch_size = 128\n\n\ntrain_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=train_idx,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size)\n\nval_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=train_idx+1,\n    max_index=val_idx,\n    step=step,\n    batch_size=batch_size)\n\ntest_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=val_idx,\n    max_index=None,\n    step=step,\n    batch_size=batch_size)\n\nval_steps = (val_idx - (train_idx+1) - lookback)\ntest_steps = (len(float_data) - (val_idx+1) - lookback)\n\n\n\n2.4 Benchmark\n\n\n The team used two metrics to establish a baseline error:\n(1) the naive method identified in a previous lecture, which we scaled to industry standard\n(2) a different framework suggested by industry\n\n\n\ndef evaluate_naive_method():\n\n    batch_maes = []\n    for step in range(val_steps):\n        if step % 1000 == 0:\n            print(step)\n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    \n    return np.mean(batch_maes)\n\nnaive_method = evaluate_naive_method()\n\n0\n1000\n2000\n\n\n\nprint('NAIVE METHOD 1:', f'{((naive_method * train_mw_std)/plant_capacity):0.2%}') #where 22 is the capacity of the plant\n\nNAIVE METHOD 1: 7.67%\n\n\n\nbenchmark1 = ((naive_method * train_mw_std)/plant_capacity) * 100\n\n\n For establishing the 2nd baseline error, the team consulted the industry to determine a baseline error. However, even the industry is still conflicted on how to establish a baseline measurement. As of now, the baseline is still being established by policy makers and grid operators. \nLuckily, consulting with industry experts gave the team a framework on how to establish a baseline error per plant. The resulting measure looked at the mean squared error of the day-ahead projection(DAP), also called scheduled (Ex-Ante or RTD), and the actual delivered (Ex-Post or RTX).\nThis was then scaled as a percentage to the capacity of the plant. This was with processed with an industry expert and not included in the notebook. The yielding error resulted with a baseline of 15.34% \n\n\nbenchmark2 = 15.34\n\n\n\n2.5 Model Training Selection\n\n\n The team utilized 4 models:\n- LSTM with 64 nodes\n- LSTM with 32 nodes\n- Stacked LSTM(2 layer) with 64 and 32 nodes\n- Stacked LSTM(2 layer) with 32 and 64 nodes\nDropout of 0.2 and Recurrent Dropout of 0.2 and a RMSProp optimized was implemented for all three.\nThe group decided to use a Epoch=50, with an EarlyStopping of patience=10 and min_delta=0.01 to avoid overfitting and to reduce runtime due to the limited time constraints. \n\n\nsteps_per_epoch_cnt = math.floor(train_idx/batch_size)\nval_steps_cnt = round(math.floor((val_idx-train_idx)/batch_size),-1)\n\n\n\n2.5.1 LSTM with 64\n\n\nfilepath_lstm64=\"Cebu_best_lstm64.hdf5\" \ncp_lstm64 = ModelCheckpoint(filepath_lstm64, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm64 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm64 = [cp_lstm64, es_lstm64]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm64 = Sequential()\nmodel_lstm64.add(layers.LSTM(64, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm64.add(layers.Dense(64,activation='relu'))\nmodel_lstm64.add(layers.Dense(1))\nmodel_lstm64.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm64 = model_lstm64.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm64, verbose=2) \n\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2050\n\nEpoch 00001: val_loss improved from inf to 0.20503, saving model to Cebu_best_lstm64.hdf5\n - 22s - loss: 0.3745 - val_loss: 0.2050\nEpoch 2/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.2099\n\nEpoch 00002: val_loss did not improve from 0.20503\n - 19s - loss: 0.2448 - val_loss: 0.2099\nEpoch 3/50\n20/20 [==============================] - 1s 44ms/step - loss: 0.1907\n\nEpoch 00003: val_loss improved from 0.20503 to 0.19068, saving model to Cebu_best_lstm64.hdf5\n - 19s - loss: 0.2185 - val_loss: 0.1907\nEpoch 4/50\n20/20 [==============================] - 1s 44ms/step - loss: 0.1947\n\nEpoch 00004: val_loss did not improve from 0.19068\n - 21s - loss: 0.1975 - val_loss: 0.1947\nEpoch 5/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1928\n\nEpoch 00005: val_loss did not improve from 0.19068\n - 20s - loss: 0.1910 - val_loss: 0.1928\nEpoch 6/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.1991\n\nEpoch 00006: val_loss did not improve from 0.19068\n - 20s - loss: 0.1823 - val_loss: 0.1991\nEpoch 7/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.2138\n\nEpoch 00007: val_loss did not improve from 0.19068\n - 23s - loss: 0.1713 - val_loss: 0.2138\nEpoch 8/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1954\n\nEpoch 00008: val_loss did not improve from 0.19068\n - 20s - loss: 0.1680 - val_loss: 0.1954\nEpoch 9/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.2211\n\nEpoch 00009: val_loss did not improve from 0.19068\n - 19s - loss: 0.1647 - val_loss: 0.2211\nEpoch 10/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.2042\n\nEpoch 00010: val_loss did not improve from 0.19068\n - 23s - loss: 0.1567 - val_loss: 0.2042\nEpoch 11/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.2127\n\nEpoch 00011: val_loss did not improve from 0.19068\n - 20s - loss: 0.1566 - val_loss: 0.2127\nEpoch 12/50\n20/20 [==============================] - 1s 56ms/step - loss: 0.2272\n\nEpoch 00012: val_loss did not improve from 0.19068\n - 22s - loss: 0.1541 - val_loss: 0.2272\nEpoch 13/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.2127\n\nEpoch 00013: val_loss did not improve from 0.19068\n - 21s - loss: 0.1491 - val_loss: 0.2127\nEpoch 00013: early stopping\n\n\n\nloss_lstm64 = history_lstm64.history['loss']\nval_loss_lstm64 = history_lstm64.history['val_loss']\n\nepochs = range(1, len(loss_lstm64) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm64, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm64, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm64) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 4.87%\n\n\n\nlstm64 = ((min(val_loss_lstm64) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.2 LSTM with 32 nodes\n\n\nfilepath_lstm32=\"Cebu_best_lstm32.hdf5\" \ncp_lstm32 = ModelCheckpoint(filepath_lstm32, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm32 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm32 = [cp_lstm32, es_lstm32]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm32 = Sequential()\nmodel_lstm32.add(layers.LSTM(32, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm32.add(layers.Dense(32, activation='relu'))\nmodel_lstm32.add(layers.Dense(1))\nmodel_lstm32.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm32 = model_lstm32.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm32, verbose=2) \n\nEpoch 1/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2389\n\nEpoch 00001: val_loss improved from inf to 0.23892, saving model to Cebu_best_lstm32.hdf5\n - 18s - loss: 0.4182 - val_loss: 0.2389\nEpoch 2/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1922\n\nEpoch 00002: val_loss improved from 0.23892 to 0.19216, saving model to Cebu_best_lstm32.hdf5\n - 16s - loss: 0.2575 - val_loss: 0.1922\nEpoch 3/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1897\n\nEpoch 00003: val_loss improved from 0.19216 to 0.18967, saving model to Cebu_best_lstm32.hdf5\n - 16s - loss: 0.2250 - val_loss: 0.1897\nEpoch 4/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1686\n\nEpoch 00004: val_loss improved from 0.18967 to 0.16857, saving model to Cebu_best_lstm32.hdf5\n - 17s - loss: 0.2017 - val_loss: 0.1686\nEpoch 5/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.1814\n\nEpoch 00005: val_loss did not improve from 0.16857\n - 17s - loss: 0.1978 - val_loss: 0.1814\nEpoch 6/50\n20/20 [==============================] - 1s 36ms/step - loss: 0.1943\n\nEpoch 00006: val_loss did not improve from 0.16857\n - 16s - loss: 0.1892 - val_loss: 0.1943\nEpoch 7/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1845\n\nEpoch 00007: val_loss did not improve from 0.16857\n - 17s - loss: 0.1793 - val_loss: 0.1845\nEpoch 8/50\n20/20 [==============================] - 1s 47ms/step - loss: 0.1899\n\nEpoch 00008: val_loss did not improve from 0.16857\n - 18s - loss: 0.1751 - val_loss: 0.1899\nEpoch 9/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.2050\n\nEpoch 00009: val_loss did not improve from 0.16857\n - 20s - loss: 0.1721 - val_loss: 0.2050\nEpoch 10/50\n20/20 [==============================] - 1s 32ms/step - loss: 0.1828\n\nEpoch 00010: val_loss did not improve from 0.16857\n - 17s - loss: 0.1644 - val_loss: 0.1828\nEpoch 11/50\n20/20 [==============================] - 1s 37ms/step - loss: 0.1875\n\nEpoch 00011: val_loss did not improve from 0.16857\n - 16s - loss: 0.1676 - val_loss: 0.1875\nEpoch 12/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1935\n\nEpoch 00012: val_loss did not improve from 0.16857\n - 17s - loss: 0.1646 - val_loss: 0.1935\nEpoch 13/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1911\n\nEpoch 00013: val_loss did not improve from 0.16857\n - 17s - loss: 0.1607 - val_loss: 0.1911\nEpoch 14/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1989\n\nEpoch 00014: val_loss did not improve from 0.16857\n - 17s - loss: 0.1528 - val_loss: 0.1989\nEpoch 00014: early stopping\n\n\n\nloss_lstm32 = history_lstm32.history['loss']\nval_loss_lstm32 = history_lstm32.history['val_loss']\n\nepochs = range(1, len(loss_lstm32) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm32, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm32, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm32) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 4.30%\n\n\n\nlstm32 = ((min(val_loss_lstm32) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.3 Stacked LSTM with 64 and 32 nodes\n\n\nfilepath_lstm6432=\"Cebu_best_lstm6432.hdf5\" \ncp_lstm6432 = ModelCheckpoint(filepath_lstm6432, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm6432 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm6432 = [cp_lstm6432, es_lstm6432]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm6432 = Sequential()\nmodel_lstm6432.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm6432.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm6432.add(layers.Dense(32, activation='relu'))\nmodel_lstm6432.add(layers.Dense(1))\nmodel_lstm6432.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm6432 = model_lstm6432.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm6432, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 93ms/step - loss: 0.2058\n\nEpoch 00001: val_loss improved from inf to 0.20581, saving model to Cebu_best_lstm6432.hdf5\n - 40s - loss: 0.3520 - val_loss: 0.2058\nEpoch 2/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2092\n\nEpoch 00002: val_loss did not improve from 0.20581\n - 39s - loss: 0.2220 - val_loss: 0.2092\nEpoch 3/50\n20/20 [==============================] - 2s 80ms/step - loss: 0.2094\n\nEpoch 00003: val_loss did not improve from 0.20581\n - 41s - loss: 0.2063 - val_loss: 0.2094\nEpoch 4/50\n20/20 [==============================] - 1s 61ms/step - loss: 0.1837\n\nEpoch 00004: val_loss improved from 0.20581 to 0.18374, saving model to Cebu_best_lstm6432.hdf5\n - 38s - loss: 0.1854 - val_loss: 0.1837\nEpoch 5/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1957\n\nEpoch 00005: val_loss did not improve from 0.18374\n - 37s - loss: 0.1823 - val_loss: 0.1957\nEpoch 6/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1878\n\nEpoch 00006: val_loss did not improve from 0.18374\n - 36s - loss: 0.1751 - val_loss: 0.1878\nEpoch 7/50\n20/20 [==============================] - 2s 76ms/step - loss: 0.1899\n\nEpoch 00007: val_loss did not improve from 0.18374\n - 39s - loss: 0.1630 - val_loss: 0.1899\nEpoch 8/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2210\n\nEpoch 00008: val_loss did not improve from 0.18374\n - 39s - loss: 0.1590 - val_loss: 0.2210\nEpoch 9/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.2153\n\nEpoch 00009: val_loss did not improve from 0.18374\n - 37s - loss: 0.1539 - val_loss: 0.2153\nEpoch 10/50\n20/20 [==============================] - 1s 72ms/step - loss: 0.2149\n\nEpoch 00010: val_loss did not improve from 0.18374\n - 38s - loss: 0.1487 - val_loss: 0.2149\nEpoch 11/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2119\n\nEpoch 00011: val_loss did not improve from 0.18374\n - 36s - loss: 0.1475 - val_loss: 0.2119\nEpoch 12/50\n20/20 [==============================] - 1s 75ms/step - loss: 0.2246\n\nEpoch 00012: val_loss did not improve from 0.18374\n - 37s - loss: 0.1471 - val_loss: 0.2246\nEpoch 13/50\n20/20 [==============================] - 1s 62ms/step - loss: 0.2063\n\nEpoch 00013: val_loss did not improve from 0.18374\n - 37s - loss: 0.1418 - val_loss: 0.2063\nEpoch 14/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2216\n\nEpoch 00014: val_loss did not improve from 0.18374\n - 38s - loss: 0.1352 - val_loss: 0.2216\nEpoch 00014: early stopping\n\n\n\nloss_lstm6432 = history_lstm6432.history['loss']\nval_loss_lstm6432 = history_lstm6432.history['val_loss']\n\nepochs = range(1, len(loss_lstm6432) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm6432, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm6432, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm6432) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 4.69%\n\n\n\nlstm6432 = ((min(val_loss_lstm6432) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\nfilepath_lstm3264 =\"Cebu_best_lstm3264.hdf5\" \ncp_lstm3264 = ModelCheckpoint(filepath_lstm3264, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm3264 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm3264 = [cp_lstm3264, es_lstm3264]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm3264 = Sequential()\nmodel_lstm3264.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm3264.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm3264.add(layers.Dense(64, activation='relu'))\nmodel_lstm3264.add(layers.Dense(1))\nmodel_lstm3264.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm3264 = model_lstm3264.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm3264, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 1s 71ms/step - loss: 0.1866\n\nEpoch 00001: val_loss improved from inf to 0.18658, saving model to Cebu_best_lstm3264.hdf5\n - 38s - loss: 0.3441 - val_loss: 0.1866\nEpoch 2/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.2028\n\nEpoch 00002: val_loss did not improve from 0.18658\n - 34s - loss: 0.2206 - val_loss: 0.2028\nEpoch 3/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2007\n\nEpoch 00003: val_loss did not improve from 0.18658\n - 36s - loss: 0.2065 - val_loss: 0.2007\nEpoch 4/50\n20/20 [==============================] - 1s 70ms/step - loss: 0.2070\n\nEpoch 00004: val_loss did not improve from 0.18658\n - 38s - loss: 0.1875 - val_loss: 0.2070\nEpoch 5/50\n20/20 [==============================] - 1s 74ms/step - loss: 0.2092\n\nEpoch 00005: val_loss did not improve from 0.18658\n - 37s - loss: 0.1841 - val_loss: 0.2092\nEpoch 6/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.1759\n\nEpoch 00006: val_loss improved from 0.18658 to 0.17589, saving model to Cebu_best_lstm3264.hdf5\n - 36s - loss: 0.1766 - val_loss: 0.1759\nEpoch 7/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1925\n\nEpoch 00007: val_loss did not improve from 0.17589\n - 37s - loss: 0.1656 - val_loss: 0.1925\nEpoch 8/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1958\n\nEpoch 00008: val_loss did not improve from 0.17589\n - 37s - loss: 0.1625 - val_loss: 0.1958\nEpoch 9/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.1980\n\nEpoch 00009: val_loss did not improve from 0.17589\n - 35s - loss: 0.1564 - val_loss: 0.1980\nEpoch 10/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.2024\n\nEpoch 00010: val_loss did not improve from 0.17589\n - 34s - loss: 0.1515 - val_loss: 0.2024\nEpoch 11/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1945\n\nEpoch 00011: val_loss did not improve from 0.17589\n - 35s - loss: 0.1507 - val_loss: 0.1945\nEpoch 12/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2235\n\nEpoch 00012: val_loss did not improve from 0.17589\n - 37s - loss: 0.1500 - val_loss: 0.2235\nEpoch 13/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2137\n\nEpoch 00013: val_loss did not improve from 0.17589\n - 38s - loss: 0.1436 - val_loss: 0.2137\nEpoch 14/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2093\n\nEpoch 00014: val_loss did not improve from 0.17589\n - 37s - loss: 0.1375 - val_loss: 0.2093\nEpoch 15/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2092\n\nEpoch 00015: val_loss did not improve from 0.17589\n - 40s - loss: 0.1401 - val_loss: 0.2092\nEpoch 16/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1887\n\nEpoch 00016: val_loss did not improve from 0.17589\n - 37s - loss: 0.1418 - val_loss: 0.1887\nEpoch 00016: early stopping\n\n\n\nloss_lstm3264 = history_lstm3264.history['loss']\nval_loss_lstm3264 = history_lstm3264.history['val_loss']\n\nepochs = range(1, len(loss_lstm3264) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm3264, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm3264, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm3264) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 4.49%\n\n\n\nlstm3264 = ((min(val_loss_lstm3264) * train_mw_std)/plant_capacity) * 100\n\n\nlstm3264\n\n0.0448781794602682\n\n\n\n\n3. Results and Discussions\n\n\n\n3.1 Results\n\n\nnames = ['Industry Agnostic Naive Method',\n         'Industry Standard Naive Method', \n         'LSTM with 64 nodes',\n         'LSTM with 32 nodes',\n         'LSTM+LSTM with 64 nodes and 32 nodes',\n         'LSTM+LSTM with 32 nodes and 64 nodes']\nresults = [benchmark1, \n           benchmark2,\n           lstm64,\n           lstm32,\n           lstm6432,\n           lstm3264]\n\n\nresults = pd.DataFrame(zip(names,results), columns=['Model', 'Error in Percentage'])\n\n\nresults['Error in Percentage'] = round(results['Error in Percentage'],2)\n\n\nresults['Improvement'] = round(1-(results['Error in Percentage']/benchmark2),4)*100\n\n\nresults\n\n\n\n\n\n\n\n\nModel\nError in Percentage\nImprovement\n\n\n\n\n0\nIndustry Agnostic Naive Method\n7.67\n50.00\n\n\n1\nIndustry Standard Naive Method\n15.34\n0.00\n\n\n2\nLSTM with 64 nodes\n4.87\n68.25\n\n\n3\nLSTM with 32 nodes\n4.30\n71.97\n\n\n4\nLSTM+LSTM with 64 nodes and 32 nodes\n4.69\n69.43\n\n\n5\nLSTM+LSTM with 32 nodes and 64 nodes\n4.49\n70.73"
  },
  {
    "objectID": "notebooks/2023-04-24-Solar/[DL] Here Comes the Sun - METROMANILA.html",
    "href": "notebooks/2023-04-24-Solar/[DL] Here Comes the Sun - METROMANILA.html",
    "title": "Sandro Luis R. Silva",
    "section": "",
    "text": "Here Comes The Sun: Clark Solar Power Plant\n\n\nLT16 - Sandro Silva and Jac Lin Yu\n\n\nTable of Contents\n\n1. Problem Statement\n2. Methodology\n\n2.1. Loading Prerequisites\n2.2. Dataset and Preprocessing\n2.3. Train, Validation, Test Split and Batch Generation\n2.4. Determining the Benchmark\n2.5. Model Training and Selection\n\n2.5.1 LSTM with 64 nodes\n2.5.2 LSTM with 32 nodes\n2.5.3 Stacked LSTM with 64 and 32 nodes\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\n3. Results and Discussions * 3.1. Results * 3.2. Discussions and Recommendations\n\n\n\n1. Problem Statement\n\n\nIn recent years, the global energy industry has changed its focus away from traditional fuel sources, such as oil and coal, to more alternative and sustainable solutions, such as hydroelectric, solar and wind. However, the shift towards renewable energy poses new threats and challenges to existing power grids across the world. One major concern surrounding renewable energy generation is the inherent variability and intermittency of its fuel source, as this can cause disruptions in power grids. Essentially, renewable energy technologies threaten to overwhelm the grid operators.  Renewable energy forecasting, in particular solar generation supply, may provide power grid operators the ability to predict and balance energy generation and consumption. In addition, power grid operators will be able to balance and schedule the distribution of generated power for not only renewable power plants but also conventional (and rigid) power plants, such as coal and natural gas.\n\n\n\n2. Methodology\n\n\n\n2.1 Loading Prerequisites\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n\n2.2 Dataset and Preprocessing\n\n\n Here, we will load the already cleaned preprocessed dataset of solar generation per plant per hour. The cleaning was processed outside of this notebook, and followed a framework which follows the industry standard of imputing missing values.\nTo list a few processes: - using the average between the before(t-1) and after hour(t+1) - using a 1-week average of all similar intervals (ex, if empty during hour 6 of dec 20, take the moving average of all hour 6 from dec 13-dec19) - using a 4-week moving average of that specific hour and day of the week (ex, if empty during hour 6 of dec 20, we take average of hour 6(dec 13, 6, 29, 22) \n\n\ndf = pd.read_excel('solar_dataset.xlsx', \n                   sheet_name='metro_manila', \n                   parse_dates=['DATETIME'])\n\n\n Here let us observe the different datatypes, notice that some columns are objects, some of which will be dropped. \n\n\ndf.dtypes\n\nDATETIME         datetime64[ns]\nYEAR                      int64\nMONTH                     int64\nDAY                       int64\nHOUR                      int64\nRESOURCE_ID              object\nMW                      float64\nLocation                 object\nTemperature             float64\nDew Point               float64\nHumidity                float64\nWind Speed                int64\nWind Gust                 int64\nPressure                float64\nCondition                object\nLocation_Name            object\ndtype: object\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nDATETIME\nYEAR\nMONTH\nDAY\nHOUR\nRESOURCE_ID\nMW\nLocation\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nCondition\nLocation_Name\n\n\n\n\n0\n2016-12-26 00:00:00\n2016\n12\n26\n1\n2VALSOL_G01\n0.0\nRPLL\n77.0\n73.0\n89.0\n20\n31\n29.6\nLight Rain\nNINOY AQUINO INT\n\n\n1\n2016-12-26 01:00:00\n2016\n12\n26\n2\n2VALSOL_G01\n0.0\nRPLL\n77.0\n75.0\n94.0\n15\n0\n29.6\nLight Rain\nNINOY AQUINO INT\n\n\n2\n2016-12-26 02:00:00\n2016\n12\n26\n3\n2VALSOL_G01\n0.0\nRPLL\n75.0\n75.0\n100.0\n15\n0\n29.6\nLight Rain\nNINOY AQUINO INT\n\n\n\n\n\n\n\n\n It is customary in traditional time-series regression to convert the month, day, hour into categorical data, which will be later used in one-hot encoding. In addition, this is the usual preprocessing used in the energy industry here in the Philippines. \n\n\ndf.MONTH = df.MONTH.astype('str')\ndf.DAY = df.DAY.astype('str')\ndf.HOUR = df.HOUR.astype('str')\n\n\n Let us drop the unimportant columns \n\n\ndf.drop(['Location_Name', 'Location', \"YEAR\"],axis=1,inplace=True)\n\n\n Let us append the main df with the one-hot encoded dataframe \n\n\ndf = df.join(pd.get_dummies(df[['MONTH','DAY','HOUR','Condition']]))\n\n\n Let us get the unique list of power plants in the region \n\n\nlocations = df.RESOURCE_ID.unique()\n\n\n For this study, let us consider the first power plant \n\n\nlocations[0]\n\n'2VALSOL_G01'\n\n\n\ndf0 = df[df.RESOURCE_ID == locations[0]].copy()\n\n\n Further dropping of columns as well as reordering of columns \n\n\ndf0 = df0.drop(['RESOURCE_ID','DATETIME','MONTH','DAY','HOUR','Condition'],\n               axis=1)\n\n\ndf0 = df0[['Temperature','MW'] + df0.columns[2:].tolist()].copy()\n\n\ndf0 = df0.astype('float')\n\n\n Final DataFrame to be used \n\n\ndf0.head()\n\n\n\n\n\n\n\n\nTemperature\nMW\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nMONTH_1\nMONTH_10\nMONTH_11\n...\nCondition_Mostly Cloudy / Windy\nCondition_Partly Cloudy\nCondition_Partly Cloudy / Windy\nCondition_Rain\nCondition_Rain / Windy\nCondition_Rain Shower\nCondition_Showers in the Vicinity\nCondition_T-Storm\nCondition_Thunder\nCondition_Thunder in the Vicinity\n\n\n\n\n0\n77.0\n0.0\n73.0\n89.0\n20.0\n31.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n77.0\n0.0\n75.0\n94.0\n15.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n75.0\n0.0\n75.0\n100.0\n15.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n75.0\n0.0\n75.0\n100.0\n16.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n77.0\n0.0\n77.0\n100.0\n13.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 99 columns\n\n\n\n\ndf0.shape\n\n(24021, 99)\n\n\n\nplant_capacity = 8.5\n\n\n\n 2.3 Train,Validate,Test Split and Batch Generation\n\n\n For this analysis, a .8, .1, .1, train-validate-test split was implemented. Since we are dealing with a time-series, the train-validate-test split cannot be randomly selected from the dataset, splitting of dataset should be conducted in a chronogical order. Below we define the starting and ending index for each group \n\n\ntrain_validate_test_split = [.8, .1, .1]\n\ntrain_idx = math.floor(df0.shape[0] * train_validate_test_split[0])\nval_idx = math.floor(df0.shape[0] * train_validate_test_split[1]) + train_idx\n\n\n The generator function below was utilized to yield the appropriate batch. Significant parameters include the lookback, or how far back of the data to consider, delay, or how far into the future are we forecasting, batch_size, refers to the number of training examples utilized in one iteration. \n\n\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, \n              batch_size=128, step=1):\n    \n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, \n                                     max_index, size=batch_size)\n        else:\n            if i + batch_size &gt;= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets\n\n\n Let us mean-center the dataset \n\n\nfloat_data = df0.values\ntrain_mean = float_data[:train_idx].mean(axis=0)\nfloat_data -= train_mean\ntrain_std = float_data[:train_idx].std(axis=0)\nfloat_data /= train_std\ntrain_mw_std = train_std[1]\n\n\n Let us initialize the parameters which will be used for the generation An industry standard practic is to utilize a lookback of either 144 hours (1 week), 288 hours (2 weeks), or 720 hours (1[30-day] month). The initial run was to utilized a 288 hour lookback period due to group’s industry expert’s experience. However, outside consultation with industry experts said 1 week would suffice with this limited dataset. \n\n\nlookback = 144 # lookback(consider) the previous week data\nstep = 1 # in hourly granularity\ndelay = 24 # to forecast the next 24 hours\nbatch_size = 128\n\n\ntrain_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=train_idx,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size)\n\nval_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=train_idx+1,\n    max_index=val_idx,\n    step=step,\n    batch_size=batch_size)\n\ntest_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=val_idx,\n    max_index=None,\n    step=step,\n    batch_size=batch_size)\n\nval_steps = (val_idx - (train_idx+1) - lookback)\ntest_steps = (len(float_data) - (val_idx+1) - lookback)\n\n\n\n2.4 Benchmark\n\n\n The team used two metrics to establish a baseline error:\n(1) the naive method identified in a previous lecture, which we scaled to industry standard\n(2) a different framework suggested by industry\n\n\n\ndef evaluate_naive_method():\n\n    batch_maes = []\n    for step in range(val_steps):\n        if step % 1000 == 0:\n            print(step)\n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    \n    return np.mean(batch_maes)\n\nnaive_method = evaluate_naive_method()\n\n0\n1000\n2000\n\n\n\nprint('NAIVE METHOD 1:', f'{((naive_method * train_mw_std)/plant_capacity):0.2%}') #where 22 is the capacity of the plant\n\nNAIVE METHOD 1: 5.84%\n\n\n\nbenchmark1 = ((naive_method * train_mw_std)/plant_capacity) * 100\n\n\n For establishing the 2nd baseline error, the team consulted the industry to determine a baseline error. However, even the industry is still conflicted on how to establish a baseline measurement. As of now, the baseline is still being established by policy makers and grid operators. \nLuckily, consulting with industry experts gave the team a framework on how to establish a baseline error per plant. The resulting measure looked at the mean squared error of the day-ahead projection(DAP), also called scheduled (Ex-Ante or RTD), and the actual delivered (Ex-Post or RTX).\nThis was then scaled as a percentage to the capacity of the plant. This was with processed with an industry expert and not included in the notebook. The yielding error resulted with a baseline of 12.48% \n\n\nbenchmark2 = 12.48\n\n\n\n2.5 Model Training Selection\n\n\n The team utilized 4 models:\n- LSTM with 64 nodes\n- LSTM with 32 nodes\n- Stacked LSTM(2 layer) with 64 and 32 nodes\n- Stacked LSTM(2 layer) with 32 and 64 nodes\nDropout of 0.2 and Recurrent Dropout of 0.2 and a RMSProp optimized was implemented for all three.\nThe group decided to use a Epoch=50, with an EarlyStopping of patience=10 and min_delta=0.01 to avoid overfitting and to reduce runtime due to the limited time constraints. \n\n\nsteps_per_epoch_cnt = math.floor(train_idx/batch_size)\nval_steps_cnt = round(math.floor((val_idx-train_idx)/batch_size),-1)\n\n\n\n2.5.1 LSTM with 64\n\n\nfilepath_lstm64=\"clark_best_lstm64.hdf5\" \ncp_lstm64 = ModelCheckpoint(filepath_lstm64, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm64 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm64 = [cp_lstm64, es_lstm64]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm64 = Sequential()\nmodel_lstm64.add(layers.LSTM(64, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm64.add(layers.Dense(64,activation='relu'))\nmodel_lstm64.add(layers.Dense(1))\nmodel_lstm64.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm64 = model_lstm64.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm64, verbose=2) \n\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n20/20 [==============================] - 1s 48ms/step - loss: 0.1711\n\nEpoch 00001: val_loss improved from inf to 0.17113, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.3853 - val_loss: 0.1711\nEpoch 2/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1504\n\nEpoch 00002: val_loss improved from 0.17113 to 0.15042, saving model to clark_best_lstm64.hdf5\n - 22s - loss: 0.2661 - val_loss: 0.1504\nEpoch 3/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.1373\n\nEpoch 00003: val_loss improved from 0.15042 to 0.13729, saving model to clark_best_lstm64.hdf5\n - 22s - loss: 0.2066 - val_loss: 0.1373\nEpoch 4/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.1295\n\nEpoch 00004: val_loss improved from 0.13729 to 0.12954, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.1742 - val_loss: 0.1295\nEpoch 5/50\n20/20 [==============================] - 1s 50ms/step - loss: 0.1284\n\nEpoch 00005: val_loss improved from 0.12954 to 0.12843, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.1633 - val_loss: 0.1284\nEpoch 6/50\n20/20 [==============================] - 1s 47ms/step - loss: 0.1436\n\nEpoch 00006: val_loss did not improve from 0.12843\n - 22s - loss: 0.1576 - val_loss: 0.1436\nEpoch 7/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.1292\n\nEpoch 00007: val_loss did not improve from 0.12843\n - 22s - loss: 0.1494 - val_loss: 0.1292\nEpoch 8/50\n20/20 [==============================] - 1s 49ms/step - loss: 0.1321\n\nEpoch 00008: val_loss did not improve from 0.12843\n - 21s - loss: 0.1475 - val_loss: 0.1321\nEpoch 9/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.1304\n\nEpoch 00009: val_loss did not improve from 0.12843\n - 22s - loss: 0.1404 - val_loss: 0.1304\nEpoch 10/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1405\n\nEpoch 00010: val_loss did not improve from 0.12843\n - 21s - loss: 0.1370 - val_loss: 0.1405\nEpoch 11/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1361\n\nEpoch 00011: val_loss did not improve from 0.12843\n - 21s - loss: 0.1375 - val_loss: 0.1361\nEpoch 12/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.1294\n\nEpoch 00012: val_loss did not improve from 0.12843\n - 20s - loss: 0.1328 - val_loss: 0.1294\nEpoch 13/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1366\n\nEpoch 00013: val_loss did not improve from 0.12843\n - 20s - loss: 0.1295 - val_loss: 0.1366\nEpoch 00013: early stopping\n\n\n\nloss_lstm64 = history_lstm64.history['loss']\nval_loss_lstm64 = history_lstm64.history['val_loss']\n\nepochs = range(1, len(loss_lstm64) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm64, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm64, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm64) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.82%\n\n\n\nlstm64 = ((min(val_loss_lstm64) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.2 LSTM with 32 nodes\n\n\nfilepath_lstm32=\"clark_best_lstm32.hdf5\" \ncp_lstm32 = ModelCheckpoint(filepath_lstm32, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm32 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm32 = [cp_lstm32, es_lstm32]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm32 = Sequential()\nmodel_lstm32.add(layers.LSTM(32, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm32.add(layers.Dense(32, activation='relu'))\nmodel_lstm32.add(layers.Dense(1))\nmodel_lstm32.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm32 = model_lstm32.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm32, verbose=2) \n\nEpoch 1/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1862\n\nEpoch 00001: val_loss improved from inf to 0.18618, saving model to clark_best_lstm32.hdf5\n - 18s - loss: 0.4808 - val_loss: 0.1862\nEpoch 2/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1496\n\nEpoch 00002: val_loss improved from 0.18618 to 0.14965, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.3122 - val_loss: 0.1496\nEpoch 3/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1462\n\nEpoch 00003: val_loss improved from 0.14965 to 0.14625, saving model to clark_best_lstm32.hdf5\n - 18s - loss: 0.2731 - val_loss: 0.1462\nEpoch 4/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.1438\n\nEpoch 00004: val_loss improved from 0.14625 to 0.14383, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.2481 - val_loss: 0.1438\nEpoch 5/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1364\n\nEpoch 00005: val_loss improved from 0.14383 to 0.13638, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.2247 - val_loss: 0.1364\nEpoch 6/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1361\n\nEpoch 00006: val_loss improved from 0.13638 to 0.13614, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.2101 - val_loss: 0.1361\nEpoch 7/50\n20/20 [==============================] - 1s 46ms/step - loss: 0.1266\n\nEpoch 00007: val_loss improved from 0.13614 to 0.12662, saving model to clark_best_lstm32.hdf5\n - 19s - loss: 0.1899 - val_loss: 0.1266\nEpoch 8/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1360\n\nEpoch 00008: val_loss did not improve from 0.12662\n - 17s - loss: 0.1716 - val_loss: 0.1360\nEpoch 9/50\n20/20 [==============================] - 1s 36ms/step - loss: 0.1264\n\nEpoch 00009: val_loss improved from 0.12662 to 0.12641, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.1583 - val_loss: 0.1264\nEpoch 10/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1375\n\nEpoch 00010: val_loss did not improve from 0.12641\n - 16s - loss: 0.1496 - val_loss: 0.1375\nEpoch 11/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1290\n\nEpoch 00011: val_loss did not improve from 0.12641\n - 17s - loss: 0.1506 - val_loss: 0.1290\nEpoch 12/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1341\n\nEpoch 00012: val_loss did not improve from 0.12641\n - 17s - loss: 0.1457 - val_loss: 0.1341\nEpoch 13/50\n20/20 [==============================] - 1s 54ms/step - loss: 0.1269\n\nEpoch 00013: val_loss did not improve from 0.12641\n - 19s - loss: 0.1442 - val_loss: 0.1269\nEpoch 14/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1369\n\nEpoch 00014: val_loss did not improve from 0.12641\n - 19s - loss: 0.1391 - val_loss: 0.1369\nEpoch 15/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1328\n\nEpoch 00015: val_loss did not improve from 0.12641\n - 18s - loss: 0.1361 - val_loss: 0.1328\nEpoch 00015: early stopping\n\n\n\nloss_lstm32 = history_lstm32.history['loss']\nval_loss_lstm32 = history_lstm32.history['val_loss']\n\nepochs = range(1, len(loss_lstm32) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm32, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm32, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm32) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.77%\n\n\n\nlstm32 = ((min(val_loss_lstm32) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.3 Stacked LSTM with 64 and 32 nodes\n\n\nfilepath_lstm6432=\"clark_best_lstm6432.hdf5\" \ncp_lstm6432 = ModelCheckpoint(filepath_lstm6432, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm6432 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm6432 = [cp_lstm6432, es_lstm6432]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm6432 = Sequential()\nmodel_lstm6432.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm6432.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm6432.add(layers.Dense(32, activation='relu'))\nmodel_lstm6432.add(layers.Dense(1))\nmodel_lstm6432.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm6432 = model_lstm6432.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm6432, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 84ms/step - loss: 0.1595\n\nEpoch 00001: val_loss improved from inf to 0.15946, saving model to clark_best_lstm6432.hdf5\n - 46s - loss: 0.4159 - val_loss: 0.1595\nEpoch 2/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1199\n\nEpoch 00002: val_loss improved from 0.15946 to 0.11991, saving model to clark_best_lstm6432.hdf5\n - 42s - loss: 0.2417 - val_loss: 0.1199\nEpoch 3/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1290\n\nEpoch 00003: val_loss did not improve from 0.11991\n - 40s - loss: 0.1874 - val_loss: 0.1290\nEpoch 4/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1210\n\nEpoch 00004: val_loss did not improve from 0.11991\n - 40s - loss: 0.1660 - val_loss: 0.1210\nEpoch 5/50\n20/20 [==============================] - 2s 77ms/step - loss: 0.1203\n\nEpoch 00005: val_loss did not improve from 0.11991\n - 41s - loss: 0.1551 - val_loss: 0.1203\nEpoch 6/50\n20/20 [==============================] - 2s 83ms/step - loss: 0.1340\n\nEpoch 00006: val_loss did not improve from 0.11991\n - 41s - loss: 0.1528 - val_loss: 0.1340\nEpoch 7/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1215\n\nEpoch 00007: val_loss did not improve from 0.11991\n - 39s - loss: 0.1419 - val_loss: 0.1215\nEpoch 8/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1299\n\nEpoch 00008: val_loss did not improve from 0.11991\n - 38s - loss: 0.1433 - val_loss: 0.1299\nEpoch 9/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1234\n\nEpoch 00009: val_loss did not improve from 0.11991\n - 38s - loss: 0.1355 - val_loss: 0.1234\nEpoch 10/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1421\n\nEpoch 00010: val_loss did not improve from 0.11991\n - 38s - loss: 0.1325 - val_loss: 0.1421\nEpoch 11/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1251\n\nEpoch 00011: val_loss did not improve from 0.11991\n - 39s - loss: 0.1322 - val_loss: 0.1251\nEpoch 12/50\n20/20 [==============================] - 1s 72ms/step - loss: 0.1347\n\nEpoch 00012: val_loss did not improve from 0.11991\n - 40s - loss: 0.1292 - val_loss: 0.1347\nEpoch 00012: early stopping\n\n\n\nloss_lstm6432 = history_lstm6432.history['loss']\nval_loss_lstm6432 = history_lstm6432.history['val_loss']\n\nepochs = range(1, len(loss_lstm6432) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm6432, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm6432, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm6432) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.63%\n\n\n\nlstm6432 = ((min(val_loss_lstm6432) * train_mw_std)/plant_capacity)  * 100\n\n\n\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\nfilepath_lstm3264 =\"clark_best_lstm3264.hdf5\" \ncp_lstm3264 = ModelCheckpoint(filepath_lstm3264, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm3264 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm3264 = [cp_lstm3264, es_lstm3264]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm3264 = Sequential()\nmodel_lstm3264.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm3264.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm3264.add(layers.Dense(64, activation='relu'))\nmodel_lstm3264.add(layers.Dense(1))\nmodel_lstm3264.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm3264 = model_lstm3264.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm3264, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 86ms/step - loss: 0.1717\n\nEpoch 00001: val_loss improved from inf to 0.17167, saving model to clark_best_lstm3264.hdf5\n - 50s - loss: 0.3827 - val_loss: 0.1717\nEpoch 2/50\n20/20 [==============================] - 1s 71ms/step - loss: 0.1447\n\nEpoch 00002: val_loss improved from 0.17167 to 0.14468, saving model to clark_best_lstm3264.hdf5\n - 38s - loss: 0.2317 - val_loss: 0.1447\nEpoch 3/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1177\n\nEpoch 00003: val_loss improved from 0.14468 to 0.11771, saving model to clark_best_lstm3264.hdf5\n - 39s - loss: 0.1797 - val_loss: 0.1177\nEpoch 4/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1282\n\nEpoch 00004: val_loss did not improve from 0.11771\n - 37s - loss: 0.1620 - val_loss: 0.1282\nEpoch 5/50\n20/20 [==============================] - 2s 90ms/step - loss: 0.1210\n\nEpoch 00005: val_loss did not improve from 0.11771\n - 42s - loss: 0.1546 - val_loss: 0.1210\nEpoch 6/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1249\n\nEpoch 00006: val_loss did not improve from 0.11771\n - 49s - loss: 0.1525 - val_loss: 0.1249\nEpoch 7/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1245\n\nEpoch 00007: val_loss did not improve from 0.11771\n - 36s - loss: 0.1434 - val_loss: 0.1245\nEpoch 8/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1350\n\nEpoch 00008: val_loss did not improve from 0.11771\n - 37s - loss: 0.1449 - val_loss: 0.1350\nEpoch 9/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1203\n\nEpoch 00009: val_loss did not improve from 0.11771\n - 37s - loss: 0.1356 - val_loss: 0.1203\nEpoch 10/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1250\n\nEpoch 00010: val_loss did not improve from 0.11771\n - 38s - loss: 0.1336 - val_loss: 0.1250\nEpoch 11/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1447\n\nEpoch 00011: val_loss did not improve from 0.11771\n - 38s - loss: 0.1332 - val_loss: 0.1447\nEpoch 12/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1234\n\nEpoch 00012: val_loss did not improve from 0.11771\n - 37s - loss: 0.1302 - val_loss: 0.1234\nEpoch 13/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1325\n\nEpoch 00013: val_loss did not improve from 0.11771\n - 38s - loss: 0.1275 - val_loss: 0.1325\nEpoch 00013: early stopping\n\n\n\nloss_lstm3264 = history_lstm3264.history['loss']\nval_loss_lstm3264 = history_lstm3264.history['val_loss']\n\nepochs = range(1, len(loss_lstm3264) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm3264, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm3264, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm3264) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.58%\n\n\n\nlstm3264 = ((min(val_loss_lstm3264) * train_mw_std)/plant_capacity)  * 100\n\n\n\n3. Results and Discussions\n\n\n\n3.1 Results\n\n\nnames = ['Industry Agnostic Naive Method',\n         'Industry Standard Naive Method', \n         'LSTM with 64 nodes',\n         'LSTM with 32 nodes',\n         'LSTM+LSTM with 64 nodes and 32 nodes',\n         'LSTM+LSTM with 32 nodes and 64 nodes']\nresults = [benchmark1, \n           benchmark2,\n           lstm64,\n           lstm32,\n           lstm6432,\n           lstm3264]\n\n\nresults = pd.DataFrame(zip(names,results), columns=['Model', 'Error in Percentage'])\n\n\nresults['Error in Percentage'] = round(results['Error in Percentage'],2)\n\n\nresults['Improvement'] = round(1-(results['Error in Percentage']/benchmark2),4)*100\n\n\nresults\n\n\n\n\n\n\n\n\nModel\nError in Percentage\nImprovement\n\n\n\n\n0\nIndustry Agnostic Naive Method\n5.84\n53.21\n\n\n1\nIndustry Standard Naive Method\n12.48\n0.00\n\n\n2\nLSTM with 64 nodes\n2.82\n77.40\n\n\n3\nLSTM with 32 nodes\n2.77\n77.80\n\n\n4\nLSTM+LSTM with 64 nodes and 32 nodes\n2.63\n78.93\n\n\n5\nLSTM+LSTM with 32 nodes and 64 nodes\n2.58\n79.33"
  },
  {
    "objectID": "notebooks/2023-04-24-Solar/[DL] HERE Comes the Sun-SUBIC (Indiv - Yu,J).html",
    "href": "notebooks/2023-04-24-Solar/[DL] HERE Comes the Sun-SUBIC (Indiv - Yu,J).html",
    "title": "Sandro Luis R. Silva",
    "section": "",
    "text": "Here Comes The Sun: Clark Solar Power Plant\n\n\nLT16 - Sandro Silva and Jac Lin Yu\n\n\nIndividual Project of Jac Lin Yu\n\n\n\nTable of Contents\n\n1. Problem Statement\n2. Methodology\n\n2.1. Loading Prerequisites\n2.2. Dataset and Preprocessing\n2.3. Train, Validation, Test Split and Batch Generation\n2.4. Determining the Benchmark\n2.5. Model Training and Selection\n\n2.5.1 LSTM with 64 nodes\n2.5.2 LSTM with 32 nodes\n2.5.3 Stacked LSTM with 64 and 32 nodes\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\n3. Results and Discussions * 3.1. Results * 3.2. Discussions and Recommendations\n4. Summary and Main Learning Points * 4.1. Summary * 4.1. Main Learning Points \n\n\n1. Problem Statement\n\n\nWith the threat of climate change, governments and industries needed to make adjustments as a response to the changes in the priorities of the people. Being one of the biggest contributors to greenhouse gases, the energy industry has shifted its focus away from traditional fuel sources, such as oil and coal, to more alternative and sustainable solutions, such as hydroelectric, solar, and wind. This movement, however, poses new threats and challenges to existing power grids across the world due to the inherent variability and intermittency of renewable fuel source. The disruptions in the power grids and the possibility of an imbalanced load threaten to overwhelm the grid operators.  Renewable energy forecasting, in particular for solar generation supply as this is the most volatile renewable power source, may provide grid operators the ability to predict and balance energy generation and consumption. In addition, power grid operators will be able to balance and schedule the distribution of generated power for not only renewable power plants but also conventional (and rigid) power plants, such as coal and natural gas. This is currently important to the Philippines, as the Department of Energy has given priority to solar power plants. The government has promised to buy all power these plants produce at a fixed rate. This encourages production which increases supply, but demand remains constant. Therefore, conventional plants need to adjust their production or incur loses and cause instability to the grid.\n\n\n\n2. Methodology\n\n\n\n2.1 Loading Prerequisites\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n\n2.2 Dataset and Preprocessing\n\n\n Here, we will load the already cleaned preprocessed dataset of solar generation per plant per hour. The cleaning was processed outside of this notebook, and followed a framework which follows the industry standard of imputing missing values.\nTo list a few processes: - using the average between the before(t-1) and after hour(t+1) - using a 1-week average of all similar intervals (ex, if empty during hour 6 of dec 20, take the moving average of all hour 6 from dec 13-dec19) - using a 4-week moving average of that specific hour and day of the week (ex, if empty during hour 6 of dec 20, we take average of hour 6(dec 13, 6, 29, 22) \n\n\ndf = pd.read_excel('solar_dataset.xlsx', \n                   sheet_name='subic', \n                   parse_dates=['DATETIME'])\n\n\n Listed below are the columns and the corresponding datatype. \n\n\ndf.dtypes\n\nDATETIME         datetime64[ns]\nYEAR                      int64\nMONTH                     int64\nDAY                       int64\nHOUR                      int64\nRESOURCE_ID              object\nMW                      float64\nLocation                 object\nTemperature             float64\nDew Point               float64\nHumidity                float64\nWind Speed                int64\nWind Gust                 int64\nPressure                float64\nCondition                object\nLocation_Name            object\ndtype: object\n\n\n\n Drop some columns such as location, resource ID, and year. The information regarding the specific plant is unimportant since the scope has already been specified, therefore, there is only one value for each of the specified column. The year is not important since weather has no seasonality in terms of year. \n\n\ndf.drop(['Location_Name', 'Location', 'YEAR', 'RESOURCE_ID'],axis=1,inplace=True)\n\n\n It is customary in traditional time-series regression to convert the month, day, hour into categorical data, which will be later used in one-hot encoding. In addition, this is the usual preprocessing used in the energy industry here in the Philippines. \n\n\ndf.MONTH = df.MONTH.astype('str')\ndf.DAY = df.DAY.astype('str')\ndf.HOUR = df.HOUR.astype('str')\n\n\n Let us append the main df with the one-hot encoded dataframe. These columns need to be further processed because keeping them as they are will give the model the wrong idea that months, day, hour, and conditions that have higher values are more significant. \n\n\ndf0 = df.join(pd.get_dummies(df[['MONTH','DAY','HOUR','Condition']]))\n\n\n Further dropping of columns as well as reordering of columns \n\n\ndf0 = df0.drop(['DATETIME','MONTH','DAY','HOUR','Condition'],\n               axis=1)\n\n\ndf0 = df0[['Temperature','MW'] + df0.columns[2:].tolist()].copy()\n\n\ndf0 = df0.astype('float')\n\n\n Final DataFrame to be used \n\n\ndf0.head()\n\n\n\n\n\n\n\n\nTemperature\nMW\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nMONTH_1\nMONTH_10\nMONTH_11\n...\nCondition_Partly Cloudy\nCondition_Partly Cloudy / Windy\nCondition_Rain\nCondition_Rain / Windy\nCondition_Rain Shower\nCondition_Showers in the Vicinity\nCondition_T-Storm\nCondition_T-Storm / Windy\nCondition_Thunder\nCondition_Thunder in the Vicinity\n\n\n\n\n0\n72.0\n0.0\n68.0\n88.0\n8.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n73.0\n0.0\n68.0\n83.0\n10.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n75.0\n0.0\n70.0\n83.0\n9.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n75.0\n0.0\n70.0\n83.0\n0.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n73.0\n0.0\n70.0\n88.0\n12.0\n0.0\n29.6\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 103 columns\n\n\n\n\ndf0.shape\n\n(24018, 103)\n\n\n\nplant_capacity = 100\n\n\n\n 2.3 Train,Validate,Test Split and Batch Generation\n\n\n For this analysis, a .8, .1, .1, train-validate-test split was implemented. Since we are dealing with a time-series, the train-validate-test split cannot be randomly selected from the dataset, splitting of dataset should be conducted in a chronogical order. Below we define the starting and ending index for each group \n\n\ntrain_validate_test_split = [.8, .1, .1]\n\ntrain_idx = math.floor(df0.shape[0] * train_validate_test_split[0])\nval_idx = math.floor(df0.shape[0] * train_validate_test_split[1]) + train_idx\n\n\n The generator function below was utilized to yield the appropriate batch. Significant parameters include the lookback, or how far back of the data to consider, delay, or how far into the future are we forecasting, batch_size, refers to the number of training examples utilized in one iteration. \n\n\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, \n              batch_size=128, step=1):\n    \n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, \n                                     max_index, size=batch_size)\n        else:\n            if i + batch_size &gt;= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets\n\n\n Let us mean-center the dataset \n\n\nfloat_data = df0.values\ntrain_mean = float_data[:train_idx].mean(axis=0)\nfloat_data -= train_mean\ntrain_std = float_data[:train_idx].std(axis=0)\nfloat_data /= train_std\ntrain_mw_std = train_std[1]\n\n\n An industry standard is to utilize a lookback of either 144 hours (1 week), 288 hours (2 weeks), or 720 hours (1[30-day] month). The initial run was to utilized a 288 hour lookback period due to group’s industry expert’s experience. However, outside consultation with industry experts said 1 week would suffice with this limited dataset. \n\n\nlookback = 144 # lookback(consider) the previous week data\nstep = 1 # in hourly granularity\ndelay = 24 # to forecast the next 24 hours\nbatch_size = 128\n\n\ntrain_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=train_idx,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size)\n\nval_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=train_idx+1,\n    max_index=val_idx,\n    step=step,\n    batch_size=batch_size)\n\ntest_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=val_idx,\n    max_index=None,\n    step=step,\n    batch_size=batch_size)\n\nval_steps = (val_idx - (train_idx+1) - lookback)\ntest_steps = (len(float_data) - (val_idx+1) - lookback)\n\n\n\n2.4 Benchmark\n\n\n Two metrics were used to establish a baseline error:\n(1) the naive method identified in a previous lecture, which we scaled to industry standard\n(2) a different framework suggested by industry\n\n\n\ndef evaluate_naive_method():\n\n    batch_maes = []\n    for step in range(val_steps):\n        if step % 1000 == 0:\n            print(step)\n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    \n    return np.mean(batch_maes)\n\nnaive_method = evaluate_naive_method()\n\n0\n1000\n2000\n\n\n\nprint('NAIVE METHOD 1:', f'{((naive_method * train_mw_std)/22):0.2%}') #where 22 is the capacity of the plant\n\nNAIVE METHOD 1: 8.87%\n\n\n\nbenchmark1 = ((naive_method * train_mw_std)/22) * 100\n\n\n For establishing the 2nd baseline error, the team consulted the industry to determine a baseline error. However, even the industry is still conflicted on how to establish a baseline measurement. As of now, the baseline is still being established by policy makers and grid operators. \nLuckily, consulting with industry experts gave the team a framework on how to establish a baseline error per plant. The resulting measure looked at the mean squared error of the day-ahead projection(DAP), also called scheduled (Ex-Ante or RTD), and the actual delivered (Ex-Post or RTX).\nThis was then scaled as a percentage to the capacity of the plant. This was with processed with an industry expert and not included in the notebook. The yielding error resulted with a baseline of 3.64% \n\n\nbenchmark2 = 3.64\n\n\n\n2.5 Model Training Selection\n\n\n Four models were trained:\n- LSTM with 64 nodes\n- LSTM with 32 nodes\n- Stacked LSTM(2 layer) with 64 and 32 nodes\n- Stacked LSTM(2 layer) with 32 and 64 nodes\nDropout of 0.2 and Recurrent Dropout of 0.2 and a RMSProp optimized was implemented for all three.\nThe group decided to use a Epoch=50, with an EarlyStopping of patience=10 and min_delta=0.01 to avoid overfitting and to reduce runtime due to the limited time constraints. \n\n\nsteps_per_epoch_cnt = math.floor(train_idx/batch_size)\nval_steps_cnt = round(math.floor((val_idx-train_idx)/batch_size),-1)\n\n\n\n2.5.1 LSTM with 64\n\n\nfilepath_lstm64=\"clark_best_lstm64.hdf5\" \ncp_lstm64 = ModelCheckpoint(filepath_lstm64, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm64 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm64 = [cp_lstm64, es_lstm64]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm64 = Sequential()\nmodel_lstm64.add(layers.LSTM(64, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm64.add(layers.Dense(64,activation='relu'))\nmodel_lstm64.add(layers.Dense(1))\nmodel_lstm64.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm64 = model_lstm64.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm64, verbose=2) \n\nWARNING:tensorflow:From /home/jaclin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/jaclin/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/jaclin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.2421\n\nEpoch 00001: val_loss improved from inf to 0.24213, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.4244 - val_loss: 0.2421\nEpoch 2/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.2260\n\nEpoch 00002: val_loss improved from 0.24213 to 0.22596, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.2940 - val_loss: 0.2260\nEpoch 3/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2035\n\nEpoch 00003: val_loss improved from 0.22596 to 0.20355, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.2379 - val_loss: 0.2035\nEpoch 4/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.2291\n\nEpoch 00004: val_loss did not improve from 0.20355\n - 21s - loss: 0.2035 - val_loss: 0.2291\nEpoch 5/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.2306\n\nEpoch 00005: val_loss did not improve from 0.20355\n - 20s - loss: 0.1880 - val_loss: 0.2306\nEpoch 6/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2080\n\nEpoch 00006: val_loss did not improve from 0.20355\n - 21s - loss: 0.1816 - val_loss: 0.2080\nEpoch 7/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.2186\n\nEpoch 00007: val_loss did not improve from 0.20355\n - 21s - loss: 0.1706 - val_loss: 0.2186\nEpoch 8/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2124\n\nEpoch 00008: val_loss did not improve from 0.20355\n - 21s - loss: 0.1624 - val_loss: 0.2124\nEpoch 9/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.2054\n\nEpoch 00009: val_loss did not improve from 0.20355\n - 21s - loss: 0.1548 - val_loss: 0.2054\nEpoch 10/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.2036\n\nEpoch 00010: val_loss did not improve from 0.20355\n - 20s - loss: 0.1517 - val_loss: 0.2036\nEpoch 11/50\n20/20 [==============================] - 1s 40ms/step - loss: 0.2477\n\nEpoch 00011: val_loss did not improve from 0.20355\n - 20s - loss: 0.1469 - val_loss: 0.2477\nEpoch 12/50\n20/20 [==============================] - 1s 45ms/step - loss: 0.1994\n\nEpoch 00012: val_loss improved from 0.20355 to 0.19939, saving model to clark_best_lstm64.hdf5\n - 22s - loss: 0.1445 - val_loss: 0.1994\nEpoch 13/50\n20/20 [==============================] - 1s 44ms/step - loss: 0.2224\n\nEpoch 00013: val_loss did not improve from 0.19939\n - 21s - loss: 0.1363 - val_loss: 0.2224\nEpoch 00013: early stopping\n\n\n\nloss_lstm64 = history_lstm64.history['loss']\nval_loss_lstm64 = history_lstm64.history['val_loss']\n\nepochs = range(1, len(loss_lstm64) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm64, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm64, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm64) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 1.44%\n\n\n\nlstm64 = ((min(val_loss_lstm64) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.2 LSTM with 32 nodes\n\n\nfilepath_lstm32=\"clark_best_lstm32.hdf5\" \ncp_lstm32 = ModelCheckpoint(filepath_lstm32, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm32 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm32 = [cp_lstm32, es_lstm32]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm32 = Sequential()\nmodel_lstm32.add(layers.LSTM(32, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm32.add(layers.Dense(32, activation='relu'))\nmodel_lstm32.add(layers.Dense(1))\nmodel_lstm32.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm32 = model_lstm32.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm32, verbose=2) \n\nEpoch 1/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.2359\n\nEpoch 00001: val_loss improved from inf to 0.23593, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.4811 - val_loss: 0.2359\nEpoch 2/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2314\n\nEpoch 00002: val_loss improved from 0.23593 to 0.23142, saving model to clark_best_lstm32.hdf5\n - 15s - loss: 0.3305 - val_loss: 0.2314\nEpoch 3/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.2202\n\nEpoch 00003: val_loss improved from 0.23142 to 0.22023, saving model to clark_best_lstm32.hdf5\n - 15s - loss: 0.2887 - val_loss: 0.2202\nEpoch 4/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2305\n\nEpoch 00004: val_loss did not improve from 0.22023\n - 15s - loss: 0.2572 - val_loss: 0.2305\nEpoch 5/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2144\n\nEpoch 00005: val_loss improved from 0.22023 to 0.21437, saving model to clark_best_lstm32.hdf5\n - 15s - loss: 0.2366 - val_loss: 0.2144\nEpoch 6/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2146\n\nEpoch 00006: val_loss did not improve from 0.21437\n - 15s - loss: 0.2151 - val_loss: 0.2146\nEpoch 7/50\n20/20 [==============================] - 1s 29ms/step - loss: 0.1948\n\nEpoch 00007: val_loss improved from 0.21437 to 0.19485, saving model to clark_best_lstm32.hdf5\n - 15s - loss: 0.2005 - val_loss: 0.1948\nEpoch 8/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2154\n\nEpoch 00008: val_loss did not improve from 0.19485\n - 16s - loss: 0.1869 - val_loss: 0.2154\nEpoch 9/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.2095\n\nEpoch 00009: val_loss did not improve from 0.19485\n - 15s - loss: 0.1743 - val_loss: 0.2095\nEpoch 10/50\n20/20 [==============================] - 1s 36ms/step - loss: 0.2087\n\nEpoch 00010: val_loss did not improve from 0.19485\n - 17s - loss: 0.1674 - val_loss: 0.2087\nEpoch 11/50\n20/20 [==============================] - 1s 54ms/step - loss: 0.2268\n\nEpoch 00011: val_loss did not improve from 0.19485\n - 22s - loss: 0.1650 - val_loss: 0.2268\nEpoch 12/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1882\n\nEpoch 00012: val_loss improved from 0.19485 to 0.18820, saving model to clark_best_lstm32.hdf5\n - 22s - loss: 0.1633 - val_loss: 0.1882\nEpoch 13/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.2057\n\nEpoch 00013: val_loss did not improve from 0.18820\n - 19s - loss: 0.1543 - val_loss: 0.2057\nEpoch 14/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.2006\n\nEpoch 00014: val_loss did not improve from 0.18820\n - 19s - loss: 0.1561 - val_loss: 0.2006\nEpoch 15/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.2172\n\nEpoch 00015: val_loss did not improve from 0.18820\n - 19s - loss: 0.1506 - val_loss: 0.2172\nEpoch 16/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1941\n\nEpoch 00016: val_loss did not improve from 0.18820\n - 19s - loss: 0.1504 - val_loss: 0.1941\nEpoch 17/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.2072\n\nEpoch 00017: val_loss did not improve from 0.18820\n - 18s - loss: 0.1448 - val_loss: 0.2072\nEpoch 00017: early stopping\n\n\n\nloss_lstm32 = history_lstm32.history['loss']\nval_loss_lstm32 = history_lstm32.history['val_loss']\n\nepochs = range(1, len(loss_lstm32) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm32, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm32, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm32) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 1.36%\n\n\n\nlstm32 = ((min(val_loss_lstm32) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.3 Stacked LSTM with 64 and 32 nodes\n\n\nfilepath_lstm6432=\"clark_best_lstm6432.hdf5\" \ncp_lstm6432 = ModelCheckpoint(filepath_lstm6432, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm6432 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm6432 = [cp_lstm6432, es_lstm6432]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm6432 = Sequential()\nmodel_lstm6432.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm6432.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm6432.add(layers.Dense(32, activation='relu'))\nmodel_lstm6432.add(layers.Dense(1))\nmodel_lstm6432.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm6432 = model_lstm6432.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm6432, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 83ms/step - loss: 0.2134\n\nEpoch 00001: val_loss improved from inf to 0.21343, saving model to clark_best_lstm6432.hdf5\n - 52s - loss: 0.4166 - val_loss: 0.2134\nEpoch 2/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2394\n\nEpoch 00002: val_loss did not improve from 0.21343\n - 38s - loss: 0.2592 - val_loss: 0.2394\nEpoch 3/50\n20/20 [==============================] - 1s 73ms/step - loss: 0.2154\n\nEpoch 00003: val_loss did not improve from 0.21343\n - 38s - loss: 0.2145 - val_loss: 0.2154\nEpoch 4/50\n20/20 [==============================] - 1s 63ms/step - loss: 0.2356\n\nEpoch 00004: val_loss did not improve from 0.21343\n - 37s - loss: 0.1888 - val_loss: 0.2356\nEpoch 5/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2106\n\nEpoch 00005: val_loss improved from 0.21343 to 0.21064, saving model to clark_best_lstm6432.hdf5\n - 37s - loss: 0.1734 - val_loss: 0.2106\nEpoch 6/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2121\n\nEpoch 00006: val_loss did not improve from 0.21064\n - 37s - loss: 0.1678 - val_loss: 0.2121\nEpoch 7/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1950\n\nEpoch 00007: val_loss improved from 0.21064 to 0.19502, saving model to clark_best_lstm6432.hdf5\n - 37s - loss: 0.1602 - val_loss: 0.1950\nEpoch 8/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.2112\n\nEpoch 00008: val_loss did not improve from 0.19502\n - 38s - loss: 0.1505 - val_loss: 0.2112\nEpoch 9/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2117\n\nEpoch 00009: val_loss did not improve from 0.19502\n - 38s - loss: 0.1454 - val_loss: 0.2117\nEpoch 10/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2128\n\nEpoch 00010: val_loss did not improve from 0.19502\n - 37s - loss: 0.1427 - val_loss: 0.2128\nEpoch 11/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2147\n\nEpoch 00011: val_loss did not improve from 0.19502\n - 39s - loss: 0.1397 - val_loss: 0.2147\nEpoch 12/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1968\n\nEpoch 00012: val_loss did not improve from 0.19502\n - 38s - loss: 0.1353 - val_loss: 0.1968\nEpoch 13/50\n20/20 [==============================] - 1s 71ms/step - loss: 0.2072\n\nEpoch 00013: val_loss did not improve from 0.19502\n - 38s - loss: 0.1296 - val_loss: 0.2072\nEpoch 14/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1979\n\nEpoch 00014: val_loss did not improve from 0.19502\n - 39s - loss: 0.1283 - val_loss: 0.1979\nEpoch 15/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2203\n\nEpoch 00015: val_loss did not improve from 0.19502\n - 39s - loss: 0.1231 - val_loss: 0.2203\nEpoch 16/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1974\n\nEpoch 00016: val_loss did not improve from 0.19502\n - 39s - loss: 0.1256 - val_loss: 0.1974\nEpoch 17/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2095\n\nEpoch 00017: val_loss did not improve from 0.19502\n - 39s - loss: 0.1221 - val_loss: 0.2095\nEpoch 00017: early stopping\n\n\n\nloss_lstm6432 = history_lstm6432.history['loss']\nval_loss_lstm6432 = history_lstm6432.history['val_loss']\n\nepochs = range(1, len(loss_lstm6432) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm6432, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm6432, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm6432) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 1.41%\n\n\n\nlstm6432 = ((min(val_loss_lstm6432) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\nfilepath_lstm3264 =\"clark_best_lstm3264.hdf5\" \ncp_lstm3264 = ModelCheckpoint(filepath_lstm3264, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm3264 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm3264 = [cp_lstm3264, es_lstm3264]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm3264 = Sequential()\nmodel_lstm3264.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm3264.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm3264.add(layers.Dense(64, activation='relu'))\nmodel_lstm3264.add(layers.Dense(1))\nmodel_lstm3264.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm3264 = model_lstm3264.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm3264, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 78ms/step - loss: 0.2272\n\nEpoch 00001: val_loss improved from inf to 0.22722, saving model to clark_best_lstm3264.hdf5\n - 41s - loss: 0.4076 - val_loss: 0.2272\nEpoch 2/50\n20/20 [==============================] - 2s 75ms/step - loss: 0.2298\n\nEpoch 00002: val_loss did not improve from 0.22722\n - 39s - loss: 0.2578 - val_loss: 0.2298\nEpoch 3/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2099\n\nEpoch 00003: val_loss improved from 0.22722 to 0.20986, saving model to clark_best_lstm3264.hdf5\n - 41s - loss: 0.2111 - val_loss: 0.2099\nEpoch 4/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2387\n\nEpoch 00004: val_loss did not improve from 0.20986\n - 37s - loss: 0.1871 - val_loss: 0.2387\nEpoch 5/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2111\n\nEpoch 00005: val_loss did not improve from 0.20986\n - 37s - loss: 0.1708 - val_loss: 0.2111\nEpoch 6/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2193\n\nEpoch 00006: val_loss did not improve from 0.20986\n - 37s - loss: 0.1655 - val_loss: 0.2193\nEpoch 7/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1926\n\nEpoch 00007: val_loss improved from 0.20986 to 0.19259, saving model to clark_best_lstm3264.hdf5\n - 38s - loss: 0.1579 - val_loss: 0.1926\nEpoch 8/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2260\n\nEpoch 00008: val_loss did not improve from 0.19259\n - 38s - loss: 0.1513 - val_loss: 0.2260\nEpoch 9/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2203\n\nEpoch 00009: val_loss did not improve from 0.19259\n - 39s - loss: 0.1417 - val_loss: 0.2203\nEpoch 10/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.2130\n\nEpoch 00010: val_loss did not improve from 0.19259\n - 37s - loss: 0.1421 - val_loss: 0.2130\nEpoch 11/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.2420\n\nEpoch 00011: val_loss did not improve from 0.19259\n - 37s - loss: 0.1376 - val_loss: 0.2420\nEpoch 12/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1928\n\nEpoch 00012: val_loss did not improve from 0.19259\n - 38s - loss: 0.1358 - val_loss: 0.1928\nEpoch 13/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2139\n\nEpoch 00013: val_loss did not improve from 0.19259\n - 38s - loss: 0.1290 - val_loss: 0.2139\nEpoch 14/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1976\n\nEpoch 00014: val_loss did not improve from 0.19259\n - 38s - loss: 0.1270 - val_loss: 0.1976\nEpoch 15/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2421\n\nEpoch 00015: val_loss did not improve from 0.19259\n - 38s - loss: 0.1227 - val_loss: 0.2421\nEpoch 16/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.2069\n\nEpoch 00016: val_loss did not improve from 0.19259\n - 38s - loss: 0.1243 - val_loss: 0.2069\nEpoch 17/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.2055\n\nEpoch 00017: val_loss did not improve from 0.19259\n - 37s - loss: 0.1201 - val_loss: 0.2055\nEpoch 00017: early stopping\n\n\n\nloss_lstm3264 = history_lstm3264.history['loss']\nval_loss_lstm3264 = history_lstm3264.history['val_loss']\n\nepochs = range(1, len(loss_lstm3264) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm3264, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm3264, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm3264) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 1.40%\n\n\n\nlstm3264 = ((min(val_loss_lstm3264) * train_mw_std)/plant_capacity)  * 100\n\n\n\n3. Results and Discussions\n\n\n\n3.1 Results\n\n\nnames = ['Industry Agnostic Naive Method',\n         'Industry Standard Naive Method', \n         'LSTM with 64 nodes',\n         'LSTM with 32 nodes',\n         'LSTM+LSTM with 64 nodes and 32 nodes',\n         'LSTM+LSTM with 32 nodes and 64 nodes']\nresults = [benchmark1, \n           benchmark2,\n           lstm64,\n           lstm32,\n           lstm6432,\n           lstm3264]\n\n\nresults = pd.DataFrame(zip(names,results), columns=['Model', 'Error in Percentage'])\n\n\nresults['Error in Percentage'] = round(results['Error in Percentage'],2)\n\n\nresults['Improvement'] = round(1-(results['Error in Percentage']/3.64),4)*100\n\n\nresults\n\n\n\n\n\n\n\n\nModel\nError in Percentage\nImprovement\n\n\n\n\n0\nIndustry Agnostic Naive Method\n8.87\n143.68\n\n\n1\nIndustry Standard Naive Method\n3.64\n0.00\n\n\n2\nLSTM with 64 nodes\n1.44\n60.44\n\n\n3\nLSTM with 32 nodes\n1.36\n62.64\n\n\n4\nLSTM+LSTM with 64 nodes and 32 nodes\n1.41\n61.26\n\n\n5\nLSTM+LSTM with 32 nodes and 64 nodes\n1.40\n61.54\n\n\n\n\n\n\n\n\n\n3.2 Discussion and Recommendation\n\nDeep learning models, particularly Long-Short Term Memory, works very well with time series data that all neural networks performed more than 60% better than current industry standards. This improvements will save companies and the national grid operators money and make our power system more efficient. Companies will pay less paid due to miscalculations of projected generation and waste from generating too much power that are not bought by the market. Grid operators will save on repairs and downtimes caused by fluctuations in the power supply.\nFurther improvements in the model can be achieved further manipulation of the architecture of the network such as trying different numbers of nodes, layers, and optimizers in the LSTM layer, adding dense layers with different nodes, or even adding a CNN layer after the LSTM layer, as some have done. Another avenue for improvement is to obtain more data that are more indicative of solar power generation such as quality of solar panels, the angle they are placed, the irridiance per hour, altitude of the area, among others.\n\n\n4. Main Learning Points\n\nIn this age of the 4th industrial revolution, technologies are making everything, even ones that require specialized technical knowledge, such as programming, available to all. Various programs enable anyone, who has a computer, internet connection, and interest, to create their own machine learning models. All these parameter tuning, model selection, and model evaluation are getting simpler and simpler to execute, but harder to explain. This is where a formal education in data science with good mathematical foundation brings an edge. Real self-respecting data scientists will be able to explain not just how to execute/program the pipline, but also how the actual algorithm works and how it was able to provide the results it did. There is a developed intuition from having understand the fundamentals. This, together with industry expertise, will determine the success of a data scientist in the field.\nIndustry expertise is still important, as there are various intricacies in an industry that are not known to outsiders. These insights might be the determining factor in the increased performance of a model, or even its acceptance."
  },
  {
    "objectID": "notebooks/2023-04-24-Solar/[DL] Here Comes the Sun - CLARK (Indiv- Silva, S).html",
    "href": "notebooks/2023-04-24-Solar/[DL] Here Comes the Sun - CLARK (Indiv- Silva, S).html",
    "title": "Sandro Luis R. Silva",
    "section": "",
    "text": "Here Comes The Sun: Clark Solar Power Plant\n\n\nLT16 - Sandro Silva and Jac Lin Yu\n\n\nIndividual Project of Sandro Silva\n\n\n\nTable of Contents\n\n1. Problem Statement\n2. Methodology\n\n2.1. Loading Prerequisites\n2.2. Dataset and Preprocessing\n2.3. Train, Validation, Test Split and Batch Generation\n2.4. Determining the Benchmark\n2.5. Model Training and Selection\n\n2.5.1 LSTM with 64 nodes\n2.5.2 LSTM with 32 nodes\n2.5.3 Stacked LSTM with 64 and 32 nodes\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\n3. Results and Discussions * 3.1. Results * 3.2. Discussions and Recommendations\n4. Summary and Main Learning Points * 4.1. Summary * 4.1. Main Learning Points \n\n\n1. Problem Statement\n\n\nIn recent years, the global energy industry has changed its focus away from traditional fuel sources, such as oil and coal, to more alternative and sustainable solutions, such as hydroelectric, solar and wind. However, the shift towards renewable energy poses new threats and challenges to existing power grids across the world. One major concern surrounding renewable energy generation is the inherent variability and intermittency of its fuel source, as this can cause disruptions in power grids. Essentially, renewable energy technologies threaten to overwhelm the grid operators.  Renewable energy forecasting, in particular solar generation supply, may provide power grid operators the ability to predict and balance energy generation and consumption. In addition, power grid operators will be able to balance and schedule the distribution of generated power for not only renewable power plants but also conventional (and rigid) power plants, such as coal and natural gas.\n\n\n\n2. Methodology\n\n\n\n2.1 Loading Prerequisites\n\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import RMSprop, Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n\n\n2.2 Dataset and Preprocessing\n\n\n Here, we will load the already cleaned preprocessed dataset of solar generation per plant per hour. The cleaning was processed outside of this notebook, and followed a framework which follows the industry standard of imputing missing values.\nTo list a few processes: - using the average between the before(t-1) and after hour(t+1) - using a 1-week average of all similar intervals (ex, if empty during hour 6 of dec 20, take the moving average of all hour 6 from dec 13-dec19) - using a 4-week moving average of that specific hour and day of the week (ex, if empty during hour 6 of dec 20, we take average of hour 6(dec 13, 6, 29, 22) \n\n\ndf = pd.read_excel('solar_dataset.xlsx', \n                   sheet_name='clark', \n                   parse_dates=['DATETIME'])\n\n\n Here let us observe the different datatypes, notice that some columns are objects, some of which will be dropped. \n\n\ndf.dtypes\n\nDATETIME         datetime64[ns]\nYEAR                      int64\nMONTH                     int64\nDAY                       int64\nHOUR                      int64\nRESOURCE_ID              object\nMW                      float64\nLocation                 object\nTemperature             float64\nDew Point               float64\nHumidity                float64\nWind Speed              float64\nWind Gust                 int64\nPressure                float64\nCondition                object\nLocation_Name            object\ndtype: object\n\n\n\ndf.head(3)\n\n\n\n\n\n\n\n\nDATETIME\nYEAR\nMONTH\nDAY\nHOUR\nRESOURCE_ID\nMW\nLocation\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nCondition\nLocation_Name\n\n\n\n\n0\n2016-12-26 00:00:00\n2016\n12\n26\n1\n1CLASOL_G01\n0.0\nRPLC\n75.0\n70.0\n83.0\n5.0\n0\n29.3\nLight Rain Shower\nANGELES/PAMPANGA\n\n\n1\n2016-12-26 01:00:00\n2016\n12\n26\n2\n1CLASOL_G01\n0.0\nRPLC\n77.0\n68.0\n74.0\n9.0\n0\n29.2\nCloudy\nANGELES/PAMPANGA\n\n\n2\n2016-12-26 02:00:00\n2016\n12\n26\n3\n1CLASOL_G01\n0.0\nRPLC\n77.0\n70.0\n78.0\n9.0\n0\n29.2\nLight Rain Shower\nANGELES/PAMPANGA\n\n\n\n\n\n\n\n\n It is customary in traditional time-series regression to convert the month, day, hour into categorical data, which will be later used in one-hot encoding. In addition, this is the usual preprocessing used in the energy industry here in the Philippines. \n\n\ndf.MONTH = df.MONTH.astype('str')\ndf.DAY = df.DAY.astype('str')\ndf.HOUR = df.HOUR.astype('str')\n\n\n Let us drop the unimportant columns \n\n\ndf.drop(['Location_Name', 'Location', \"YEAR\"],axis=1,inplace=True)\n\n\n Let us append the main df with the one-hot encoded dataframe \n\n\ndf = df.join(pd.get_dummies(df[['MONTH','DAY','HOUR','Condition']]))\n\n\n Let us get the unique list of power plants in the region \n\n\nlocations = df.RESOURCE_ID.unique()\n\n\n For this study, let us consider the first power plant \n\n\nlocations[0]\n\n'1CLASOL_G01'\n\n\n\ndf0 = df[df.RESOURCE_ID == locations[0]].copy()\n\n\n Further dropping of columns as well as reordering of columns \n\n\ndf0 = df0.drop(['RESOURCE_ID','DATETIME','MONTH','DAY','HOUR','Condition'],\n               axis=1)\n\n\ndf0 = df0[['Temperature','MW'] + df0.columns[2:].tolist()].copy()\n\n\ndf0 = df0.astype('float')\n\n\n Final DataFrame to be used \n\n\ndf0.head()\n\n\n\n\n\n\n\n\nTemperature\nMW\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nMONTH_1\nMONTH_10\nMONTH_11\n...\nCondition_Mostly Cloudy / Windy\nCondition_Partly Cloudy\nCondition_Partly Cloudy / Windy\nCondition_Patches of Fog\nCondition_Rain\nCondition_Rain / Windy\nCondition_Rain Shower\nCondition_Showers in the Vicinity\nCondition_T-Storm\nCondition_Thunder\n\n\n\n\n0\n75.0\n0.0\n70.0\n83.0\n5.0\n0.0\n29.3\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n77.0\n0.0\n68.0\n74.0\n9.0\n0.0\n29.2\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n77.0\n0.0\n70.0\n78.0\n9.0\n0.0\n29.2\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n79.0\n0.0\n70.0\n74.0\n12.0\n0.0\n29.2\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n79.0\n0.0\n70.0\n74.0\n12.0\n0.0\n29.2\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 101 columns\n\n\n\n\ndf0.shape\n\n(24031, 101)\n\n\n\nplant_capacity = 22\n\n\n\n 2.3 Train,Validate,Test Split and Batch Generation\n\n\n For this analysis, a .8, .1, .1, train-validate-test split was implemented. Since we are dealing with a time-series, the train-validate-test split cannot be randomly selected from the dataset, splitting of dataset should be conducted in a chronogical order. Below we define the starting and ending index for each group \n\n\ntrain_validate_test_split = [.8, .1, .1]\n\ntrain_idx = math.floor(df0.shape[0] * train_validate_test_split[0])\nval_idx = math.floor(df0.shape[0] * train_validate_test_split[1]) + train_idx\n\n\n The generator function below was utilized to yield the appropriate batch. Significant parameters include the lookback, or how far back of the data to consider, delay, or how far into the future are we forecasting, batch_size, refers to the number of training examples utilized in one iteration. \n\n\ndef generator(data, lookback, delay, min_index, max_index, shuffle=False, \n              batch_size=128, step=1):\n    \n    if max_index is None:\n        max_index = len(data) - delay - 1\n    i = min_index + lookback\n    \n    while 1:\n        if shuffle:\n            rows = np.random.randint(min_index + lookback, \n                                     max_index, size=batch_size)\n        else:\n            if i + batch_size &gt;= max_index:\n                i = min_index + lookback\n            rows = np.arange(i, min(i + batch_size, max_index))\n            i += len(rows)\n\n        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        for j, row in enumerate(rows):\n            indices = range(rows[j] - lookback, rows[j], step)\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay][1]\n        yield samples, targets\n\n\n Let us mean-center the dataset \n\n\nfloat_data = df0.values\ntrain_mean = float_data[:train_idx].mean(axis=0)\nfloat_data -= train_mean\ntrain_std = float_data[:train_idx].std(axis=0)\nfloat_data /= train_std\ntrain_mw_std = train_std[1]\n\n\n Let us initialize the parameters which will be used for the generation An industry standard practic is to utilize a lookback of either 144 hours (1 week), 288 hours (2 weeks), or 720 hours (1[30-day] month). The initial run was to utilized a 288 hour lookback period due to group’s industry expert’s experience. However, outside consultation with industry experts said 1 week would suffice with this limited dataset. \n\n\nlookback = 144 # lookback(consider) the previous week data\nstep = 1 # in hourly granularity\ndelay = 24 # to forecast the next 24 hours\nbatch_size = 128\n\n\ntrain_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=0,\n    max_index=train_idx,\n    shuffle=True,\n    step=step,\n    batch_size=batch_size)\n\nval_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=train_idx+1,\n    max_index=val_idx,\n    step=step,\n    batch_size=batch_size)\n\ntest_gen = generator(float_data,\n    lookback=lookback,\n    delay=delay,\n    min_index=val_idx,\n    max_index=None,\n    step=step,\n    batch_size=batch_size)\n\nval_steps = (val_idx - (train_idx+1) - lookback)\ntest_steps = (len(float_data) - (val_idx+1) - lookback)\n\n\n\n2.4 Benchmark\n\n\n The team used two metrics to establish a baseline error:\n(1) the naive method identified in a previous lecture, which we scaled to industry standard\n(2) a different framework suggested by industry\n\n\n\ndef evaluate_naive_method():\n\n    batch_maes = []\n    for step in range(val_steps):\n        if step % 1000 == 0:\n            print(step)\n        samples, targets = next(val_gen)\n        preds = samples[:, -1, 1]\n        mae = np.mean(np.abs(preds - targets))\n        batch_maes.append(mae)\n    \n    return np.mean(batch_maes)\n\nnaive_method = evaluate_naive_method()\n\n0\n1000\n2000\n\n\n\nprint('NAIVE METHOD 1:', f'{((naive_method * train_mw_std)/22):0.2%}') #where 22 is the capacity of the plant\n\nNAIVE METHOD 1: 6.58%\n\n\n\nbenchmark1 = ((naive_method * train_mw_std)/22) * 100\n\n\n For establishing the 2nd baseline error, the team consulted the industry to determine a baseline error. However, even the industry is still conflicted on how to establish a baseline measurement. As of now, the baseline is still being established by policy makers and grid operators. \nLuckily, consulting with industry experts gave the team a framework on how to establish a baseline error per plant. The resulting measure looked at the mean squared error of the day-ahead projection(DAP), also called scheduled (Ex-Ante or RTD), and the actual delivered (Ex-Post or RTX).\nThis was then scaled as a percentage to the capacity of the plant. This was with processed with an industry expert and not included in the notebook. The yielding error resulted with a baseline of 12.01% \n\n\nbenchmark2 = 12.01\n\n\n\n2.5 Model Training Selection\n\n\n The team utilized 4 models:\n- LSTM with 64 nodes\n- LSTM with 32 nodes\n- Stacked LSTM(2 layer) with 64 and 32 nodes\n- Stacked LSTM(2 layer) with 32 and 64 nodes\nDropout of 0.2 and Recurrent Dropout of 0.2 and a RMSProp optimized was implemented for all three.\nThe group decided to use a Epoch=50, with an EarlyStopping of patience=10 and min_delta=0.01 to avoid overfitting and to reduce runtime due to the limited time constraints. Lastly, the ModelCheckpoint was utilized to extract the best model. \n\n\nsteps_per_epoch_cnt = math.floor(train_idx/batch_size)\nval_steps_cnt = round(math.floor((val_idx-train_idx)/batch_size),-1)\n\n\n\n2.5.1 LSTM with 64\n\n\nfilepath_lstm64=\"clark_best_lstm64.hdf5\" \ncp_lstm64 = ModelCheckpoint(filepath_lstm64, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm64 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm64 = [cp_lstm64, es_lstm64]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm64 = Sequential()\nmodel_lstm64.add(layers.LSTM(64, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm64.add(layers.Dense(64,activation='relu'))\nmodel_lstm64.add(layers.Dense(1))\nmodel_lstm64.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm64 = model_lstm64.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm64, verbose=2) \n\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /home/ssilva/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n20/20 [==============================] - 1s 47ms/step - loss: 0.1993\n\nEpoch 00001: val_loss improved from inf to 0.19933, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.4025 - val_loss: 0.1993\nEpoch 2/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1724\n\nEpoch 00002: val_loss improved from 0.19933 to 0.17236, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.2765 - val_loss: 0.1724\nEpoch 3/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1564\n\nEpoch 00003: val_loss improved from 0.17236 to 0.15639, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.2005 - val_loss: 0.1564\nEpoch 4/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1634\n\nEpoch 00004: val_loss did not improve from 0.15639\n - 19s - loss: 0.1664 - val_loss: 0.1634\nEpoch 5/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1585\n\nEpoch 00005: val_loss did not improve from 0.15639\n - 19s - loss: 0.1598 - val_loss: 0.1585\nEpoch 6/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1697\n\nEpoch 00006: val_loss did not improve from 0.15639\n - 19s - loss: 0.1500 - val_loss: 0.1697\nEpoch 7/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1465\n\nEpoch 00007: val_loss improved from 0.15639 to 0.14650, saving model to clark_best_lstm64.hdf5\n - 19s - loss: 0.1445 - val_loss: 0.1465\nEpoch 8/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1521\n\nEpoch 00008: val_loss did not improve from 0.14650\n - 19s - loss: 0.1431 - val_loss: 0.1521\nEpoch 9/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1565\n\nEpoch 00009: val_loss did not improve from 0.14650\n - 19s - loss: 0.1339 - val_loss: 0.1565\nEpoch 10/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1584\n\nEpoch 00010: val_loss did not improve from 0.14650\n - 19s - loss: 0.1301 - val_loss: 0.1584\nEpoch 11/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1515\n\nEpoch 00011: val_loss did not improve from 0.14650\n - 19s - loss: 0.1297 - val_loss: 0.1515\nEpoch 12/50\n20/20 [==============================] - 1s 43ms/step - loss: 0.1624\n\nEpoch 00012: val_loss did not improve from 0.14650\n - 19s - loss: 0.1285 - val_loss: 0.1624\nEpoch 13/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1430\n\nEpoch 00013: val_loss improved from 0.14650 to 0.14299, saving model to clark_best_lstm64.hdf5\n - 21s - loss: 0.1255 - val_loss: 0.1430\nEpoch 14/50\n20/20 [==============================] - 1s 53ms/step - loss: 0.1586\n\nEpoch 00014: val_loss did not improve from 0.14299\n - 23s - loss: 0.1221 - val_loss: 0.1586\nEpoch 15/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1457\n\nEpoch 00015: val_loss did not improve from 0.14299\n - 19s - loss: 0.1162 - val_loss: 0.1457\nEpoch 16/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1452\n\nEpoch 00016: val_loss did not improve from 0.14299\n - 18s - loss: 0.1186 - val_loss: 0.1452\nEpoch 17/50\n20/20 [==============================] - 1s 39ms/step - loss: 0.1591\n\nEpoch 00017: val_loss did not improve from 0.14299\n - 19s - loss: 0.1152 - val_loss: 0.1591\nEpoch 18/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1563\n\nEpoch 00018: val_loss did not improve from 0.14299\n - 19s - loss: 0.1138 - val_loss: 0.1563\nEpoch 19/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1538\n\nEpoch 00019: val_loss did not improve from 0.14299\n - 18s - loss: 0.1129 - val_loss: 0.1538\nEpoch 20/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1551\n\nEpoch 00020: val_loss did not improve from 0.14299\n - 18s - loss: 0.1128 - val_loss: 0.1551\nEpoch 21/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1505\n\nEpoch 00021: val_loss did not improve from 0.14299\n - 19s - loss: 0.1118 - val_loss: 0.1505\nEpoch 22/50\n20/20 [==============================] - 1s 42ms/step - loss: 0.1461\n\nEpoch 00022: val_loss did not improve from 0.14299\n - 19s - loss: 0.1109 - val_loss: 0.1461\nEpoch 23/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.1543\n\nEpoch 00023: val_loss did not improve from 0.14299\n - 23s - loss: 0.1093 - val_loss: 0.1543\nEpoch 00023: early stopping\n\n\n\nloss_lstm64 = history_lstm64.history['loss']\nval_loss_lstm64 = history_lstm64.history['val_loss']\n\nepochs = range(1, len(loss_lstm64) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm64, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm64, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm64) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 3.21%\n\n\n\nlstm64 = ((min(val_loss_lstm64) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.2 LSTM with 32 nodes\n\n\nfilepath_lstm32=\"clark_best_lstm32.hdf5\" \ncp_lstm32 = ModelCheckpoint(filepath_lstm32, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm32 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm32 = [cp_lstm32, es_lstm32]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm32 = Sequential()\nmodel_lstm32.add(layers.LSTM(32, \n                       dropout=0.2,\n                       recurrent_dropout=0.2, \n                       input_shape=(None, float_data.shape[-1])))\nmodel_lstm32.add(layers.Dense(32, activation='relu'))\nmodel_lstm32.add(layers.Dense(1))\nmodel_lstm32.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm32 = model_lstm32.fit_generator(train_gen,\n                                        steps_per_epoch=steps_per_epoch_cnt, \n                                        epochs=50,\n                                        validation_data=val_gen, \n                                        validation_steps=val_steps_cnt,\n                                        callbacks=callbacks_list_lstm32, verbose=2) \n\nEpoch 1/50\n20/20 [==============================] - 1s 41ms/step - loss: 0.1916\n\nEpoch 00001: val_loss improved from inf to 0.19160, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.4936 - val_loss: 0.1916\nEpoch 2/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1696\n\nEpoch 00002: val_loss improved from 0.19160 to 0.16960, saving model to clark_best_lstm32.hdf5\n - 19s - loss: 0.3050 - val_loss: 0.1696\nEpoch 3/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1460\n\nEpoch 00003: val_loss improved from 0.16960 to 0.14600, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.2672 - val_loss: 0.1460\nEpoch 4/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1610\n\nEpoch 00004: val_loss did not improve from 0.14600\n - 17s - loss: 0.2259 - val_loss: 0.1610\nEpoch 5/50\n20/20 [==============================] - 1s 35ms/step - loss: 0.1525\n\nEpoch 00005: val_loss did not improve from 0.14600\n - 17s - loss: 0.2027 - val_loss: 0.1525\nEpoch 6/50\n20/20 [==============================] - 1s 36ms/step - loss: 0.1681\n\nEpoch 00006: val_loss did not improve from 0.14600\n - 16s - loss: 0.1687 - val_loss: 0.1681\nEpoch 7/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.1471\n\nEpoch 00007: val_loss did not improve from 0.14600\n - 17s - loss: 0.1564 - val_loss: 0.1471\nEpoch 8/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1418\n\nEpoch 00008: val_loss improved from 0.14600 to 0.14184, saving model to clark_best_lstm32.hdf5\n - 16s - loss: 0.1522 - val_loss: 0.1418\nEpoch 9/50\n20/20 [==============================] - 1s 31ms/step - loss: 0.1502\n\nEpoch 00009: val_loss did not improve from 0.14184\n - 17s - loss: 0.1418 - val_loss: 0.1502\nEpoch 10/50\n20/20 [==============================] - 1s 38ms/step - loss: 0.1595\n\nEpoch 00010: val_loss did not improve from 0.14184\n - 19s - loss: 0.1390 - val_loss: 0.1595\nEpoch 11/50\n20/20 [==============================] - 1s 30ms/step - loss: 0.1491\n\nEpoch 00011: val_loss did not improve from 0.14184\n - 17s - loss: 0.1370 - val_loss: 0.1491\nEpoch 12/50\n20/20 [==============================] - 1s 34ms/step - loss: 0.1505\n\nEpoch 00012: val_loss did not improve from 0.14184\n - 17s - loss: 0.1363 - val_loss: 0.1505\nEpoch 13/50\n20/20 [==============================] - 1s 33ms/step - loss: 0.1394\n\nEpoch 00013: val_loss improved from 0.14184 to 0.13940, saving model to clark_best_lstm32.hdf5\n - 17s - loss: 0.1334 - val_loss: 0.1394\nEpoch 00013: early stopping\n\n\n\nloss_lstm32 = history_lstm32.history['loss']\nval_loss_lstm32 = history_lstm32.history['val_loss']\n\nepochs = range(1, len(loss_lstm32) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm32, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm32, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm32) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 3.13%\n\n\n\nlstm32 = ((min(val_loss_lstm32) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.3 Stacked LSTM with 64 and 32 nodes\n\n\nfilepath_lstm6432=\"clark_best_lstm6432.hdf5\" \ncp_lstm6432 = ModelCheckpoint(filepath_lstm6432, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm6432 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm6432 = [cp_lstm6432, es_lstm6432]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm6432 = Sequential()\nmodel_lstm6432.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm6432.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm6432.add(layers.Dense(32, activation='relu'))\nmodel_lstm6432.add(layers.Dense(1))\nmodel_lstm6432.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm6432 = model_lstm6432.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm6432, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 77ms/step - loss: 0.1807\n\nEpoch 00001: val_loss improved from inf to 0.18073, saving model to clark_best_lstm6432.hdf5\n - 35s - loss: 0.4037 - val_loss: 0.1807\nEpoch 2/50\n20/20 [==============================] - 2s 80ms/step - loss: 0.1452\n\nEpoch 00002: val_loss improved from 0.18073 to 0.14524, saving model to clark_best_lstm6432.hdf5\n - 34s - loss: 0.2290 - val_loss: 0.1452\nEpoch 3/50\n20/20 [==============================] - 1s 60ms/step - loss: 0.1595\n\nEpoch 00003: val_loss did not improve from 0.14524\n - 41s - loss: 0.1754 - val_loss: 0.1595\nEpoch 4/50\n20/20 [==============================] - 1s 60ms/step - loss: 0.1460\n\nEpoch 00004: val_loss did not improve from 0.14524\n - 34s - loss: 0.1528 - val_loss: 0.1460\nEpoch 5/50\n20/20 [==============================] - 1s 70ms/step - loss: 0.1535\n\nEpoch 00005: val_loss did not improve from 0.14524\n - 37s - loss: 0.1483 - val_loss: 0.1535\nEpoch 6/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1510\n\nEpoch 00006: val_loss did not improve from 0.14524\n - 39s - loss: 0.1385 - val_loss: 0.1510\nEpoch 7/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1482\n\nEpoch 00007: val_loss did not improve from 0.14524\n - 35s - loss: 0.1345 - val_loss: 0.1482\nEpoch 8/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1382\n\nEpoch 00008: val_loss improved from 0.14524 to 0.13823, saving model to clark_best_lstm6432.hdf5\n - 34s - loss: 0.1336 - val_loss: 0.1382\nEpoch 9/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1606\n\nEpoch 00009: val_loss did not improve from 0.13823\n - 34s - loss: 0.1249 - val_loss: 0.1606\nEpoch 10/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1529\n\nEpoch 00010: val_loss did not improve from 0.13823\n - 34s - loss: 0.1214 - val_loss: 0.1529\nEpoch 11/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1430\n\nEpoch 00011: val_loss did not improve from 0.13823\n - 34s - loss: 0.1213 - val_loss: 0.1430\nEpoch 12/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1400\n\nEpoch 00012: val_loss did not improve from 0.13823\n - 34s - loss: 0.1209 - val_loss: 0.1400\nEpoch 00012: early stopping\n\n\n\nloss_lstm6432 = history_lstm6432.history['loss']\nval_loss_lstm6432 = history_lstm6432.history['val_loss']\n\nepochs = range(1, len(loss_lstm6432) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm6432, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm6432, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm6432) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 3.10%\n\n\n\nlstm6432 = ((min(val_loss_lstm6432) * train_mw_std)/plant_capacity) * 100\n\n\n\n2.5.4 Stacked LSTM with 32 and 64 nodes\n\n\nfilepath_lstm3264 =\"clark_best_lstm3264.hdf5\" \ncp_lstm3264 = ModelCheckpoint(filepath_lstm3264, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\nes_lstm3264 = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10, min_delta=.01)\ncallbacks_list_lstm3264 = [cp_lstm3264, es_lstm3264]\n\n# fix random seed for reproducibility\nseed = 1337\nnp.random.seed(seed)\n\nmodel_lstm3264 = Sequential()\nmodel_lstm3264.add(layers.LSTM(64,\n                    dropout=0.2,\n                    recurrent_dropout=0.2,\n                    return_sequences=True,\n                    input_shape=(None, float_data.shape[-1])))\nmodel_lstm3264.add(layers.LSTM(32,\n                    dropout=0.2,\n                    recurrent_dropout=0.2))\nmodel_lstm3264.add(layers.Dense(64, activation='relu'))\nmodel_lstm3264.add(layers.Dense(1))\nmodel_lstm3264.compile(optimizer=RMSprop(), loss='mae')\n\nhistory_lstm3264 = model_lstm3264.fit_generator(train_gen,\n                                             steps_per_epoch=steps_per_epoch_cnt,\n                                             epochs=50,\n                                             validation_data=val_gen, \n                                             validation_steps=val_steps_cnt,\n                                             callbacks=callbacks_list_lstm3264, verbose=2)\n\nEpoch 1/50\n20/20 [==============================] - 2s 77ms/step - loss: 0.1807\n\nEpoch 00001: val_loss improved from inf to 0.18071, saving model to clark_best_lstm3264.hdf5\n - 36s - loss: 0.4016 - val_loss: 0.1807\nEpoch 2/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1537\n\nEpoch 00002: val_loss improved from 0.18071 to 0.15374, saving model to clark_best_lstm3264.hdf5\n - 33s - loss: 0.2138 - val_loss: 0.1537\nEpoch 3/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1639\n\nEpoch 00003: val_loss did not improve from 0.15374\n - 33s - loss: 0.1709 - val_loss: 0.1639\nEpoch 4/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1513\n\nEpoch 00004: val_loss improved from 0.15374 to 0.15125, saving model to clark_best_lstm3264.hdf5\n - 33s - loss: 0.1528 - val_loss: 0.1513\nEpoch 5/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1401\n\nEpoch 00005: val_loss improved from 0.15125 to 0.14008, saving model to clark_best_lstm3264.hdf5\n - 33s - loss: 0.1501 - val_loss: 0.1401\nEpoch 6/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1481\n\nEpoch 00006: val_loss did not improve from 0.14008\n - 33s - loss: 0.1413 - val_loss: 0.1481\nEpoch 7/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1498\n\nEpoch 00007: val_loss did not improve from 0.14008\n - 33s - loss: 0.1372 - val_loss: 0.1498\nEpoch 8/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1588\n\nEpoch 00008: val_loss did not improve from 0.14008\n - 33s - loss: 0.1362 - val_loss: 0.1588\nEpoch 9/50\n20/20 [==============================] - 1s 71ms/step - loss: 0.1470\n\nEpoch 00009: val_loss did not improve from 0.14008\n - 33s - loss: 0.1273 - val_loss: 0.1470\nEpoch 10/50\n20/20 [==============================] - 1s 64ms/step - loss: 0.1598\n\nEpoch 00010: val_loss did not improve from 0.14008\n - 36s - loss: 0.1242 - val_loss: 0.1598\nEpoch 11/50\n20/20 [==============================] - 1s 59ms/step - loss: 0.1428\n\nEpoch 00011: val_loss did not improve from 0.14008\n - 34s - loss: 0.1246 - val_loss: 0.1428\nEpoch 12/50\n20/20 [==============================] - 1s 59ms/step - loss: 0.1389\n\nEpoch 00012: val_loss improved from 0.14008 to 0.13890, saving model to clark_best_lstm3264.hdf5\n - 33s - loss: 0.1239 - val_loss: 0.1389\nEpoch 13/50\n20/20 [==============================] - 1s 66ms/step - loss: 0.1276\n\nEpoch 00013: val_loss improved from 0.13890 to 0.12760, saving model to clark_best_lstm3264.hdf5\n - 39s - loss: 0.1203 - val_loss: 0.1276\nEpoch 14/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1609\n\nEpoch 00014: val_loss did not improve from 0.12760\n - 37s - loss: 0.1178 - val_loss: 0.1609\nEpoch 15/50\n20/20 [==============================] - 1s 70ms/step - loss: 0.1446\n\nEpoch 00015: val_loss did not improve from 0.12760\n - 36s - loss: 0.1108 - val_loss: 0.1446\nEpoch 16/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1417\n\nEpoch 00016: val_loss did not improve from 0.12760\n - 36s - loss: 0.1140 - val_loss: 0.1417\nEpoch 17/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1453\n\nEpoch 00017: val_loss did not improve from 0.12760\n - 37s - loss: 0.1111 - val_loss: 0.1453\nEpoch 18/50\n20/20 [==============================] - 1s 72ms/step - loss: 0.1500\n\nEpoch 00018: val_loss did not improve from 0.12760\n - 38s - loss: 0.1105 - val_loss: 0.1500\nEpoch 19/50\n20/20 [==============================] - 1s 68ms/step - loss: 0.1578\n\nEpoch 00019: val_loss did not improve from 0.12760\n - 36s - loss: 0.1090 - val_loss: 0.1578\nEpoch 20/50\n20/20 [==============================] - 1s 65ms/step - loss: 0.1500\n\nEpoch 00020: val_loss did not improve from 0.12760\n - 37s - loss: 0.1094 - val_loss: 0.1500\nEpoch 21/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1490\n\nEpoch 00021: val_loss did not improve from 0.12760\n - 36s - loss: 0.1088 - val_loss: 0.1490\nEpoch 22/50\n20/20 [==============================] - 1s 67ms/step - loss: 0.1482\n\nEpoch 00022: val_loss did not improve from 0.12760\n - 36s - loss: 0.1063 - val_loss: 0.1482\nEpoch 23/50\n20/20 [==============================] - 1s 69ms/step - loss: 0.1551\n\nEpoch 00023: val_loss did not improve from 0.12760\n - 36s - loss: 0.1036 - val_loss: 0.1551\nEpoch 00023: early stopping\n\n\n\nloss_lstm3264 = history_lstm3264.history['loss']\nval_loss_lstm3264 = history_lstm3264.history['val_loss']\n\nepochs = range(1, len(loss_lstm3264) + 1)\n\nplt.figure()\n\nplt.plot(epochs, loss_lstm3264, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_lstm3264, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()\n\n\n\n\n\nprint('Model Error:', f'{((min(val_loss_lstm3264) * train_mw_std)/plant_capacity):0.2%}')\n\nModel Error: 2.86%\n\n\n\nlstm3264 = ((min(val_loss_lstm3264) * train_mw_std)/plant_capacity) *100\n\n\n\n3. Results and Discussions\n\n\nnames = ['Industry Agnostic Naive Method',\n         'Industry Standard Naive Method', \n         'LSTM with 64 nodes',\n         'LSTM with 32 nodes',\n         'LSTM+LSTM with 64 nodes and 32 nodes',\n         'LSTM+LSTM with 32 nodes and 64 nodes']\nresults = [benchmark1, \n           benchmark2,\n           lstm64,\n           lstm32,\n           lstm6432,\n           lstm3264]\n\n\nresults = pd.DataFrame(zip(names,results), columns=['Model', 'Error in Percentage'])\n\n\nresults['Error in Percentage'] = round(results['Error in Percentage'],2)\n\n\nresults['Improvement'] = round(1-(results['Error in Percentage']/12.01),4)*100\n\n\nresults\n\n\n\n\n\n\n\n\nModel\nError in Percentage\nImprovement\n\n\n\n\n0\nIndustry Agnostic Naive Method\n6.58\n45.21\n\n\n1\nIndustry Standard Naive Method\n12.01\n0.00\n\n\n2\nLSTM with 64 nodes\n3.21\n73.27\n\n\n3\nLSTM with 32 nodes\n3.13\n73.94\n\n\n4\nLSTM+LSTM with 64 nodes and 32 nodes\n3.10\n74.19\n\n\n5\nLSTM+LSTM with 32 nodes and 64 nodes\n2.86\n76.19\n\n\n\n\n\n\n\n\n\n3.1 Results\n\n\n Results are indicated in the table above. All four models appear to have performed better than the naive models by quite a margin, with the Two layer LSTM with 64 nodes and 32 nodes having the lowest error in terms of percentage. This translates to an improvement by 73.27% (LSTM with 64 nodes), 73.94% (LSTM with 32 nodes), 74.19% (LSTM+LSTM with 64 nodes and 32 nodes) and 76.19% (LSTM+LSTM with 32 nodes and 64 nodes). \n\n\n\n3.2 Discussions and Recommendations\n\n\n\n\nAdding more epochs and allowing the model train even longer would have some benefits. Results could have improved on all four models if the patience and the min_delta were relaxed.\nMy recommendation is to include more technical data in the dataset. Based on industry knowledge, irridiance is one of the main drivers of forecasting solar generation. I would expect that including this as a feature would drastically improve the already impressive model results. Similarly, adding power grid data such as power line capacity and degradation, solar panel angle, may improve in each models accuracy.\n\n\nMoving forward, I would want to explore different types of architectures. Similar journal articles suggested using an LSTM-CNN and/or a CNN-LSTM architectures. These two types of models capture not only the temporal data (LSTM) but also the spatial element (CNN). In addition, using a CNN to include images of the panels may improve the models accuracy. as this may capture the overall health of the panel.\n\n\n\n\n\n4. Summary and Main Learning Points\n\n\n\n4.1 Summary\n\n\n The main objective of the study was to apply deep learning models to forecast solar power supply generation. The data collected was from two online sources, the Wholesale Electrcity Spot Market and Weather Underground. Actual plant generation was obtained from the Wholesale Electricity Spot Market and 13 locational temperature data (Clark International Airport) was collected from Weather Underground. Next, domain knowledge was utilized in the preprocessing data to impute missing data. Here, two forms of benchmarks were idenfied, one was an industry agnostic standard while another error was sourced through consultation of industry experts. Afterwhich, a total of 4 different LSTM architecures were tested and all seem to have beaten both the industry agnostic and industry standard errors. Dropouts and Recurrent Dropouts were utilized during training as fine tuning of the model. In addition, ModelCheckpoints and EarlyStopping was implemented during the training to monitor the output and reduce the runtime. For the results, the 2 layer LSTM with 32 nodes and 64 nodes had the best improvement of 76.19%, from an 12.01% error to 2.86% error. \n\n\n\n4.2 Main Learning Points\n\n\n The main role of domain knowledge and expertise can be augmented with data science and machine learning.\n\n\nThe class has taught me the fundamentals of machine learning and deep learning algorithms, such as MLP, Convolutional, to Recurrent neural networks. However, what I discovered during this process is that domain knowledge influences and augments the data science flow. Yes, we may have developed the fundamentals of understanding deep learning techniques but the true potential of an algorithm can be truly harnessed only when some form of domain knowledge is utilized.\n\n\n\n\nIndustry heuristics can improve the data science pipeline from the feature engineering, benchmarking, model selection and hyperparameter turing. That with prior knowledge on the topic at hand can improve on the accuracy of the resulting model.\n\n\n\n\nOn the contrary, having industry knowledge can allow me to think more critical with regards to other peoples models (whether in journals or at the office), both from the machine learning and technical perspective.\n\n\n\n\nLastly, one may know the inner workings of the model and know how to implement the model, however with domain knowledge, one will also know how to ask the right questions. Questions relating to (1) business value, on how to extract more value from a model, and/or (2) how can we improve or build on the model. Essentially asking what are the next steps, and how can we improve on this.\n\n\nNot really data science related but: tap the industry the topic is related about. You do not know everything in an industry so leverage on the network to figure out things\n\n\nEven if I was already in the energy industry, I was not particularly familiar with renewable energy forecasting and the standard protocols. If my groupmate and I were not familiar with the industry, it would have been very difficult for us to verify protocols. However, since I am blessed with very talented people from the industry, a quick 5 minute call was all we needed to verify certain aspect of the industry.\n\n\n\nFind a field/aspect of data science that you’re interested in, stick to it, build on it and fix it!\n\n\n\nFrom the numerous of talks I have listened in this class, it was already ingrained to me that data science is applicable to almost every industry, and as aspiring data scientists it may be difficult to hone in into a certain field or aspect. For me, I find joy when I see the application of data science to the energy sector as I have experienced and seen where the holes in the analytics are. Now, I have the fundamental skillset to cover up and fix these holes."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html",
    "href": "posts/2023-04-25-Solar/index.html",
    "title": "Here Comes The Sun",
    "section": "",
    "text": "This analysis was done together with my groupmate Jac Lin T. Yu\nOriginal creation and submission of this report was last Dec 2020."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-proposed-framework",
    "href": "posts/2023-04-25-Solar/index.html#sec-proposed-framework",
    "title": "Here Comes The Sun",
    "section": "4 Proposed Framework",
    "text": "4 Proposed Framework\n\n4.1 Long-Short Term Memory\n\n\n\nFigure 1: Architecture of an LSTM Cell.\n\n\nLong Short Term Memory, or also known as LSTM, is a deep learning method proposed in 1997 to address the vanishing or exploding gradient problem encountered in Recurrent Neural Networks when dealing with time series forecasting (Zhang, Chi, and Xiao 2018). It is a type of recurrent neural network (RNN) that can learn long-term dependencies between time steps of sequence data. It does this by integrating a memory cells, also known as gates, that allows the learning of complex and long-term temporal dynamics of the data (Qing and Niu 2018). Figure 1 shows the gates mechanism that allows the interaction of the memory cell (Rassem, El-Beltagy, and Saleh 2017).\nThese “gates” determine which unimportant data should be forgotten and which important feature should be remembered. These gates are typically composed on sigmoid neural net layers and a pointwise multiplication operation. There are three gates in a typical LSTM: forget gate, input gate, and output gate.\n\\[\n    \\vec{i}_t=\\sigma(\\vec{W}_{xi}\\vec{x}^{t}+\\vec{W}_{hi}\\vec{h}^{(t-1)}+\\vec{b}_i)\n\\tag{1}\\]\n\\[\n    \\vec{g}_t=tanh(\\vec{W}_{xg}\\vec{x}^{t}+\\vec{W}_{hg}\\vec{h}^{(t-1)}+\\vec{b}_g)\n\\tag{2}\\]\n\\[\n    \\vec{f}_t=\\sigma(\\vec{W}_{xf}\\vec{x}^{t}+\\vec{W}_{hf}\\vec{h}^{(t-1)}+\\vec{b}_f)\n\\tag{3}\\]\n\\[\n    \\vec{o}_t=\\sigma(\\vec{W}_{xo}\\vec{x}^{t}+\\vec{W}_{ho}\\vec{h}^{(t-1)}+\\vec{b}_o)\n\\tag{4}\\]\n\\[\n    \\vec{C}^t=(\\vec{C}^{(t-1)}\\odot\\vec{f}_{t})\\oplus(\\vec{i}_{t}\\odot\\vec{g}_{t})\n\\tag{5}\\]\n\\[\n    \\vec{h}_t=\\vec{o}_{t}\\odot tanh(\\vec{C}_{t})\n\\tag{6}\\]\nThe input gate (\\(i_t\\)) seen in Equation 1 and input modulation gate (\\(g_t\\)) seen in Equation 2, where \\(\\vec{W}\\) are the weights matrices and \\(\\vec{b}\\) are the bias terms, allows the updating of the cell.\nEquation 3 shows the computation of the forget gate (\\(f_t\\)) which determines if the information is suppressed or allowed to pass through and Equation 4 shows the computation of the output gate (\\(o_t\\)) which decides how the hidden states are updated.\nAt the current time step (\\(t\\)), the cell is updated by Equation 5 and the values of the hidden units are updated by Equation 6\n\n\n4.2 Evaluation Metric\nTraditionally, researchers utilized classical statistical indicators used in solar power forecasting, specically the Mean Absolute Error, or also known as \\(MAE\\), as mentioned by(Wang, Qi, and Liu 2019) and (Sobri, Koohi-Kamali, and Rahim 2018). The \\(MAE\\) refers to the average distance between the measured values and forecasting model. This analysis is appropriate to evaluate uniform prediction errors.\n\\[\n  MAE = \\frac{1}{N} \\sum_{n=1}^{N} | P_{pred}-{P_{act} |}\n\\tag{7}\\]\nwhere \\(P_{pred}\\) defines the predicted energy generation by the model and \\(P_{act}\\) defines the actual energy generation\nHowever, the \\(MAE\\) fails to consider power grid elements and requirements. It is therefore important to utilize performance metrics that asses power generation forecasting by considering various forecast time-scales, and capacity restrictions. In addition, Sobri, Koohi-Kamali, and Rahim (2018) indicated that depending on the use case, metrics assessments can be classified into four types i.e., statistical, ramp characterization, uncertainty quantification, and economic metrics. As such, Almeida, Perpiñán, and Narvarte (2015), implemented a modified \\(MAE\\), defined as cvMAE, which evaluates a model by that penalize the hourly or daily energy error. The \\(cvMAE\\) measures the goodness of the predictions for applications requiring hourly predictions during a period of a day. (Almeida, Perpiñán, and Narvarte 2015)\n\\[\ncvMAE = \\frac{MAE}{P_{mean}}\n\\tag{8}\\]\nwhere \\(P_{mean}\\) defines the mean of measured power in a given time-horizon.\nIn this study, a modified version of the \\(MAE\\) and \\(cvMAE\\) will be utilized. The proposed metric takes into consideration not only classical statistical error measurement observed in the \\(MAE\\), but also energy industry related considerations, such as day-ahead projected generation and rated capacity of each specific plant, labeled as \\(mMAE\\)\n\\[\nmMAE = \\frac{\\frac{1}{N} \\sum_{n=1}^{N} | P_{pred}-{P_{act} |}}{P_{max}}=\\frac{MAE}{P_{max}}\n\\tag{9}\\]\nwhere \\(P_{max}\\) indicates a solar power plant’s peak installed capacity, or the intended full-load sustained output of the power plant."
  },
  {
    "objectID": "posts/2023-04-25-Solar/index.html#sec-appendix",
    "href": "posts/2023-04-25-Solar/index.html#sec-appendix",
    "title": "Here Comes The Sun",
    "section": "9 Appendix",
    "text": "9 Appendix"
  },
  {
    "objectID": "posts/2023-04-26-Music/index.html#sec-authors",
    "href": "posts/2023-04-26-Music/index.html#sec-authors",
    "title": "Music Through The Years",
    "section": "1 Authors",
    "text": "1 Authors\nThis analysis was done together with my groupmate Jac Lin T. Yu\nOriginal creation and submission of this report was last Dec 2019."
  },
  {
    "objectID": "posts/2023-04-26-Music/index.html#sec-executivesummary",
    "href": "posts/2023-04-26-Music/index.html#sec-executivesummary",
    "title": "Music Through The Years",
    "section": "3 Infographic",
    "text": "3 Infographic\nFor more details, you can view the Infographic above."
  }
]