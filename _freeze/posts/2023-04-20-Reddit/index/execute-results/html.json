{
  "hash": "0d68193b81af7907dd73ebf6e518ccb0",
  "result": {
    "markdown": "---\ntitle: Unraveling Subreddits\ndescription: 'Identifying underlying themes in Reddit titles using non-supervised clustering '\ndate: '2022-04-23'\ncategories:\n  - python\n  - EDA\n  - SQL\n  - analysis\nimage: thumbnail.jpeg\nformat:\n  html:\n    code-fold: true\neditor:\n  render-on-save: true\ndraft: true\n---\n\n![](thumbnail.jpeg)\n\n<h1>Executive Summary</h1>\n<div class='executive-summary'>\n\nOriginal creation and submission for this notebook was last June 2019.\n\n[Reddit](https://www.reddit.com/) is an American discussion and aggregation website for user-generated content with more than 500 million monthly visitors. We uncovered the underlying categories or \"subreddits\" by performing representative-based clustering on a sample of Reddit titles. Results reveal that Reddit is a very American-centric platform and that the two main themes are (a) U.S. politics, and (b) seeking help from other Redditors on a wide range of topics including technology, food, and legal matters. For a small number of clusters ($n=10$), the high-level categories that emerged include the 2016 U.S. Presidential Elections (Donald Trump, Hillary Clinton, and Bernie Sanders), food, legal concerns, and \"New year\"-related subreddits. Increasing the number of clusters to $n=16$ revealed more specific subreddits such as technical support, recipes, apartment/rental concerns, and reaction compilations.\n</div>\n\n<h2>Acknowledgements</h2>\n\nThis analysis was done together with my Lab partner, George Esleta, and Cohortmates - Gilbert Chua, Nigel Silva and Oonre Advincula-Go. \n\n<h2> A. Introduction and the Problem Statement </h2>\n\n[Reddit](https://www.reddit.com/) is a discussion and content aggregation website. As of June 2019, it is ranked as the #5 most visited website in the United States and #15 worldwide. Registered members, also called \"Redditors\", upload user-generated content (UGC) which are either voted up or voted down by other Redditors. Reddit posts are grouped into user-created boards or communities called \"subreddits\", with each subreddit having a specific topic such as food, entertainment, and politics.\n\n<h2> B. Methodology</h2>\n\nReddit titles and authors were scraped from the [Reddit](https://www.reddit.com/) website and stored in a text file. The file is tab-delimited and has two columns: author and title. Flow of this notebook is as follows:\n\n<ol>\n<li> Pre-requisites: Load Requirement Package\n<li> Reading the Reddit data\n<li> Initial data cleaning\n<li> Exploratory data analysis\n<li> Vectorization (bag-of-words representation) using TFIDF Vectorizer\n<li> Dimensionality reduction using Latent Semantic Analysis (LSA)\n<li> Representative-based clustering using *k*-means\n</ol>\n\n<h3>Pre-requisites: Load Requirement Package</h3>\n\nBefore anything else, let us first load all important modules for this exercise.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-summary=\"Loading required modules\"}\n# These are standard imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom collections import Counter\nfrom IPython.display import display_html\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import calinski_harabaz_score, silhouette_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n    from sklearn.metrics import confusion_matrix\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist, RegexpTokenizer\n\nfrom IPython.display import HTML\nfrom scipy.spatial.distance import euclidean\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import euclidean, cityblock\n\nimport urllib\nimport requests\nimport pprint\nfrom PIL import Image\n\npp = pprint.PrettyPrinter(indent=4)\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n```\n:::\n\n\n<h3>Reading the Reddit data</h3>\n\n\nThe data was stored in the ACCESS lab cloud computer (JOJIE). So for this instance, the sample Reddit text file was read using the `read_csv` function of `pandas`. In reading the file, tab (`\\t`) separator was used, and the first line was skipped.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-summary=\"Load Reddit data\"}\ndf = pd.read_csv('reddit-dmw-sample.txt', sep='\\t', skiprows=1, header=None, \n                 usecols = [1, 2], \n                 names = ['author', 'title'],)\n```\n:::\n\n\nThe resulting dataframe has two columns: `author` and `title`. A sample of the Reddit topics are shown below.\n\n| author        | title                                           |\n|---------------|-------------------------------------------------|\n| PrimotechInc  | 7 Interesting Hidden Features of apple ios9      |\n| xvagabondx    | Need an advice on gaming laptop                 |\n| nkindustries  | Semi automatic ROPP Capping machine / ROPP Cap...|\n| Philo1927     | Microsoft Plumbs Ocean’s Depths to Test Underw...|\n| tuyetnt171    | OPPO F1 chính hãng - Fptshop.com.vn              |\n: Tale 1: Sample Reddits from the Data\n\n<h3> Initial data cleaning </h3>\n\nOnce the Reddit data is stored in a dataframe, initial data cleaning was performed by converting all titles to lowercase, and removing leading and trailing spaces. Duplicate titles were also dropped. \n\nThe unique titles are then stored in a list:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-summary=\"Viewing the unique titles\"}\ndf['title'] = df['title'].str.lower()\ndf['title'] = df['title'].str.strip()\ndocs = list(df.title.unique())\npp.pprint('Number of titles: ' + str(len(docs)))\n```\n:::\n\n\n```\n'Number of titles: 5848'\n```\n\nAfter removing duplicates, there are 5848 unique Reddit titles in the sample dataset.\n\nSample cleaned topics are shown below. Note that there still are punctuation marks and special characters. These will be removed during vectorization.\n\n```\n[   '7 interesting hidden features of apple ios9',\n    'need an advice on gaming laptop',\n    'semi automatic ropp capping machine / ropp cap sealing machine',\n    'microsoft plumbs ocean’s depths to test underwater data center',\n    'oppo f1 chính hãng - fptshop.com.vn',\n    'stewed ramps w/ fixings in helvatia, wv',\n    '(california) is it a crime when a religious figure/ lecturer has '\n    'relations with one of his followers',\n    'being accused of public indecency, among other things. this is a '\n    'misunderstanding because i had health issues. [kansas, usa]',\n    '[us-ca/nv] my parents have been separated for almost 20 years, my mom '\n    'found out my dad divorced her without her knowledge a few years ago when '\n    'i was about 14--can she claim backdated child support?',\n    'skyrim ps4?']\n```\n\n<h3> Exploratory Data Analysis </h3>\n\nShown below are the top 15 most used words in the Reddit sample data. Note that the most common word is \"TIL\" (Today I Learned), followed by the names of prominent US politicians such as US President Donald Trump, 2016 presidential candidate Hillary Clinton, and Senator Bernie Sanders. All three of them figured prominently in the 2016 U.S. presidential elections, with Trump as the presidential nominee under the Republican party, while Clinton and Sanders as the Democratic party presidential candidates. Initial analysis of the data reveal that the **2016 U.S. Presidential Election is one of the primary themes in the sample Reddit data**. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-summary=\"Viewing the unique titles\"}\nmask = np.array(Image.open('Icon-Reddit1.png'))\ntokenizer = RegexpTokenizer(r'\\w{2,}')\nword_tokens = tokenizer.tokenize(' '.join(docs))\nstopword = set(stopwords.words('english') + list(stop_words.ENGLISH_STOP_WORDS))\ndocs_filtered = [w for w in word_tokens if not w in stopword] \ncounts = FreqDist(docs_filtered)\nlabels, values = zip(*counts.items())\nindSort = np.argsort(values)[::]\nlabels = np.array(labels)[indSort[-15:]]\nvalues = np.array(values)[indSort[-15:]]\nindexes = np.arange(len(labels))\nbar_width = 0.35\n\nfig, ax = plt.subplots(1, 2, dpi=100, figsize = (10,4))\nax[0].barh(indexes, values, color='#ff4500')\nax[0].set_xlabel('Word count')\nax[0].set_title('Most common words in the Reddit sample data')\nax[0].set_yticks(indexes + bar_width);\nax[0].set_yticklabels(labels);\n\nwordcloud = WordCloud(background_color=\"white\", max_words=100, min_font_size=8, mask=mask, \n                      prefer_horizontal=0.6, colormap='copper', relative_scaling=.5).generate(' '.join(docs_filtered))\n\nax[1].imshow(wordcloud, interpolation='bilinear')\nax[1].axis('off');\n```\n:::\n\n\n![FIGURE 1: (1) Frequency distribution of the top 15 words in the Reddit sample data, and (b) the corresponding wordcloud. Top words include TIL (Today I learned) and the names of 2016 US Presidential Aspirants (Donald Trump, Bernie Sanders, Hillary Clinton) ](image-outputs/1-TILoutput.png){width=100%}\n\n<h3> Vectorization/Bag-of-words representation </h3>\n\nTo uncover the other underlying themes apart from the 2016 U.S. Presidential Election, unsupervised clustering via k-means is performed on the collection of Reddit titles. To do this, the topics need to be *vectorized* or be converted to their bag-of-words representation. The `TfidfVectorizer` vectorizer was used to perform the vectorization. \n\nThe `TFidfVectorizer` also performs further cleaning of the data by removing both frequent words and rare words. *Stopwords*, or words that appear too frequently in the English language (e.g., the, a, an, and, or), were dropped from the corpus. Reddit-specific stopwords, or words that appear in more than 70% of the titles, were also ignored by setting the `max_df` parameter of `TFidfVectorizer` to $0.7$.\n\nRare words, or words that appeared in less than 0.1% of the Reddit titles, were also excluded from the corpus. This was implemented by setting the `min_df` parameter of the `TFidfVectorizer` to $0.001$.\n\n*n*-grams of word lengths between $n=1$ and $n=3$ were extracted as part of the vocabulary. This range of n-grams was chosen in order to preserve the context of certain phrases such as \"Hillary Clinton\" ($n=2$), \"Donald Trump\" ($n=2$), and \"Happy new year\" ($n=3$) \n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-summary=\"Using a TFIDF Vectorizer\"}\n# Creating a TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b\\w(?:\\w|\\-)+\\b', \n                                   stop_words=stopword, \n                                   ngram_range=(1,3), min_df=0.001, max_df=0.7)\n\n# Creating a Bag-Of-Words\nbow = tfidf_vectorizer.fit_transform(docs)\n\n# Identifying the vocabulary\nvocabulary = tfidf_vectorizer.get_feature_names()\n```\n:::\n\n\nAfter vectorization, the sample of Reddit titles has **1,655 unique items** in its vocabulary:\n\nSample vocabulary items are shown below:\n\n```\n[   'years old',\n    'yesterday',\n    'york',\n    'york times',\n    'young',\n    'youtube',\n    'zealand',\n    'zelda',\n    'ăn',\n    'طريقة']\n```\n\n<h3> Dimensionality reduction using Latent Semantics Analysis (LSA) </h3>\n\nThe resulting bag of words is a 5848 by 1655 matrix:\n\n```\nprint(\"\\t Dimensions: \", bow.shape)\nDimensions:  (5848, 1655)\n```\n\n\nIt would be computationally expensive to perform clustering on a dataset this large. To minimize computational power,  dimensionality reduction was performed on the vectorized data via **latent semantic analysis (LSA)**. This was implemented using the `TruncatedSVD` class of `sklearn`.\n\nAs a general rule for LSA, fewer dimensions allow for broader comparisons of the themes contained in a collection of text, while a higher number of dimensions enable more specific comparisons of themes. A dimesionality between 50 and 1,000 are suitable depending on the size and nature of the document collection, with $300 \\pm 50$ the optimum value. Unlike in other dimensionality reduction techniques such as principal component analysis (PCA), checking the proportion of variance retained to determine the optimal dimensionality is **NOT** applicable to LSA. \n\nIn this study, $n_{components} = 100$ is used to extract the underlying themes.\n\n<h4> Reduced dimensionality</h4>\n\nFor the purposes of this study, we reduced the dimensionality of the Reddit sample data to **100 components**. The transformed Reddit dataset is shown below. Numerous subclusters are revealed, confirming that there are multiple categories or themes in the dataset. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-summary=\"Using a TruncatedSVD to reduce the number of components to 100 components, then getting bag of words, the using TSNE\"}\ntsvd1 = TruncatedSVD(n_components=100)\nnormalizer = Normalizer(copy=False)\nlsa1 = make_pipeline(tsvd1, normalizer)\n\n# Getting the bag of words\nbow_tsvd1 = lsa1.fit_transform(bow)\n\n# Applying a TSNE model\ntsne = TSNE(n_components=2, random_state=1704, n_iter=1500, n_iter_without_progress=500, perplexity=50, learning_rate=10)\nbow_tsne1 = tsne.fit_transform(bow_tsvd1)\n```\n:::\n\n\nNow, let's make a graph on the TSNE model's bag of words. Below you'll see it flatted TSNE of the bag of words.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-summary=\"Creating the plot of the TSNE.\"}\nfig, ax = plt.subplots(dpi = 100)\nax.scatter(bow_tsne1[:,0], bow_tsne1[:,1], alpha=0.5, label=True, marker='o', s=8, c=\"#ff4500\");\n```\n:::\n\n\n![FIGURE 2: Reddit topics visualized in two dimensions using TSNE. ](image-outputs/2-TSNE.png){width=100%}\n\n<h3>Representative-based clustering using *k*-means </h3>\n<h4>1. Selecting the optimal number of clusters `k` </h4>\n\nTo determine the optimal number of clusters, we perform k-means clustering for different values of the cluster count $k$ from $k=2$ to $k=20$. This range of values was chosen to keep the clustering parsimonious.  \n\nSeveral internal validation measures were computed for each value of $k$. The internal validation measures that were used are (a) intracluster to intercluster distance ratio, (b) Calinski-Harabasz score (c) Silhouette coefficient, and (d) sum of square distances to centroids coefficient. \n\nThe optimal value of $k$ is then chosen based on the following criteria:\n\n1. Sum-of-square distance to centroid is minimized\n2. Calinski-Harabasz index is maximized\n3. Intracluster to intercluster distance ratio is minimized\n4. Silhouette coefficient is maximized\n\nPlotted below are the values of the internal validation criteria for values of $k$ between 2 and 20. **$k = 10$** and **$k = 16$** are good candidates for the number of clusters because for these values, the intracluster to intercluster distance ratio has a local minima, and the silhouette coefficient has a relatively high value. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-summary=\"Function for computing intracluster to intercluster distance ratio\"}\ndef intra_to_inter(X, y, dist, r):\n    \"\"\"\n    Compute intracluster to intercluster distance ratio\n    \n    Parameters\n    ----------\n    X : array\n        Design matrix with each row corresponding to a point\n    y : array\n        Class label of each point\n    dist : callable\n        Distance between two points. It should accept two arrays, each \n        corresponding to the coordinates of each point\n    r : integer\n        Number of pairs to sample\n        \n    Returns\n    -------\n    ratio : float\n        Intracluster to intercluster distance ratio\n    \"\"\"\n    P = []\n    Q = []\n    p=0\n    q=0\n    np.random.seed(11)\n    d = np.random.choice(range(0,len(X)), (r,2), replace=True)\n    for a in range(0,len(d)):\n        if  d[a][0] == d[a][1]:\n            continue\n        elif y[d[a][0]] == y[d[a][1]]:\n            P.append(dist(X[d[a][0]],X[d[a][1]]))\n            p+=1\n        else:\n            Q.append(dist(X[d[a][0]],X[d[a][1]]))\n            q+=1\n    if len(P) == 0 or len(Q) == 0:\n        ratio = 0\n    else:\n        ratio = (np.sum(P)/len(P))/(np.sum(Q)/len(Q))\n    return ratio\n\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-summary=\"Function to create the clusters, cluster labels (and scores of internal validation values)\"}\ndef cluster_range(X, clusterer, k_start=2, k_stop=16, actual=None):\n    \"\"\"\n    Accepts the design matrix, the clustering object, the initial and final \n    values to step through, and, optionally, actual labels. It should return a\n    dictionary of the cluster labels, internal validation values and, if\n    actual labels is given, external validation values, for every k.\n\n    Parameters\n    ----------\n    X : design matrix\n    clusterer : clustering object\n    k_start : initial value of clusters\n    k_stop : final value of clusters\n    actual : actual labels\n\n    Returns\n    -------\n    dictionary\n    \"\"\"\n\n\n    validation_dict = {\"chs\": [],\n                       \"iidrs\": [],\n                       \"inertias\": [],\n                       \"scs\": [],\n                       \"ys\": []\n                       }\n    if actual is not None:\n        validation_dict = {\"chs\": [],\n                           \"iidrs\": [],\n                           \"inertias\": [],\n                           \"scs\": [],\n                           \"ys\": [],\n                           \"amis\": [],\n                           \"ars\": [],\n                           \"ps\": []\n                           }\n    for k in range(k_start, k_stop+1):\n        clusterer.n_clusters = k\n        np.random.seed(11)\n        y_predict = clusterer.fit_predict(X)\n        validation_dict[\"chs\"].append(calinski_harabaz_score(X, y_predict))\n        validation_dict[\"iidrs\"].append(\n            intra_to_inter(X, y_predict, euclidean, 50))\n        validation_dict[\"inertias\"].append(clusterer.inertia_)\n        validation_dict[\"scs\"].append(silhouette_score(X, y_predict))\n        validation_dict[\"ys\"].append(y_predict)\n        if actual is not None:\n            validation_dict[\"amis\"].append(\n                adjusted_mutual_info_score(actual, y_predict))\n            validation_dict[\"ars\"].append(\n                adjusted_rand_score(actual, y_predict))\n            validation_dict[\"ps\"].append(purity(actual, y_predict))\n    return validation_dict\n```\n:::\n\n\n```\nkmeans_dict1 = cluster_range(\n    # bag of words results\n    bow_tsvd1, \n    # using a k-means clusterer\n    KMeans(random_state=1704, n_init=20, max_iter=1000, tol=1e-6), \n    2,\n    20)\n```\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-summary=\"Function to plot internal validation values\"}\ndef plot_internal(inertias, chs, iidrs, scs):\n    \"\"\"\n    Plot internal validation values\n    \"\"\"\n    fig, ax = plt.subplots(1,4,dpi=100, figsize=(10,2))\n    ks = np.arange(2, len(inertias)+2)\n    ax[0].plot(ks, inertias, '-o', label='SSE')\n    ax[1].plot(ks, chs, '-ro', label='CH')\n    ax[0].set_xlabel('$k$')\n    ax[1].set_xlabel('$k$')\n    ax[0].set_title('SSE')\n    ax[1].set_title('CH')\n    ax[2].plot(ks, iidrs, '-go', label='Inter-intra')\n    ax[3].plot(ks, scs, '-ko', label='Silhouette coefficient')\n    ax[2].set_xlabel('$k$')\n    ax[2].set_title('Inter-intra')\n    ax[3].set_xlabel('$k$')\n    ax[3].set_title('Silhouette coefficient')\n    return ax\n```\n:::\n\n\nLet's plot the interal validation values\n```\nplot_internal(kmeans_dict1['inertias'], kmeans_dict1['chs'], \n    kmeans_dict1['iidrs'], kmeans_dict1['scs']);\n```\n![FIGURE 3: Plots of the various internal validation measures for $k=2$ to $k=20$. $k=10$ and $k=16$ were chosen because of the low intercluster-intracluster ration and the high silhouette coefficients for those values.](image-outputs/3-internal-validation.png){width=100%}\n\n<h4>2. `k = 10`</h4>\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-summary=\"Code of setting k-means k=10\"}\nn1 = 10\nkmeans1 = KMeans(n_clusters=n1, random_state=1704, n_init=20, max_iter=1000, tol=1e-6)\ny_predict_kmeans1 = kmeans1.fit_predict(bow_tsvd1)\n```\n:::\n\n\n$k$-means clustering was performed on the Reddit sample titles using 10 clusters. The resulting clusters and the cluster size distribution are shown below.\nThe clusters are relatives balanced in terms of cluster size, with the exception of one cluster (cluster 5) which contains more than 2500 Reddit titles.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code code-summary=\"Creating the plot and clusters\"}\nfig, ax = plt.subplots(1,2, dpi = 100, figsize=(10,3))\nax[0].scatter(bow_tsne1[:,0], bow_tsne1[:,1], c=y_predict_kmeans1, alpha=0.5, label=True, marker='o', s=8);\n\ncounts = Counter(y_predict_kmeans1)\nlabels, values = zip(*counts.items())\nindSort = np.argsort(values)[::]\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\nindexes = np.arange(len(labels))\nbar_width = 0.35\nax[1].barh(indexes, values, color='#ff4500')\nax[1].set_xlabel('Cluster size')\nax[1].set_ylabel('Cluster labels')\nax[1].set_yticks(indexes + bar_width);\nax[1].set_yticklabels(labels);\n```\n:::\n\n\n![FIGURE 4: Resulting clustering for $n=10$, and the cluster size distribution per cluster. The clusters are relatives balanced in terms of cluster size, with the exception of one cluster (cluster 5) which contains more than 2500 Reddit titles.](image-outputs/4-clusters10.png){width=100%}\n\nTo understand the theme or category of each cluster, the **top words** or the most frequent words for each cluster was obtained. Shown below are the top 10 most occurring words per cluster, and the corresponding wordclouds.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code code-summary=\"Code to see the top 10 most occuring words per cluster\"}\nfeatures = tfidf_vectorizer.get_feature_names()\nweights = np.dot(kmeans1.cluster_centers_, tsvd1.components_)\nweights = np.abs(weights)\n\nfig, ax = plt.subplots(2,5, dpi=100, figsize=(16,5))\nplt.subplots_adjust(wspace=1.1, hspace=.8)\nfor i in range(kmeans1.n_clusters):\n    indices = np.argsort(weights[i])[-10:][::]\n    top_words = [features[index] for index in indices]\n    values = weights[i,indices]\n    indices = np.arange(len(top_words))\n    bar_width = 0.35\n    ax[i//5][i%5].barh(indices, values, color='#ff4500')\n    ax[i//5][i%5].set_xlabel('Weight')\n    ax[i//5][i%5].set_ylabel('Top words')\n    ax[i//5][i%5].set_yticks(indexes + bar_width);\n    ax[i//5][i%5].set_yticklabels(top_words);\n    ax[i//5][i%5].set_title('Cluster '+ str(i+1))\n```\n:::\n\n\n:::{.column-page}\n![FIGURE 5: Top 10 most occuring words per cluster ($k=10$)](image-outputs/5-topwords.png){width=100%}\n:::\n\n::: {.cell execution_count=14}\n``` {.python .cell-code code-summary=\"Code of placing a Reddit mask wordcloud\"}\nmask = np.array(Image.open('Icon-Reddit1.png'))\ntopic = ['TIL', 'Apartment/Rent', 'New Year', 'Iowa Caucus', 'Food', 'Technology', 'Donald Trump', 'Bernie Sanders', 'Advise-seeking', 'Hillary Clinton']\nfig, ax = plt.subplots(2,5,dpi=100, figsize=(16,4))\nplt.subplots_adjust(wspace=.2, hspace=0)\nfor ctr in range(kmeans1.n_clusters):\n    indices = [i for i, x in enumerate(y_predict_kmeans1) if x == ctr]\n    wordcloud = WordCloud(background_color=\"white\", max_words=100, min_font_size=8, mask=mask, \n                          prefer_horizontal=0.7, colormap='copper').generate(' '.join([docs[index] for index in indices]))\n    ax[ctr//5][ctr%5].imshow(wordcloud, interpolation='bilinear')\n    ax[ctr//5][ctr%5].axis('off');\n    ax[ctr//5][ctr%5].set_title(topic[ctr]);\n```\n:::\n\n\n:::{.column-page}\n![FIGURE 6: Wordcloud per cluster ($k=10$)](image-outputs/6-wordclouds.png){width=100%}\n:::\n\nUsing $k = 10$, the general categories listed below were revealed. Sample Reddit titles for each category are also shown.\n\nThe clustering results show that **Reddit is a very American-centric platform**. Four of the ten topics are related to the 2016 U.S. Presidential Election, specifically on the Republican nominee Donald Trump, Democrat candidates Hillary Clinton and Bernie Sanders, and the Iowa caucuses, a biennial electoral event that marks the first major contest of the United States presidential primary season. \n\nAlso, two of the topics are related to \"advise\". This reveals that apart from discussing politics, Reddit users go to Reddit to ask help or seek advise from other Redditors, whethere it be technology-related or legal. \n\nCluster topics:\n\n<h5> TIL (Today I Learned) </h5>\n```\n[   'til maternal kisses are not effective in alleviating minor childhood '\n    'injuries (boo-boos): a randomized, controlled and blinded study.',\n    'til goldman sachs did a study in 2009 that estimated a unified korea '\n    'could boast an economy larger than france, germany, and even japan by '\n    '2050 with a gdp of $6 trillion.',\n    \"til the justice league's martian manhunter was once addicted to cookies \"\n    'and hulked out when deprived of them.']\n```\n\n<h5> Apartment/rent-related topics </h5>\n```\n[   'business telephone service',\n    'illegal towing from apartment complex md.',\n    'can a landlord place a security camera in a communal space']\n```\n\n<h5> \"Happy new year\" </h5>\n```\n[   'new poll shows how far hillary has fallen with democrats',\n    'new zealand - new tenants wanting to take over internet account, isp is '\n    'refusing.',\n    'watch \"best of maskedman - 2015 new year special.\" on youtube',\n    'happy new year reddit! :) i quit retirement, spent $250k in savings and '\n    '51 months (15,000 hours) to develop the new internet, owned by the '\n    'people, powered by humanity - the first company in history equally owned '\n    'by everybody alive! we are uniting the whole world into one!',\n    'til new yorkers at nye time square wear diapers and pads for lack of '\n    'porta potties']\n```\n\n<h5> \"Iowa caucuses\" </h5>\n```\n[   'the gop’s condemnation of ‘sanctuary cities’ is surprisingly awkward in '\n    'iowa',\n    \"jesse watters questions sanders' young supporters in iowa\",\n    '2016 iowa democratic caucus locations - get out there and caucus!!']\n```\n\n<h5> Food </h5>\n```\n[   'i have access to tons and tons of green onion tops,what can i make?',\n    'mexican chopped salad',\n    's&amp;m restaurant in beijing caters to millennial’s basic instincts by '\n    'serving food &amp; sex']\n```\n\n<h5> Donald Trump </h5>\n```\n[   \"donald trump says he'll attract democrats; polls say that's unlikely\",\n    'inside the clintons’ plan to defeat donald trump',\n    'hecklers disrupt trump rally, photographer shoved to the ground']\n```\n\n<h5> Bernie Sanders </h5>\n```\n[   '“it appears that mr. trump is getting nervous that working families are '\n    'catching on that his policies represent the interests of the billionaire '\n    'class against almost everyone else.\" - bernie sanders',\n    'killer mike: bernie sanders is the only candidate for black voters',\n    'bernie sanders: the most fascist candidate of all.']\n```\n\n<h5> Food </h5>\n```\n[   'i have access to tons and tons of green onion tops,what can i make?',\n    'mexican chopped salad',\n    's&amp;m restaurant in beijing caters to millennial’s basic instincts by '\n    'serving food &amp; sex']\n```\n\n<h5> Advise-seeking </h5>\n```\n[   'need an advice on gaming laptop',\n    'charged with dui 2 years and 4 months after i was involved in a single '\n    'car accident.',\n    \"i'd like to buy the rights of a post on reddit to recreate in another \"\n    'medium. how do i create a legal contract for this between strangers '\n    'online?',\n    'significant online food ordering trend',\n    'md - need advice evicting a suicidal/alcoholic tenant/roommate.']\n```\n\n<h5> Hillary Clinton </h5>\n```\n[   'semi automatic ropp capping machine / ropp cap sealing machine',\n    'state: hundreds of old clinton emails newly classified',\n    'georgia poll: clinton 70, sanders 23']\n```\n\n<h4>3. `k = 16`</h4>\n\nLet's try with a `k=16`\n\n::: {.cell execution_count=15}\n``` {.python .cell-code code-summary=\"Setting k-means to k=16\"}\nn2 = 16\n\nkmeans2 = KMeans(n_clusters=n2, random_state=1704)\ny_predict_kmeans2 = kmeans2.fit_predict(bow_tsvd1)\n\nfig, ax = plt.subplots(1,2, dpi = 100, figsize=(10,3))\nax[0].scatter(bow_tsne1[:,0], bow_tsne1[:,1], c=y_predict_kmeans2, alpha=0.5, label=True, marker='o', s=8);\n\ncounts = Counter(y_predict_kmeans2)\nlabels, values = zip(*counts.items())\nindSort = np.argsort(values)[::]\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\nindexes = np.arange(len(labels))\nbar_width = 0.35\nax[1].barh(indexes, values, color='#ff4500')\nax[1].set_xlabel('Cluster size')\nax[1].set_ylabel('Cluster label')\nax[1].set_yticks(indexes + bar_width);\nax[1].set_yticklabels(labels);\n```\n:::\n\n\n![FIGURE 7. Clustering and Cluster size distribution for $k=16$)](image-outputs/7-k16.png){width=100%}\n\nTo understand the theme or category of each cluster, the **top words** or the most frequent words for each cluster was obtained. Shown below are the top 10 most occurring words per cluster, and the corresponding wordclouds.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code code-summary=\"Code to see the top words per 16 clusters\"}\nfeatures = tfidf_vectorizer.get_feature_names()\nweights = np.dot(kmeans2.cluster_centers_, tsvd1.components_)\nweights = np.abs(weights)\nfig, ax = plt.subplots(4,4, dpi=100, figsize=(10,10))\nplt.subplots_adjust(wspace=1.1, hspace=.8)\nfor i in range(kmeans2.n_clusters):\n    indices = np.argsort(weights[i])[-10:][::]\n    top_words = [features[index] for index in indices]\n    values = weights[i,indices]\n    indices = np.arange(len(top_words))\n    bar_width = 0.35\n    ax[i//4][i%4].barh(top_words, values, color='#ff4500')\n    ax[i//4][i%4].set_xlabel('Weight')\n    ax[i//4][i%4].set_ylabel('Top words')\n#    ax[i//4][i%4].set_yticks(indexes + bar_width);\n    ax[i//4][i%4].set_yticklabels(top_words);\n    ax[i//4][i%4].set_title('Cluster '+ str(i+1))\n```\n:::\n\n\n:::{.column-page}\n![FIGURE 8. Top 10 most frequest words per cluster ($k=16$)](image-outputs/8-k16-words.png){width=100%}\n:::\n\n::: {.cell execution_count=17}\n``` {.python .cell-code code-summary=\"Code of placing a Reddit mask wordcloud for k=16\"}\ntopics1 = ['Tech support', 'Clinton', 'Trump', 'Cluster 4', 'Recipes', 'TIL', 'Aparment/Rent', 'Technology'\n           , 'Sanders', 'Food', 'Cluster 11', 'Car-related queries', 'New Year', 'Reaction Compilation', 'Legal concerns', 'Cluster 15']\n\nfig, ax = plt.subplots(4,4,dpi=100, figsize=(16,12))\nplt.subplots_adjust(wspace=0.2, hspace=0)\nfor ctr in range(kmeans2.n_clusters):\n    indices = [i for i, x in enumerate(y_predict_kmeans2) if x == ctr]\n    wordcloud = WordCloud(background_color=\"white\", max_words=100, min_font_size=8, mask=mask, \n                          prefer_horizontal=0.6, colormap='copper').generate(' '.join([docs[index] for index in indices]))\n\n    ax[ctr//4][ctr%4].imshow(wordcloud, interpolation='bilinear')\n    ax[ctr//4][ctr%4].axis('off');\n    ax[ctr//4][ctr%4].set_title(topics1[ctr]);\n```\n:::\n\n\n:::{.column-page}\n![FIGURE 9. Wordcloud per cluster ($k=16$)](image-outputs/9-k16-wordclouds.png){width=100%}\n:::\n\nAs the number of clusters was increased, the more specific themes emerge. The following themes emerged:  \n\n\n<h5> Technical Support </h5>\n```\n[   'smoothly solution for the canon printer 1-877-776-4348',\n    'follow these step and escan antivirus easily',\n    'norton antivirus technical support phone number',\n    'itech logik | uk microsoft help desk phone number',\n    'til tupak shakur became the first artist to have an album at number one '\n    'on the billboard 200 while serving a prison sentence.']\n```\n\n<h5> Hillary Clinton Emails </h5>\n```\n[   'semi automatic ropp capping machine / ropp cap sealing machine',\n    'state: hundreds of old clinton emails newly classified',\n    'georgia poll: clinton 70, sanders 23',\n    'nsa whistleblower: clinton email server was ‘open to being hacked by '\n    'anybody in the world’',\n    \"hillary clinton's new hampshire boosters out in full force\"]\n```\n\n<h5> Donald Trump </h5>\n```\n[   \"donald trump says he'll attract democrats; polls say that's unlikely\",\n    'inside the clintons’ plan to defeat donald trump',\n    'hecklers disrupt trump rally, photographer shoved to the ground',\n    'a photographer covering a donald trump rally in virginia said a secret '\n    'service agent choked him and slammed him to the ground monday as he tried '\n    'to leave a media pen at the event where a protest erupted.',\n    'til that donald trump sold steaks at the sharper image']\n```\n\n<h5> Recipes </h5>\n```\n[   'i have access to tons and tons of green onion tops,what can i make?',\n    'mexican chopped salad',\n    'dublin, ireland has gone mad for spice bags from takeaways, i tried my '\n    'hand at my own. potato, shredded chicken, peppers and onions, cooked in '\n    'garlic, chillies and spices. apologies for photo quality!',\n    'tips for starting a healthy lifestyle!',\n    \"arroz carreteiro - brazilian wagoners' rice gourmetcentric\"]\n```\n<h5> TIL (Today I learned) topics </h5>\n```\n[   'til maternal kisses are not effective in alleviating minor childhood '\n    'injuries (boo-boos): a randomized, controlled and blinded study.',\n    'til goldman sachs did a study in 2009 that estimated a unified korea '\n    'could boast an economy larger than france, germany, and even japan by '\n    '2050 with a gdp of $6 trillion.',\n    \"til the justice league's martian manhunter was once addicted to cookies \"\n    'and hulked out when deprived of them.',\n    'til that the top 25 hedge fund managers in the us earn more than all us '\n    'kindergarten teachers (and pay a smaller tax rate)',\n    'til leonard howell (\"the first rasta\") preached that ras tafari, the '\n    'emperor of ethiopia in 1933, was the messiah, that black people were the '\n    'chosen of god and that they would soon be restored to their native '\n    'country of ethiopia.']\n```\n\n<h5> Apartment and Rent Concerns </h5>\n```\n[   'my dad died yesterday and my mom is now being evicted because she is not '\n    'on the lease, what can she do?',\n    'landlord charged my girlfriend a damage fee of $5,000 for a broken door',\n    'is refusing to take a breathalyzer test an admission of guilt?',\n    'does an apartment with living/housing code violations against the city '\n    'equal a nullified lease?',\n    'had verbal agreement with room mate for payment of back monies to be '\n    'paid, to be formalized in writing, questions, etc.']\n```\n\n<h5> Bernie Sanders </h5>\n```\n[   '“it appears that mr. trump is getting nervous that working families are '\n    'catching on that his policies represent the interests of the billionaire '\n    'class against almost everyone else.\" - bernie sanders',\n    'killer mike: bernie sanders is the only candidate for black voters',\n    'hillary clinton has already forgotten about bernie sanders | vice | '\n    'united states',\n    'bernie sanders ‘revolution’ threatens hillary clinton in iowa.',\n    \"jesse watters questions sanders' young supporters in iowa\"]\n```\n\n<h5> Food </h5>\n```\n[   'semi automatic ropp capping machine / ropp cap sealing machine',\n    'state: hundreds of old clinton emails newly classified',\n    'georgia poll: clinton 70, sanders 23']\n```\n<h5> Bernie Sanders </h5>\n```\n[   '“it appears that mr. trump is getting nervous that working families are '\n    'catching on that his policies represent the interests of the billionaire '\n    'class against almost everyone else.\" - bernie sanders',\n    'killer mike: bernie sanders is the only candidate for black voters',\n    'bernie sanders: the most fascist candidate of all.']\n```\n\n<h5> Food </h5>\n```\n[   'significant online food ordering trend',\n    's&amp;m restaurant in beijing caters to millennial’s basic instincts by '\n    'serving food &amp; sex',\n    'food near me – jasper arkansas',\n    'as i am very polish and grew up eating a lot of good food, i decided to '\n    'cook it for myself tonight.',\n    'food in thailand - november 2015']\n```\n\n<h5> Car-related topics </h5>\n```\n[   'charged with dui 2 years and 4 months after i was involved in a single '\n    'car accident.',\n    \"[md] car impounded after repair order wasn't received in september, \"\n    'options?',\n    'tesla unveils the model 3, its lowest-priced car',\n    'lincolnshire county council hit by £1m malware demand',\n    'florida adverse possession law question and others regarding married but '\n    'separate couples.']\n```\n\n<h5> \"Happy new year\" reddits </h5>\n```\n[   'watch \"best of maskedman - 2015 new year special.\" on youtube',\n    'happy new year reddit! :) i quit retirement, spent $250k in savings and '\n    '51 months (15,000 hours) to develop the new internet, owned by the '\n    'people, powered by humanity - the first company in history equally owned '\n    'by everybody alive! we are uniting the whole world into one!',\n    'mobile game of the year 2015: horizon chase',\n    'til that j. r. r. tolkien hand wrote and illustrated letters to his '\n    'children from father christmas every year for 20 years.',\n    '[bellingham, wa] wife looking at buying property in bella coola, bc; '\n    'legal implications of living half the year here in bellingham and half '\n    'the year in bella coola']\n```\n<h5> Vines reaction compilation </h5>\n```\n[   'happy 2016 and may all your games be mostly bug free',\n    'eight issues that could shape politics in 2016',\n    '2016 iowa democratic caucus locations - get out there and caucus!!',\n    '♥ dragonsden vines 🔒 - february 1, 2016 🔱 dragonsden reaction compilation '\n    '🎨',\n    'ark: surival evolved rings in 2016 with giant, rideable kangaroos']\n```\n\n<h5> Legal concerns </h5>\n```\n[   '(california) is it a crime when a religious figure/ lecturer has '\n    'relations with one of his followers',\n    'being accused of public indecency, among other things. this is a '\n    'misunderstanding because i had health issues. [kansas, usa]',\n    \"[philadelphia, pa] i'm concerned that something suspicious is going on at \"\n    \"my neighbor's house. i'd like to know if there's enough probable cause \"\n    'for the police to investigate.',\n    \"refugee's treated worse than dogs!\",\n    'i have a great idea for a trpg.']\n```\n\n\n<h2>Conclusions and Recommendations</h2>\n\nThe clustering results show that **Reddit is a very American-centric platform**. Majority of the topics are related to the 2016 U.S. Presidential Election, specifically on the Republican nominee Donald Trump, Democrat candidates Hillary Clinton, and the Iowa caucuses. \n\nAlso, the results show that the two primary motivations of users to post in Reddit is (a) to discuss politics, and (b) to seek advise/ask help from other Redditors, whether technological, legal, or culinary in nature.\n\nIncreasing the number of clusters also reveal more specific topics such as 'technical support' and ''Vines reaction compilation videos'.\n\nFor future studies, it is suggested to perform hierarchical clustering.\n\n<h2>References</h2>\n<i>\n<ul>\n<li> [Thumbnail Image](https://www.techspot.com/news/91081-reddit-bans-subreddit-accused-spreading-covid-19-disinformation.html)\n<li> [All the news](https://towardsdatascience.com/all-the-news-17fa34b52b9d)\n<li> [Clustering text documents using k-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n<li> [Latent Semantic Analysis](http://www.scholarpedia.org/article/Latent_semantic_analysis)\n</ul>\n</i>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}