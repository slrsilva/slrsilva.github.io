{
  "hash": "9950dae5cb94f11f80c22a05f6287ad0",
  "result": {
    "markdown": "---\ntitle: Unraveling Subreddits\ndescription: 'Identifying underlying themes in Reddit titles using non-supervised clustering '\ndate: '2022-04-20'\ncategories:\n  - python\n  - EDA\n  - SQL\n  - analysis\nimage: thumbnail.jpeg\nformat:\n  html:\n    code-fold: true\neditor:\n  render-on-save: true\ndraft: true\n---\n\n<!-- ![](thumbnail.jpeg) -->\n\n<h1>Executive Summary</h1>\n<div class='executive-summary'>\n\nOriginal creation and submission for this notebook was last June 2019.\n\n[Reddit](https://www.reddit.com/) is an American discussion and aggregation website for user-generated content with more than 500 million monthly visitors. We uncovered the underlying categories or \"subreddits\" by performing representative-based clustering on a sample of Reddit titles. Results reveal that Reddit is a very American-centric platform and that the two main themes are (a) U.S. politics, and (b) seeking help from other Redditors on a wide range of topics including technology, food, and legal matters. For a small number of clusters ($n=10$), the high-level categories that emerged include the 2016 U.S. Presidential Elections (Donald Trump, Hillary Clinton, and Bernie Sanders), food, legal concerns, and \"New year\"-related subreddits. Increasing the number of clusters to $n=16$ revealed more specific subreddits such as technical support, recipes, apartment/rental concerns, and reaction compilations.\n</div>\n\n<h2>Acknowledgements</h2>\n\nThis analysis was done together with my Lab partner, George Esleta, and Cohortmates - Gilbert Chua, Nigel Silva and Oonre Advincula-Go. \n\n<h2> A. Introduction and the Problem Statement </h2>\n\n[Reddit](https://www.reddit.com/) is a discussion and content aggregation website. As of June 2019, it is ranked as the #5 most visited website in the United States and #15 worldwide. Registered members, also called \"Redditors\", upload user-generated content (UGC) which are either voted up or voted down by other Redditors. Reddit posts are grouped into user-created boards or communities called \"subreddits\", with each subreddit having a specific topic such as food, entertainment, and politics.\n\n<h2> B. Methodology</h2>\n\nReddit titles and authors were scraped from the [Reddit](https://www.reddit.com/) website and stored in a text file. The file is tab-delimited and has two columns: author and title.\n\nThe general workflow for clustering the Reddit titles involves the following steps:\n<ol>\n<li> Pre-requisites: Load Requirement Package\n<li> Reading the Reddit data\n<li> Initial data cleaning\n<li> Exploratory data analysis\n<li> Vectorization (bag-of-words representation) using TFIDF Vectorizer\n<li> Dimensionality reduction using Latent Semantic Analysis (LSA)\n<li> Representative-based clustering using *k*-means\n</ol>\n\n<h3>Pre-requisites: Load Requirement Package</h3>\n\nBefore anything else, let us first load all important modules for this exercise.\n\n\n# These are standard imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom collections import Counter\nfrom IPython.display import display_html\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import calinski_harabaz_score, silhouette_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist, RegexpTokenizer\n\nfrom IPython.display import HTML\nfrom scipy.spatial.distance import euclidean\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import euclidean, cityblock\n\nimport urllib\nimport requests\nimport pprint\nfrom PIL import Image\n\npp = pprint.PrettyPrinter(indent=4)\n\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n\nAdditionally, we should create a sqlite3 database for where we will store the data what we will scrape. For that, we shall use the `sqlite3` python module.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-summary=\"Import the SQLite3 moule then create an empty database called chessgames.\"}\nimport sqlite3\nconn = sqlite3.connect('chessgames.db')\n```\n:::\n\n\nNext, since we will be doing some web scraping, which may want to set our proxy and headers. A proxy server can help a scraper avoid IP address blocking, access geographically restricted content, facilitate high-volume scraping, and avoid detection. Headers in web scraping are a part of the HTTP request that provides information about the client making the request. They are important because they can affect the response received from the server. Some websites may block or restrict access to content based on the header information. To avoid being detected as a bot or being blocked by the server, it is important to ensure that the headers used in web scraping are appropriate and mimic those of a real user.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-summary=\"Edit our Proxy and Heading\"}\n# Setting of proxy\nos.environ['HTTP_PROXY'] = 'http://13.115.147.132:8080'\nos.environ['HTTPS_PROXY'] = 'http://13.115.147.132:8080'\n\n# Setting of header\nheader = {'''accept: text/html,application/xhtml+xml,application/xml;q=0.9,\n            image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\n            accept-Encoding: gzip, deflate\n            accept-Language: en-US,en;q=0.9\n            cache-Control: max-age=0\n            connection: keep-alive\n            host: www.chessgames.com\n            referer: http://www.chessgames.com/perl/chess.pl?page=16&pid=14125\n            &playercomp=either&year=2010&yearcomp=ge\n            upgrade-Insecure-Requests: 1\n            user-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \n            (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'''}\n```\n:::\n\n\n<h3>Step 1: Understand the Data and Conduct Web Scraping Tool</h3>\n\nTo analyze the top opening moves of the highest-rated chess players, web data extraction was performed on the pages of [ChessGames.com](ChessGames.com). We focused on two entities, namely the chess players (`players`) and their games (`games`). Data for both entities were saved to an SQLlite database (`chessgames.db`). \n\nThe `players` table was obtained by scraping all web pages under the Chess Players Directory [http://www.chessgames.com/directory/](http://www.chessgames.com/directory/). Each page in this directory corresponds to a letter of the alphabet (e.g.,  [http://www.chessgames.com/directory/A](http://www.chessgames.com/directory/A). Each player has the following information:\n\n<center> Table 1: `player` table fields </center>\n| Field       | Description                   | Long Description                        | Data Type |\n|-------------|-------------------------------|-----------------------------------------|-----------|\n| pid         | Player ID                     | indentification number                  | integer   |\n| lname       | Last Name                     | player's last name                      | varchar   |\n| fname       | First Name                    | player's first name                     | varchar   |\n| rating      | Rating                        | highest rating achieved in the database | integer   |\n| start_year  | Start Year                    | player's starting year                  | integer   |\n| end_year    | End Year                      | player's ending year                    | integer   |\n| game_count  | Number of games               | number of games in database             | integer   |\n\nThe `get_player_info` method was used to scrape the player information (seen below).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-summary=\"Function description for get_player_info()\"}\ndef get_player_info(url):\n    '''\n    Scrapes player info from the specified URL.\n\n    Parameter\n    ---------\n    URL : URL of player page\n\n    Return\n    ------\n    list of tuples (pid, lname, fname, rating, start_year, end_year, \n    game_count)\n    '''\n    players_list = []\n    resp = requests.get(url, headers=headers)\n    time.sleep(1)\n    print(\"\\tStatus code: \", resp.status_code)\n    resp_soup = BeautifulSoup(resp.text, 'lxml')\n    players = resp_soup.select('tr[bgcolor=\"#FFFFFF\"],tr[bgcolor=\"#FFEEDD\"]')\n    for player in players:\n        data = player.select('td')\n        rating = data[0].text.strip()\n        name = data[2].text.split(',')\n        if len(name) == 2:\n            fname = name[1].strip()\n        else:\n            fname = None\n        lname = name[0].strip()\n        years = data[3].text.strip()\n        game_count = data[4].text.strip()\n        start_year = re.match('(\\d{4})?-?(\\d{4})', years).group(1)\n        end_year = re.match('(\\d{4})?-?(\\d{4})', years).group(2)\n\n        url = str(player.select('a')[-1])\n        pid = re.match('.*?pid=(\\d+)', url).group(1)\n\n        tup = (pid, lname, fname, rating, start_year, end_year, game_count)\n        print('\\t', tup)\n        players_list.append(tup)\n    return players_list\n```\n:::\n\n\nFor the `get_player_info` function, the `Requests` module is a Python library used for making HTTP requests. We can use basic methods such as the `GET`, `POST`, `PUT`, `DELETE`, and others. The module also provides support for handling cookies, adding custom headers, and handling redirects. We also used the `BeautifulSoup` Python library, which is a typical package in parsing HTML and XML documents. The package parses the pased HTML source code into a parsed tree, which can be easily traversed. Finally, we used the `re` Python package to utilize `Regular Expression` for easier string matching.\n\n<br>\nNext, the players data are then inserted to the `players` table using the `insert_players` method:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-summary=\"Function description for insert_players()\"}\ndef insert_players(conn):\n    '''\n    Inserts players into the players table\n\n    Parameter\n    ---------\n    conn : sqlite connection\n    '''\n    cur = conn.cursor()\n    for char in string.ascii_uppercase:\n        url = \"http://www.chessgames.com/directory/\" + char + \".html\"\n        print(url)\n        players = get_player_info(url)\n        cur.executemany('''INSERT INTO players \n                            VALUES (?, ?, ?, ?, ?, ?, ?);''', players)\n        conn.commit()\n```\n:::\n\n\nHere, we access the enter the created database, then Insert the players and associated metadata into the database.\n\n<br>\nNext, this study will focus on the games of the thirty (30) highest-rated players. The ranking was based on the rating provided by the website. To extract the games of these players, we first obtained their player IDs (`pid`) by using the `pandas` method `read_sql`. Here we can pass a `SQL` statement:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-summary=\"Method to query the database\"}\ndf_players = pd.read_sql(\"\"\"SELECT pid, fname || ' ' || lname, rating, \n                                    game_count \n                            FROM players\n                            WHERE rating != ''\n                            ORDER BY rating DESC\"\"\", conn)\ndf_players.columns = ['Player ID', 'Name', 'Rating', 'Number of Games']\ndf_players.head(30)\n```\n:::\n\n\n|    Player ID |                     Name | Rating | Number of Games |\n|-------------:|----------------|-------:|----------------:|\n|        52948 |           Magnus CARLSEN |   2882 |           3,016 |\n|        15940 |           Garry KASPAROV |   2851 |           2,385 |\n|        76172 |         Fabiano CARUANA |   2844 |           1,891 |\n|        17316 |           Levon ARONIAN |   2830 |           2,708 |\n|        95915 |                 Wesley SO |   2822 |           1,400 |\n|        56798 | Maxime VACHIER-LAGRAVE |   2819 |           2,369 |\n|        12088 |         Viswanathan ANAND |   2817 |           3,542 |\n|        12295 |        Vladimir KRAMNIK |   2817 |           3,026 |\n|        12089 |           Veselin TOPALOV |   2816 |           2,278 |\n|        50065 |   Shakhriyar MAMEDYAROV |   2814 |           2,254 |\n|        10084 |          Hikaru NAKAMURA |   2814 |           2,424 |\n|        17279 |      Alexander GRISCHUK |   2797 |           2,586 |\n|        52629 |               Ding LIREN |   2797 |             920 |\n|       107252 |              Anish GIRI |   2793 |           1,522 |\n|        49796 |         Teimour RADJABOV |   2793 |           1,746 |\n|        54535 |         Sergey KARJAKIN |   2788 |           2,399 |\n|        11719 |  Alexander MOROZEVICH |   2788 |           1,847 |\n|        12183 |         Vassily IVANCHUK |   2787 |           3,752 |\n|        19233 |      Robert James FISCHER |   2785 |           1,052 |\n|        20719 |           Anatoly KARPOV |   2780 |           3,609 |\n|        13847 |            Boris GELFAND |   2777 |           3,014 |\n|        79968 |            Peter SVIDLER |   2769 |           2,786 |\n|        49080 |  Leinier Dominguez PEREZ |   2768 |           1,342 |\n|        12109 |         Ruslan PONOMARIOV |   2768 |           1,989 |\n|        54683 |      Ian NEPOMNIACHTCHI |   2767 |           1,614 |\n|        49246 |     Pentala HARIKRISHNA |   2766 |           1,442 |\n|        49456 |            Pavel ELJANOV |   2765 |           1,409 |\n|        15874 |               Gata KAMSKY |   2763 |           1,889 |\n|        12290 |                Peter LEKO |   2763 |           2,364 |\n|       112240 |                Yu YANGYI |   2762 |             991 |\n: Table 2: Top 30 chess players based on Rating\n\nThe `games` table was then populated by web scraping <small>`http://www.chessgames.com/perl/chessplayer?pid=<pid>`</small> and iterating over the top 30 player IDs. The following fields were extracted for each game:\n\n\n| Field         | Description        | Data Type |\n|---------------|--------------------|-----------|\n| gid           | Game ID            | integer   |\n| white_pid     | White Player ID    | int       |\n| black_pid     | Black Player ID    | int       |\n| result        | Result             | varchar   |\n| moves         | Number of moves    | integer   |\n| year          | Year               | integer   |\n| tournament    | Tournament Name    | varchar   |\n| eco           | Encyclopaedia of Chess Openings  | varchar   |\n| opening_move  | Opening move       | varchar   |\n: Table 3: `games` table fields \n\nThe `get_players_games` function was implemented to scrape the game data for a given Player ID `pid` and page number `page_start`. This writes the games data of the player to a CSV file (`<pid>.csv`):\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-summary=\"Function description for the get_players_games()\"}\ndef get_player_games(pid, page_start):\n    \"\"\"\n    Web scrapes the games list for a player and writes it to a CSV \n\n    Parameters:\n    -----------\n    pid : player ID\n    page_start : starting page\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    url = 'http://www.chessgames.com/perl/chessplayer?pid=' + str(pid)\n    resp = requests.get(url, headers=headers)\n    print('pid = ', pid, '\\turl = ', url, '\\tcode = ', resp.status_code)\n    time.sleep(np.random.randint(1, 2))\n    soup = BeautifulSoup(resp.text, 'lxml')\n    div_page_count = soup.select(\n        'td[background$=\"/chessimages/table_stripes.gif\"]')\n    page_count = int(re.findall('of (\\d+)\\;', div_page_count[0].text)[0])\n\n    with open(f'{pid}.csv', 'a') as file:\n        csv_writer = csv.writer(file, delimiter=',', quotechar='\"')\n        for page in range(page_start, page_count+1):\n            page_url = 'http://www.chessgames.com/perl/chess.pl?page=' + \\\n                str(page) + '&pid=' + str(pid)\n            page_resp = requests.get(page_url, headers=headers)\n            print('\\tpage = ', page, '\\turl = ', page_url,\n                  '\\tcode = ', page_resp.status_code)\n            time.sleep(np.random.randint(1, 2))\n            page_soup = BeautifulSoup(page_resp.text, 'lxml')\n            games = page_soup.select(\n                'tr[bgcolor=\"#FFFFFF\"],tr[bgcolor=\"#EEDDCC\"]')\n\n            for game in games:\n                data = game.select('td')\n                game_url = data[0].find(\"a\")['href']\n                game_id = re.findall('(\\d+)', game_url)[0]\n                result = data[2].text.strip()\n                moves = data[3].text.strip()\n                year = data[4].text.strip()\n                tournament = data[5].text.strip()\n                eco = data[6].select('a')[0].text.strip()\n                opening_move = re.findall(\n                    '^[A-E0-9][0-9]{2} (.*)', data[6].text.strip())[0]\n\n                game_resp = requests.get(\n                    'http://www.chessgames.com' + game_url, headers=headers)\n                time.sleep(np.random.randint(1, 2))\n                game_soup = BeautifulSoup(game_resp.text, 'lxml')\n                players = game_soup.select('center')[0].select('a')\n                try:\n                    white_id = re.findall('(\\d+)', players[0]['href'])[0]\n                except:\n                    white_id = None\n                try:\n                    black_id = re.findall('(\\d+)', players[1]['href'])[0]\n                except:\n                    black_id = None\n                tup = (game_id, white_id, black_id, result,\n                       moves, year, tournament, eco, opening_move)\n\n                try:\n                    csv_writer.writerow(tup)\n                except:\n                    print('\\t\\tgameID: ', game_id, '\\tWrite to CSV failed')\n```\n:::\n\n\nAll player csv files were then inserted to the `games` table of the `chessgames.db` database using the `read_csvs` function:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-summary=\"Function description for read_csvs()\"}\ndef read_csvs(conn):\n    \"\"\"\n    Read all player csvs and save them to the games table\n    \"\"\"\n    gid_failed = []\n    cur = conn.cursor()\n    for file_name in glob.glob('./games/*.csv'):\n        with open(file_name) as file:\n            print(file_name)\n            reader = csv.reader(file, delimiter = ',')\n            for line in reader:\n                try:\n                    cur.execute('''INSERT INTO games VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);''', line)\n                    print('\\t\\tgameID: ', line[0], '\\tInsert success!')\n                except Exception as e:\n                    gid_failed.append(line[0])\n                    print('\\t\\tgameID: ', line[0], '\\tInsert Failed!\\t', e)\n    return gid_failed\n```\n:::\n\n\nNow, let us examine the games dataframe by calling it with the `read_sql` Pandas module.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-summary=\"Accessing the database.\"}\ndf_games = pd.read_sql(\"\"\"SELECT * FROM games\"\"\", conn)\ndf_games.columns = (['Game ID', 'White Player ID', 'Black Player ID',\n                     'Result', 'Number of Moves', 'Year', 'Tournament', 'ECO',\n                     'Opening Move'])\n```\n:::\n\n\nBelow you can see that **Table 4** and **Table 5** shows the summary statistics for the `players` and `games` table. **8,574 players** and **50,087** games were inserted into the chessgames database.\n\n\n|Statistic | Rating |\n|---------|-----------|\n|count\t|8574.000000|\n|mean\t|2361.279566|\n|std\t|184.344627|\n|min\t|1379.000000|\n|25%\t|2268.000000|\n|50%\t|2389.000000|\n|75%\t|2479.000000|\n|max\t|2882.000000|\n\n: Table 4: `players` table summary statistics\n\n| Statistic |\tNumber of Moves |\tYear |\t\n|---------|-----------|---------|\n| count\t| 50087.000000\t| \t50087.000000 |\n| mean\t| \t42.893485\t| \t2005.074950 |\n| std\t| 17.288031\t| \t11.643902 |\n| min\t| 0.000000\t| \t1953.000000 |\n| 25%\t| 31.000000\t| \t2000.000000 |\n| 50%\t| 41.000000\t| \t2008.000000 |\n| 75%\t| \t52.000000\t| \t2014.000000 |\n| max\t| \t255.000000\t| \t2019.000000 |\n\n: Table 5: `games` table summary statistics\n\n<h2>Results</h2>\n\n**Figure 1** below shows the top 10 opening moves used by the thirty highest-ranked chess players. 356 games, or nearly 2.6% of the games won by the top players, were opened using the Sicilian, Najdorf (ECO = B90) move. \n\nHowever, looking at the top opening moves per player as seen in **Figure 3**, the Sicilian, Najdorf (ECO = B90) move does not appear as the top move for most of the players. For instance, Magnus Carlsen, the highest-ranked player, has the Ruy Lopez, Berlin Defense as his top opening move. As shown in **Table 7**, Only 7 of the top 30 players have the Sicilian, Najdorf move as their top winning opening move. Also, if we look at the top 5 highest-rated players, 3 of 5 of them have the Ruy Lopez, Berlin Defense move as the top winning move.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-summary=\"Function to see top 10 opening moves and % won.\"}\ndf_top_moves = pd.read_sql(\"\"\"SELECT g.white_pid, p.fname || ' ' || \n                            p.lname, p.rating, g.eco, g.opening_move, count(*) \n                            FROM games g\n                          INNER JOIN players p on g.white_pid = p.pid\n                          WHERE g.white_pid IN (SELECT pid FROM players \n                                              WHERE rating != ''\n                                              ORDER BY rating DESC limit 30)\n                          AND g.result = '1-0'\n                          GROUP BY g.white_pid, g.eco, g.opening_move\n                          ORDER BY p.rating DESC, count(*) DESC\"\"\", conn)\ndf_top_moves.columns = ['PID', 'Name', 'Rating',\n                        'ECO', 'Top Opening Move', 'Number of Games Won']\ndf_top_moves = df_top_moves.groupby(['ECO', 'Top Opening Move'])[\n    'Number of Games Won'].sum().to_frame()\ndf_top_moves = df_top_moves.sort_values('Number of Games Won', ascending=False)\ndf_top_moves.reset_index(inplace=True)\ndf_top_moves['% of Games Won'] = (\n    df_top_moves['Number of Games Won'] / df_top_moves['Number of Games Won'].sum() * 100)\ndf_top_moves.head(10)\n```\n:::\n\n\n|   ECO | Top Opening Move           |   Number of Games Won |   % of Games Won |\n|-------|----------------------------|----------------------|------------------|\n| B90   | Sicilian, Najdorf          |                  356 |         2.566321 |\n| A07   | King's Indian Attack       |                  250 |         1.802191 |\n| B12   | Caro-Kann Defense          |                  227 |         1.63639  |\n| C78   | Ruy Lopez                   |                  206 |         1.485006 |\n| C42   | Petrov Defense             |                  205 |         1.477797 |\n| C11   | French                     |                  204 |         1.470588 |\n| E15   | Queen's Indian             |                  203 |         1.463379 |\n| A04   | Reti Opening               |                  192 |         1.384083 |\n| D37   | Queen's Gambit Declined    |                  186 |         1.34083  |\n| C65   | Ruy Lopez, Berlin Defense  |                  176 |         1.268743 |\n\n: Table 6: Top Opening Moves of the 30 highest-rated players\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-summary=\"Code to generate the Top Opening Moves.\"}\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(dpi=150)\ndf_top_moves[10::-1].plot.barh('Top Opening Move',\n                               '% of Games Won', ax=ax, color='#BF5209',\n                               legend=False)\nax.set_xlabel('% of Games Won')\n```\n:::\n\n\n<!-- ![FIGURE 1:Top Opening Moves of the 30 highest-rated chess players](image-outputs/image1-topmoves.png) -->\n\nNow, let us view the top opening moves per top player (top 10 players)\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-summary=\"Code to generate the Top Opening Moves for top players\"}\ndf_top_moves = pd.read_sql(\"\"\"SELECT g.white_pid, p.fname || ' ' || \n                            p.lname, p.rating, g.eco, g.opening_move, count(*) \n                            FROM games g\n                          INNER JOIN players p on g.white_pid = p.pid\n                          WHERE g.white_pid IN (SELECT pid FROM players \n                                              WHERE rating != ''\n                                              ORDER BY rating DESC limit 30)\n                          AND g.result = '1-0'\n                          GROUP BY g.white_pid, g.eco\n                          ORDER BY p.rating DESC, count(*) DESC\"\"\", conn)\ndf_top_moves.columns = ['PID', 'Name', 'Rating', 'ECO', 'Top Opening Move', 'Number of Games Won']\ndf_top_moves = (df_top_moves\n                .groupby(['Rating', 'PID', 'Name', 'ECO', 'Top Opening Move'])\n                ['Number of Games Won'].sum().to_frame())\ndf_top_moves = (df_top_moves.reset_index()\n                .sort_values(['Rating', 'Number of Games Won'],\n                             ascending=[False, False])\n                .set_index(['PID', 'Rating', 'Name', 'ECO', 'Top Opening Move']))\ndf_top_moves = df_top_moves.groupby(level=0).head(10)\ndf_top_moves.reset_index(inplace=True)\ndf_top_moves\n\nnames = df_top_moves['Name'].unique()\n\nfor name in names:\n    fig, ax = plt.subplots()\n    df = df_top_moves[df_top_moves['Name'] == name]\n    df[10::-1].plot.barh('Top Opening Move', 'Number of Games Won',\n                         ax=ax, color='#BF5209', legend=False)\n    ax.set_title(name)\n    ax.set_xlabel('Number of Games Won')\n```\n:::\n\n\n:::  {#fig-top layout-ncol=1}\n\nTop Winning Moves Per Top Player\n:::\n\n<h2>Conclusions and Recommendations</h2>\n\nThe clustering results show that **Reddit is a very American-centric platform**. Majority of the topics are related to the 2016 U.S. Presidential Election, specifically on the Republican nominee Donald Trump, Democrat candidates Hillary Clinton, and the Iowa caucuses. \n\nAlso, the results show that the two primary motivations of users to post in Reddit is (a) to discuss politics, and (b) to seek advise/ask help from other Redditors, whether technological, legal, or culinary in nature.\n\nIncreasing the number of clusters also reveal more specific topics such as 'technical support' and ''Vines reaction compilation videos'.\n\nFor future studies, it is suggested to perform hierarchical clustering.\n\n<h2>References</h2>\n<i>\n<ul>\n<li> [All the news](https://towardsdatascience.com/all-the-news-17fa34b52b9d)\n<li> [Clustering text documents using k-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n<li> [Latent Semantic Analysis](http://www.scholarpedia.org/article/Latent_semantic_analysis)\n</ul>\n</i>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}