{
  "hash": "a3f36841f466f81e6732e7b5c1798bef",
  "result": {
    "markdown": "---\ntitle: Unraveling Subreddits\ndescription: 'Identifying underlying themes in Reddit titles using non-supervised clustering '\ndate: '2022-04-20'\ncategories:\n  - python\n  - EDA\n  - SQL\n  - analysis\nimage: thumbnail.jpeg\nformat:\n  html:\n    code-fold: true\n    css: custom-styles.scss\neditor:\n  render-on-save: true\ndraft: true\n---\n\n![](thumbnail.jpeg)\n\n<h1>Executive Summary</h1>\n<div class='executive-summary'>\n\nOriginal creation and submission for this notebook was last June 2019.\n\nReddit (https://www.reddit.com/) is an American discussion and aggregation website for user-generated content with more than 500 million monthly visitors. We uncovered the underlying categories or \"subreddits\" by performing representative-based clustering on a sample of Reddit titles. Results reveal that Reddit is a very American-centric platform and that the two main themes are (a) U.S. politics, and (b) seeking help from other Redditors on a wide range of topics including technology, food, and legal matters. For a small number of clusters ($n=10$), the high-level categories that emerged include the 2016 U.S. Presidential Elections (Donald Trump, Hillary Clinton, and Bernie Sanders), food, legal concerns, and \"New year\"-related subreddits. Increasing the number of clusters to $n=16$ revealed more specific subreddits such as technical support, recipes, apartment/rental concerns, and reaction compilations.\n</div>\n\n<h2>Acknowledgements</h2>\n\nThis analysis was done together with my Lab partner, George Esleta, and Cohortmates - Gilbert Chua, Nigel Silva and Oonre Advincula-Go. \n\n<h2> A. Introduction and the Problem Statement </h2>\n\nChess is a two player strategy game played on a checked 8 by 8 board. The 8 by 8 grid consists of 64 squares where each player is located at opposite ends of the checked board. At the beginning of the game, each player assigned a color of either Black or White. The color scheme indicates which player will move first, with the player with the white pieces moving first. Moves are alternating between players, with each player allowed to only move one chess piece each round.\n\nEach player is given six unique types of pieces with varying numbers, with each unique piece moving differently across the board. Pieces may move to squares occupied by another chess piece. If the player moves into a square occupied by an opponent’s piece, then the opponent’s piece is captured and taken off the chessboard. The purpose of the game is to capture the opponent’s king, signaling a checkmate. Each game is then concluded with either of three scenarios: \"1–0\" means White won, \"0–1\" means Black won, and \"½–½\" indicates a draw (also known as a stalemate).\n\nThe first moves in any chess game, also known as a player’s opening move, are the most crucial as it sets the tone, foundation and area of control of the players. The rationale behind this rests on the assumption that the first few moves will influence a player’s probability of winning. The objectives of the opening moves allow each player needs to gain dominance of the grid by: (1) getting most of their pieces out of their default positions as this allows more possible moves), and (2) getting control of the centre of the board (as this allows better dominance).\n\nWith the assumption the player’s opening moves may play influential role to the player’s grid control and outcome, the paper will explore the top ranked players opening moves. The purpose of this paper is to determine if there is a specific opening move that each of the top-ranking players use.\n\n<h2> B. Methodology</h2>\n\n<h3>Pre-requisites: Load Requirement Package</h3>\n\nBefore anything else, let us first load all important modules for this exercise.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-summary=\"Loading required modules\"}\n# These are the standard imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n\n%matplotlib inline\n\n# This is to allow a code fold in jupyter notebook\nfrom IPython.display import HTML\nHTML('''<script>\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n\n  $( document ).ready(function(){\n    code_shown=false;\n    $('div.input').hide()\n  });\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>''')\n```\n:::\n\n\nAdditionally, we should create a sqlite3 database for where we will store the data what we will scrape. For that, we shall use the `sqlite3` python module.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-summary=\"Import the SQLite3 moule then create an empty database called chessgames.\"}\nimport sqlite3\nconn = sqlite3.connect('chessgames.db')\n```\n:::\n\n\nNext, since we will be doing some web scraping, which may want to set our proxy and headers. A proxy server can help a scraper avoid IP address blocking, access geographically restricted content, facilitate high-volume scraping, and avoid detection. Headers in web scraping are a part of the HTTP request that provides information about the client making the request. They are important because they can affect the response received from the server. Some websites may block or restrict access to content based on the header information. To avoid being detected as a bot or being blocked by the server, it is important to ensure that the headers used in web scraping are appropriate and mimic those of a real user.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-summary=\"Edit our Proxy and Heading\"}\n# Setting of proxy\nos.environ['HTTP_PROXY'] = 'http://13.115.147.132:8080'\nos.environ['HTTPS_PROXY'] = 'http://13.115.147.132:8080'\n\n# Setting of header\nheader = {'''accept: text/html,application/xhtml+xml,application/xml;q=0.9,\n            image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\n            accept-Encoding: gzip, deflate\n            accept-Language: en-US,en;q=0.9\n            cache-Control: max-age=0\n            connection: keep-alive\n            host: www.chessgames.com\n            referer: http://www.chessgames.com/perl/chess.pl?page=16&pid=14125\n            &playercomp=either&year=2010&yearcomp=ge\n            upgrade-Insecure-Requests: 1\n            user-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \n            (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'''}\n```\n:::\n\n\n<h3>Step 1: Understand the Data and Conduct Web Scraping Tool</h3>\n\nTo analyze the top opening moves of the highest-rated chess players, web data extraction was performed on the pages of [ChessGames.com](ChessGames.com). We focused on two entities, namely the chess players (`players`) and their games (`games`). Data for both entities were saved to an SQLlite database (`chessgames.db`). \n\nThe `players` table was obtained by scraping all web pages under the Chess Players Directory [http://www.chessgames.com/directory/](http://www.chessgames.com/directory/). Each page in this directory corresponds to a letter of the alphabet (e.g.,  [http://www.chessgames.com/directory/A](http://www.chessgames.com/directory/A). Each player has the following information:\n\n<center> Table 1: `player` table fields </center>\n| Field       | Description                   | Long Description                        | Data Type |\n|-------------|-------------------------------|-----------------------------------------|-----------|\n| pid         | Player ID                     | indentification number                  | integer   |\n| lname       | Last Name                     | player's last name                      | varchar   |\n| fname       | First Name                    | player's first name                     | varchar   |\n| rating      | Rating                        | highest rating achieved in the database | integer   |\n| start_year  | Start Year                    | player's starting year                  | integer   |\n| end_year    | End Year                      | player's ending year                    | integer   |\n| game_count  | Number of games               | number of games in database             | integer   |\n\nThe `get_player_info` method was used to scrape the player information (seen below).\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-summary=\"Function description for get_player_info()\"}\ndef get_player_info(url):\n    '''\n    Scrapes player info from the specified URL.\n\n    Parameter\n    ---------\n    URL : URL of player page\n\n    Return\n    ------\n    list of tuples (pid, lname, fname, rating, start_year, end_year, \n    game_count)\n    '''\n    players_list = []\n    resp = requests.get(url, headers=headers)\n    time.sleep(1)\n    print(\"\\tStatus code: \", resp.status_code)\n    resp_soup = BeautifulSoup(resp.text, 'lxml')\n    players = resp_soup.select('tr[bgcolor=\"#FFFFFF\"],tr[bgcolor=\"#FFEEDD\"]')\n    for player in players:\n        data = player.select('td')\n        rating = data[0].text.strip()\n        name = data[2].text.split(',')\n        if len(name) == 2:\n            fname = name[1].strip()\n        else:\n            fname = None\n        lname = name[0].strip()\n        years = data[3].text.strip()\n        game_count = data[4].text.strip()\n        start_year = re.match('(\\d{4})?-?(\\d{4})', years).group(1)\n        end_year = re.match('(\\d{4})?-?(\\d{4})', years).group(2)\n\n        url = str(player.select('a')[-1])\n        pid = re.match('.*?pid=(\\d+)', url).group(1)\n\n        tup = (pid, lname, fname, rating, start_year, end_year, game_count)\n        print('\\t', tup)\n        players_list.append(tup)\n    return players_list\n```\n:::\n\n\nFor the `get_player_info` function, the `Requests` module is a Python library used for making HTTP requests. We can use basic methods such as the `GET`, `POST`, `PUT`, `DELETE`, and others. The module also provides support for handling cookies, adding custom headers, and handling redirects. We also used the `BeautifulSoup` Python library, which is a typical package in parsing HTML and XML documents. The package parses the pased HTML source code into a parsed tree, which can be easily traversed. Finally, we used the `re` Python package to utilize `Regular Expression` for easier string matching.\n\n<br>\nNext, the players data are then inserted to the `players` table using the `insert_players` method:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-summary=\"Function description for insert_players()\"}\ndef insert_players(conn):\n    '''\n    Inserts players into the players table\n\n    Parameter\n    ---------\n    conn : sqlite connection\n    '''\n    cur = conn.cursor()\n    for char in string.ascii_uppercase:\n        url = \"http://www.chessgames.com/directory/\" + char + \".html\"\n        print(url)\n        players = get_player_info(url)\n        cur.executemany('''INSERT INTO players \n                            VALUES (?, ?, ?, ?, ?, ?, ?);''', players)\n        conn.commit()\n```\n:::\n\n\nHere, we access the enter the created database, then Insert the players and associated metadata into the database.\n\n<br>\nNext, this study will focus on the games of the thirty (30) highest-rated players. The ranking was based on the rating provided by the website. To extract the games of these players, we first obtained their player IDs (`pid`) by using the `pandas` method `read_sql`. Here we can pass a `SQL` statement:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-summary=\"Method to query the database\"}\ndf_players = pd.read_sql(\"\"\"SELECT pid, fname || ' ' || lname, rating, \n                                    game_count \n                            FROM players\n                            WHERE rating != ''\n                            ORDER BY rating DESC\"\"\", conn)\ndf_players.columns = ['Player ID', 'Name', 'Rating', 'Number of Games']\ndf_players.head(30)\n```\n:::\n\n\n|    Player ID |                     Name | Rating | Number of Games |\n|-------------:|----------------|-------:|----------------:|\n|        52948 |           Magnus CARLSEN |   2882 |           3,016 |\n|        15940 |           Garry KASPAROV |   2851 |           2,385 |\n|        76172 |         Fabiano CARUANA |   2844 |           1,891 |\n|        17316 |           Levon ARONIAN |   2830 |           2,708 |\n|        95915 |                 Wesley SO |   2822 |           1,400 |\n|        56798 | Maxime VACHIER-LAGRAVE |   2819 |           2,369 |\n|        12088 |         Viswanathan ANAND |   2817 |           3,542 |\n|        12295 |        Vladimir KRAMNIK |   2817 |           3,026 |\n|        12089 |           Veselin TOPALOV |   2816 |           2,278 |\n|        50065 |   Shakhriyar MAMEDYAROV |   2814 |           2,254 |\n|        10084 |          Hikaru NAKAMURA |   2814 |           2,424 |\n|        17279 |      Alexander GRISCHUK |   2797 |           2,586 |\n|        52629 |               Ding LIREN |   2797 |             920 |\n|       107252 |              Anish GIRI |   2793 |           1,522 |\n|        49796 |         Teimour RADJABOV |   2793 |           1,746 |\n|        54535 |         Sergey KARJAKIN |   2788 |           2,399 |\n|        11719 |  Alexander MOROZEVICH |   2788 |           1,847 |\n|        12183 |         Vassily IVANCHUK |   2787 |           3,752 |\n|        19233 |      Robert James FISCHER |   2785 |           1,052 |\n|        20719 |           Anatoly KARPOV |   2780 |           3,609 |\n|        13847 |            Boris GELFAND |   2777 |           3,014 |\n|        79968 |            Peter SVIDLER |   2769 |           2,786 |\n|        49080 |  Leinier Dominguez PEREZ |   2768 |           1,342 |\n|        12109 |         Ruslan PONOMARIOV |   2768 |           1,989 |\n|        54683 |      Ian NEPOMNIACHTCHI |   2767 |           1,614 |\n|        49246 |     Pentala HARIKRISHNA |   2766 |           1,442 |\n|        49456 |            Pavel ELJANOV |   2765 |           1,409 |\n|        15874 |               Gata KAMSKY |   2763 |           1,889 |\n|        12290 |                Peter LEKO |   2763 |           2,364 |\n|       112240 |                Yu YANGYI |   2762 |             991 |\n: Table 2: Top 30 chess players based on Rating\n\nThe `games` table was then populated by web scraping <small>`http://www.chessgames.com/perl/chessplayer?pid=<pid>`</small> and iterating over the top 30 player IDs. The following fields were extracted for each game:\n\n\n| Field         | Description        | Data Type |\n|---------------|--------------------|-----------|\n| gid           | Game ID            | integer   |\n| white_pid     | White Player ID    | int       |\n| black_pid     | Black Player ID    | int       |\n| result        | Result             | varchar   |\n| moves         | Number of moves    | integer   |\n| year          | Year               | integer   |\n| tournament    | Tournament Name    | varchar   |\n| eco           | Encyclopaedia of Chess Openings  | varchar   |\n| opening_move  | Opening move       | varchar   |\n: Table 3: `games` table fields \n\nThe `get_players_games` function was implemented to scrape the game data for a given Player ID `pid` and page number `page_start`. This writes the games data of the player to a CSV file (`<pid>.csv`):\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-summary=\"Function description for the get_players_games()\"}\ndef get_player_games(pid, page_start):\n    \"\"\"\n    Web scrapes the games list for a player and writes it to a CSV \n\n    Parameters:\n    -----------\n    pid : player ID\n    page_start : starting page\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    url = 'http://www.chessgames.com/perl/chessplayer?pid=' + str(pid)\n    resp = requests.get(url, headers=headers)\n    print('pid = ', pid, '\\turl = ', url, '\\tcode = ', resp.status_code)\n    time.sleep(np.random.randint(1, 2))\n    soup = BeautifulSoup(resp.text, 'lxml')\n    div_page_count = soup.select(\n        'td[background$=\"/chessimages/table_stripes.gif\"]')\n    page_count = int(re.findall('of (\\d+)\\;', div_page_count[0].text)[0])\n\n    with open(f'{pid}.csv', 'a') as file:\n        csv_writer = csv.writer(file, delimiter=',', quotechar='\"')\n        for page in range(page_start, page_count+1):\n            page_url = 'http://www.chessgames.com/perl/chess.pl?page=' + \\\n                str(page) + '&pid=' + str(pid)\n            page_resp = requests.get(page_url, headers=headers)\n            print('\\tpage = ', page, '\\turl = ', page_url,\n                  '\\tcode = ', page_resp.status_code)\n            time.sleep(np.random.randint(1, 2))\n            page_soup = BeautifulSoup(page_resp.text, 'lxml')\n            games = page_soup.select(\n                'tr[bgcolor=\"#FFFFFF\"],tr[bgcolor=\"#EEDDCC\"]')\n\n            for game in games:\n                data = game.select('td')\n                game_url = data[0].find(\"a\")['href']\n                game_id = re.findall('(\\d+)', game_url)[0]\n                result = data[2].text.strip()\n                moves = data[3].text.strip()\n                year = data[4].text.strip()\n                tournament = data[5].text.strip()\n                eco = data[6].select('a')[0].text.strip()\n                opening_move = re.findall(\n                    '^[A-E0-9][0-9]{2} (.*)', data[6].text.strip())[0]\n\n                game_resp = requests.get(\n                    'http://www.chessgames.com' + game_url, headers=headers)\n                time.sleep(np.random.randint(1, 2))\n                game_soup = BeautifulSoup(game_resp.text, 'lxml')\n                players = game_soup.select('center')[0].select('a')\n                try:\n                    white_id = re.findall('(\\d+)', players[0]['href'])[0]\n                except:\n                    white_id = None\n                try:\n                    black_id = re.findall('(\\d+)', players[1]['href'])[0]\n                except:\n                    black_id = None\n                tup = (game_id, white_id, black_id, result,\n                       moves, year, tournament, eco, opening_move)\n\n                try:\n                    csv_writer.writerow(tup)\n                except:\n                    print('\\t\\tgameID: ', game_id, '\\tWrite to CSV failed')\n```\n:::\n\n\nAll player csv files were then inserted to the `games` table of the `chessgames.db` database using the `read_csvs` function:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-summary=\"Function description for read_csvs()\"}\ndef read_csvs(conn):\n    \"\"\"\n    Read all player csvs and save them to the games table\n    \"\"\"\n    gid_failed = []\n    cur = conn.cursor()\n    for file_name in glob.glob('./games/*.csv'):\n        with open(file_name) as file:\n            print(file_name)\n            reader = csv.reader(file, delimiter = ',')\n            for line in reader:\n                try:\n                    cur.execute('''INSERT INTO games VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);''', line)\n                    print('\\t\\tgameID: ', line[0], '\\tInsert success!')\n                except Exception as e:\n                    gid_failed.append(line[0])\n                    print('\\t\\tgameID: ', line[0], '\\tInsert Failed!\\t', e)\n    return gid_failed\n```\n:::\n\n\nNow, let us examine the games dataframe by calling it with the `read_sql` Pandas module.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-summary=\"Accessing the database.\"}\ndf_games = pd.read_sql(\"\"\"SELECT * FROM games\"\"\", conn)\ndf_games.columns = (['Game ID', 'White Player ID', 'Black Player ID',\n                     'Result', 'Number of Moves', 'Year', 'Tournament', 'ECO',\n                     'Opening Move'])\n```\n:::\n\n\nBelow you can see that **Table 4** and **Table 5** shows the summary statistics for the `players` and `games` table. **8,574 players** and **50,087** games were inserted into the chessgames database.\n\n\n|Statistic | Rating |\n|---------|-----------|\n|count\t|8574.000000|\n|mean\t|2361.279566|\n|std\t|184.344627|\n|min\t|1379.000000|\n|25%\t|2268.000000|\n|50%\t|2389.000000|\n|75%\t|2479.000000|\n|max\t|2882.000000|\n\n: Table 4: `players` table summary statistics\n\n| Statistic |\tNumber of Moves |\tYear |\t\n|---------|-----------|---------|\n| count\t| 50087.000000\t| \t50087.000000 |\n| mean\t| \t42.893485\t| \t2005.074950 |\n| std\t| 17.288031\t| \t11.643902 |\n| min\t| 0.000000\t| \t1953.000000 |\n| 25%\t| 31.000000\t| \t2000.000000 |\n| 50%\t| 41.000000\t| \t2008.000000 |\n| 75%\t| \t52.000000\t| \t2014.000000 |\n| max\t| \t255.000000\t| \t2019.000000 |\n\n: Table 5: `games` table summary statistics\n\n<h2>Results</h2>\n\n**Figure 1** below shows the top 10 opening moves used by the thirty highest-ranked chess players. 356 games, or nearly 2.6% of the games won by the top players, were opened using the Sicilian, Najdorf (ECO = B90) move. \n\nHowever, looking at the top opening moves per player as seen in **Figure 3**, the Sicilian, Najdorf (ECO = B90) move does not appear as the top move for most of the players. For instance, Magnus Carlsen, the highest-ranked player, has the Ruy Lopez, Berlin Defense as his top opening move. As shown in **Table 7**, Only 7 of the top 30 players have the Sicilian, Najdorf move as their top winning opening move. Also, if we look at the top 5 highest-rated players, 3 of 5 of them have the Ruy Lopez, Berlin Defense move as the top winning move.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code code-summary=\"Function to see top 10 opening moves and % won.\"}\ndf_top_moves = pd.read_sql(\"\"\"SELECT g.white_pid, p.fname || ' ' || \n                            p.lname, p.rating, g.eco, g.opening_move, count(*) \n                            FROM games g\n                          INNER JOIN players p on g.white_pid = p.pid\n                          WHERE g.white_pid IN (SELECT pid FROM players \n                                              WHERE rating != ''\n                                              ORDER BY rating DESC limit 30)\n                          AND g.result = '1-0'\n                          GROUP BY g.white_pid, g.eco, g.opening_move\n                          ORDER BY p.rating DESC, count(*) DESC\"\"\", conn)\ndf_top_moves.columns = ['PID', 'Name', 'Rating',\n                        'ECO', 'Top Opening Move', 'Number of Games Won']\ndf_top_moves = df_top_moves.groupby(['ECO', 'Top Opening Move'])[\n    'Number of Games Won'].sum().to_frame()\ndf_top_moves = df_top_moves.sort_values('Number of Games Won', ascending=False)\ndf_top_moves.reset_index(inplace=True)\ndf_top_moves['% of Games Won'] = (\n    df_top_moves['Number of Games Won'] / df_top_moves['Number of Games Won'].sum() * 100)\ndf_top_moves.head(10)\n```\n:::\n\n\n|   ECO | Top Opening Move           |   Number of Games Won |   % of Games Won |\n|-------|----------------------------|----------------------|------------------|\n| B90   | Sicilian, Najdorf          |                  356 |         2.566321 |\n| A07   | King's Indian Attack       |                  250 |         1.802191 |\n| B12   | Caro-Kann Defense          |                  227 |         1.63639  |\n| C78   | Ruy Lopez                   |                  206 |         1.485006 |\n| C42   | Petrov Defense             |                  205 |         1.477797 |\n| C11   | French                     |                  204 |         1.470588 |\n| E15   | Queen's Indian             |                  203 |         1.463379 |\n| A04   | Reti Opening               |                  192 |         1.384083 |\n| D37   | Queen's Gambit Declined    |                  186 |         1.34083  |\n| C65   | Ruy Lopez, Berlin Defense  |                  176 |         1.268743 |\n\n: Table 6: Top Opening Moves of the 30 highest-rated players\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-summary=\"Code to generate the Top Opening Moves.\"}\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(dpi=150)\ndf_top_moves[10::-1].plot.barh('Top Opening Move',\n                               '% of Games Won', ax=ax, color='#BF5209',\n                               legend=False)\nax.set_xlabel('% of Games Won')\n```\n:::\n\n\n![FIGURE 1:Top Opening Moves of the 30 highest-rated chess players](image-outputs/image1-topmoves.png)\n\nNow, let us view the top opening moves per top player (top 10 players)\n\n::: {.cell execution_count=12}\n``` {.python .cell-code code-summary=\"Code to generate the Top Opening Moves for top players\"}\ndf_top_moves = pd.read_sql(\"\"\"SELECT g.white_pid, p.fname || ' ' || \n                            p.lname, p.rating, g.eco, g.opening_move, count(*) \n                            FROM games g\n                          INNER JOIN players p on g.white_pid = p.pid\n                          WHERE g.white_pid IN (SELECT pid FROM players \n                                              WHERE rating != ''\n                                              ORDER BY rating DESC limit 30)\n                          AND g.result = '1-0'\n                          GROUP BY g.white_pid, g.eco\n                          ORDER BY p.rating DESC, count(*) DESC\"\"\", conn)\ndf_top_moves.columns = ['PID', 'Name', 'Rating', 'ECO', 'Top Opening Move', 'Number of Games Won']\ndf_top_moves = (df_top_moves\n                .groupby(['Rating', 'PID', 'Name', 'ECO', 'Top Opening Move'])\n                ['Number of Games Won'].sum().to_frame())\ndf_top_moves = (df_top_moves.reset_index()\n                .sort_values(['Rating', 'Number of Games Won'],\n                             ascending=[False, False])\n                .set_index(['PID', 'Rating', 'Name', 'ECO', 'Top Opening Move']))\ndf_top_moves = df_top_moves.groupby(level=0).head(10)\ndf_top_moves.reset_index(inplace=True)\ndf_top_moves\n\nnames = df_top_moves['Name'].unique()\n\nfor name in names:\n    fig, ax = plt.subplots()\n    df = df_top_moves[df_top_moves['Name'] == name]\n    df[10::-1].plot.barh('Top Opening Move', 'Number of Games Won',\n                         ax=ax, color='#BF5209', legend=False)\n    ax.set_title(name)\n    ax.set_xlabel('Number of Games Won')\n```\n:::\n\n\n:::  {#fig-top layout-ncol=1}\n\n![Magnus Carlsen](image-outputs/1-carlsen.png){width=100%}\n![Garry Kasparov](image-outputs/2-kasparov.png){width=100%}\n\n![Fabiano Caruana](image-outputs/3-caruana.png){width=100%}\n![Levon Aronian](image-outputs/4-aronian.png){width=100%}\n\n![Wesley So](image-outputs/5-so.png){width=100%}\n![Maxmime Vachier-Lagrave](image-outputs/6-lagrave.png){width=100%}\n\n![Viswanathan Anand](image-outputs/7-anand.png){width=100%}\n![Vladimir Kramnik](image-outputs/8-kramnik.png){width=100%}\n\n![Veselin Topalov](image-outputs/9-topalov.png){width=100%}\n![Hikaru Nakamura](image-outputs/10-nakamura.png){width=100%}\n\nTop Winning Moves Per Top Player\n:::\n\n<h2>Conclusions and Recommendations</h2>\n\nThe clustering results show that **Reddit is a very American-centric platform**. Majority of the topics are related to the 2016 U.S. Presidential Election, specifically on the Republican nominee Donald Trump, Democrat candidates Hillary Clinton, and the Iowa caucuses. \n\nAlso, the results show that the two primary motivations of users to post in Reddit is (a) to discuss politics, and (b) to seek advise/ask help from other Redditors, whether technological, legal, or culinary in nature.\n\nIncreasing the number of clusters also reveal more specific topics such as 'technical support' and ''Vines reaction compilation videos'.\n\nFor future studies, it is suggested to perform hierarchical clustering.\n\n<h2>References</h2>\n<i>\n<ul>\n<li> [All the news](https://towardsdatascience.com/all-the-news-17fa34b52b9d)\n<li> [Clustering text documents using k-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n<li> [Latent Semantic Analysis](http://www.scholarpedia.org/article/Latent_semantic_analysis)\n</ul>\n</i>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}