{
  "hash": "c6af66be76bc7c280c62cff02e5bbf5e",
  "result": {
    "markdown": "---\ntitle: An analysis of the opening moves of the highest-ranked chess players\ndescription: 'Did you hear about the chess players who only used the Sicilian, Najdorf opening? It was their knight in shining armor! to victory'\ndate: '2022-04-17'\ncategories:\n  - python\n  - EDA\n  - SQL\n  - analysis\nimage: thumbnail.jpeg\nformat:\n  html:\n    code-fold: true\n    css: styles.css\neditor:\n  render-on-save: true\ndraft: true\n---\n\n<!-- <h4>Looks like voting in the Philippines is all about location, location, location! Literacy and religion may not be factors, but hometown pride definitely is.</h4> -->\n![](thumbnail.jpeg)\n\n<h1>Executive Summary</h1>\n\nOriginal creation and submission for this notebook was last May 29, 2019.\n\nThis jupyter notebook was created for my Data Mining and Wrangling class for my Masters in Data Science program in the Asian Institute of Management. In particular, this was done during the 2nd semester for a class - Data Mining and Wrangling - as one of the core requirements. In this report, we shall do an analysis on opening chess moves through exploratory data analysis. In this exercise, we extracted the data from [ChessGames.com](ChessGames.com) using the modules `Beautifulsoup`, `requests`, and `re` Python modules, then stored the data in an sqlite database. Results show that the Sicilian, Najdorf (ECO = B90) opening was used in nearly 2.6% of the total games won by the top players. However, looking at the individual players, there is no predominant opening move, with only 7 of the 30 players having the Sicilian, Najdorf move as their top winning opening move.\n\n\n<h2>Acknowledgements</h2>\n\nThis analysis was done together with my Lab partner, George Esleta. I would like to thank Prof Christian Alis and the ACCeSS Laboratory for the access to the high-performance computing facility.\n\n<h2> A. Introcution and the Problem Statement </h2>\n\nChess is a two player strategy game played on a checked 8 by 8 board. The 8 by 8 grid consists of 64 squares where each player is located at opposite ends of the checked board. At the beginning of the game, each player assigned a color of either Black or White. The color scheme indicates which player will move first, with the player with the white pieces moving first. Moves are alternating between players, with each player allowed to only move one chess piece each round.\n\nEach player is given six unique types of pieces with varying numbers, with each unique piece moving differently across the board. Pieces may move to squares occupied by another chess piece. If the player moves into a square occupied by an opponent’s piece, then the opponent’s piece is captured and taken off the chessboard. The purpose of the game is to capture the opponent’s king, signaling a checkmate. Each game is then concluded with either of three scenarios: \"1–0\" means White won, \"0–1\" means Black won, and \"½–½\" indicates a draw (also known as a stalemate).\n\nThe first moves in any chess game, also known as a player’s opening move, are the most crucial as it sets the tone, foundation and area of control of the players. The rationale behind this rests on the assumption that the first few moves will influence a player’s probability of winning. The objectives of the opening moves allow each player needs to gain dominance of the grid by: (1) getting most of their pieces out of their default positions as this allows more possible moves), and (2) getting control of the centre of the board (as this allows better dominance).\n\nWith the assumption the player’s opening moves may play influential role to the player’s grid control and outcome, the paper will explore the top ranked players opening moves. The purpose of this paper is to determine if there is a specific opening move that each of the top-ranking players use.\n\n<h2> B. Methodology</h2>\n\n<h3>Pre-requisites: Load Requirement Package</h3>\n\nBefore anything else, let us first load all important modules for this exercise.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-summary=\"Loading required modules\"}\n# These are the standard imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)\n\n%matplotlib inline\n\n# This is to allow a code fold in jupyter notebook\nfrom IPython.display import HTML\nHTML('''<script>\n  function code_toggle() {\n    if (code_shown){\n      $('div.input').hide('500');\n      $('#toggleButton').val('Show Code')\n    } else {\n      $('div.input').show('500');\n      $('#toggleButton').val('Hide Code')\n    }\n    code_shown = !code_shown\n  }\n\n  $( document ).ready(function(){\n    code_shown=false;\n    $('div.input').hide()\n  });\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>''')\n```\n:::\n\n\nAdditionally, we should create a sqlite3 database for where we will store the data what we will scrape. For that, we shall use the `sqlite3` python module.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-summary=\"Import the SQLite3 moule then create an empty database called chessgames.\"}\nimport sqlite3\nconn = sqlite3.connect('chessgames.db')\n```\n:::\n\n\nNext, since we will be doing some web scraping, which may want to set our proxy and headers. A proxy server can help a scraper avoid IP address blocking, access geographically restricted content, facilitate high-volume scraping, and avoid detection. Headers in web scraping are a part of the HTTP request that provides information about the client making the request. They are important because they can affect the response received from the server. Some websites may block or restrict access to content based on the header information. To avoid being detected as a bot or being blocked by the server, it is important to ensure that the headers used in web scraping are appropriate and mimic those of a real user.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-summary=\"Edit our Proxy and Heading\"}\n# Setting of proxy\nos.environ['HTTP_PROXY'] = 'http://13.115.147.132:8080'\nos.environ['HTTPS_PROXY'] = 'http://13.115.147.132:8080'\n\n# Setting of header\nheader = {'''accept: text/html,application/xhtml+xml,application/xml;q=0.9,\n            image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\n            accept-Encoding: gzip, deflate\n            accept-Language: en-US,en;q=0.9\n            cache-Control: max-age=0\n            connection: keep-alive\n            host: www.chessgames.com\n            referer: http://www.chessgames.com/perl/chess.pl?page=16&pid=14125\n            &playercomp=either&year=2010&yearcomp=ge\n            upgrade-Insecure-Requests: 1\n            user-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \n            (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'''}\n```\n:::\n\n\n<h3>Step 1: Understand the Data and Conduct Web Scraping Tool</h3>\n\nTo analyze the top opening moves of the highest-rated chess players, web data extraction was performed on the pages of [ChessGames.com](ChessGames.com). We focused on two entities, namely the chess players (`players`) and their games (`games`). Data for both entities were saved to an SQLlite database (`chessgames.db`). \n\nThe `players` table was obtained by scraping all web pages under the Chess Players Directory [http://www.chessgames.com/directory/](http://www.chessgames.com/directory/). Each page in this directory corresponds to a letter of the alphabet (e.g.,  [http://www.chessgames.com/directory/A](http://www.chessgames.com/directory/A). Each player has the following information:\n\n<center> Table 1: `player` table fields </center>\n| Field       | Description                   | Long Description                        | Data Type |\n|-------------|-------------------------------|-----------------------------------------|-----------|\n| pid         | Player ID                     | indentification number                  | integer   |\n| lname       | Last Name                     | player's last name                      | varchar   |\n| fname       | First Name                    | player's first name                     | varchar   |\n| rating      | Rating                        | highest rating achieved in the database | integer   |\n| start_year  | Start Year                    | player's starting year                  | integer   |\n| end_year    | End Year                      | player's ending year                    | integer   |\n| game_count  | Number of games               | number of games in database             | integer   |\n\nThe `get_player_info` method was used to scrape the player information (seen below).\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-summary=\"Function description for get_player_info()\"}\ndef get_player_info(url):\n    '''\n    Scrapes player info from the specified URL.\n\n    Parameter\n    ---------\n    URL : URL of player page\n\n    Return\n    ------\n    list of tuples (pid, lname, fname, rating, start_year, end_year, \n    game_count)\n    '''\n    players_list = []\n    resp = requests.get(url, headers=headers)\n    time.sleep(1)\n    print(\"\\tStatus code: \", resp.status_code)\n    resp_soup = BeautifulSoup(resp.text, 'lxml')\n    players = resp_soup.select('tr[bgcolor=\"#FFFFFF\"],tr[bgcolor=\"#FFEEDD\"]')\n    for player in players:\n        data = player.select('td')\n        rating = data[0].text.strip()\n        name = data[2].text.split(',')\n        if len(name) == 2:\n            fname = name[1].strip()\n        else:\n            fname = None\n        lname = name[0].strip()\n        years = data[3].text.strip()\n        game_count = data[4].text.strip()\n        start_year = re.match('(\\d{4})?-?(\\d{4})', years).group(1)\n        end_year = re.match('(\\d{4})?-?(\\d{4})', years).group(2)\n\n        url = str(player.select('a')[-1])\n        pid = re.match('.*?pid=(\\d+)', url).group(1)\n\n        tup = (pid, lname, fname, rating, start_year, end_year, game_count)\n        print('\\t', tup)\n        players_list.append(tup)\n    return players_list\n```\n:::\n\n\nFor the `get_player_info` function, the `Requests` module is a Python library used for making HTTP requests. We can use basic methods such as the `GET`, `POST`, `PUT`, `DELETE`, and others. The module also provides support for handling cookies, adding custom headers, and handling redirects. We also used the `BeautifulSoup` Python library, which is a typical package in parsing HTML and XML documents. The package parses the pased HTML source code into a parsed tree, which can be easily traversed. Finally, we used the `re` Python package to utilize `Regular Expression` for easier string matching.\n\n<br>\nNext, the players data are then inserted to the `players` table using the `insert_players` method:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-summary=\"Function description for insert_players()\"}\ndef insert_players(conn):\n    '''\n    Inserts players into the players table\n\n    Parameter\n    ---------\n    conn : sqlite connection\n    '''\n    cur = conn.cursor()\n    for char in string.ascii_uppercase:\n        url = \"http://www.chessgames.com/directory/\" + char + \".html\"\n        print(url)\n        players = get_player_info(url)\n        cur.executemany('''INSERT INTO players \n                            VALUES (?, ?, ?, ?, ?, ?, ?);''', players)\n        conn.commit()\n```\n:::\n\n\nHere, we access the enter the created database, then Insert the players and associated metadata into the database.\n\n<br>\nNext, this study will focus on the games of the thirty (30) highest-rated players. The ranking was based on the rating provided by the website. To extract the games of these players, we first obtained their player IDs (`pid`) by using the `pandas` method `read_sql`. Here we can pass a `SQL` statement:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-summary=\"Method to query the database\"}\ndf_players = pd.read_sql(\"\"\"SELECT pid, fname || ' ' || lname, rating, \n                                    game_count \n                            FROM players\n                            WHERE rating != ''\n                            ORDER BY rating DESC\"\"\", conn)\ndf_players.columns = ['Player ID', 'Name', 'Rating', 'Number of Games']\ndf_players.head(30)\n```\n:::\n\n\n<center> Table 2: Top 30 chess players based on Rating </center>\n|    Player ID |                     Name | Rating | Number of Games |\n|-------------:|----------------|-------:|----------------:|\n|        52948 |           Magnus CARLSEN |   2882 |           3,016 |\n|        15940 |           Garry KASPAROV |   2851 |           2,385 |\n|        76172 |         Fabiano CARUANA |   2844 |           1,891 |\n|        17316 |           Levon ARONIAN |   2830 |           2,708 |\n|        95915 |                 Wesley SO |   2822 |           1,400 |\n|        56798 | Maxime VACHIER-LAGRAVE |   2819 |           2,369 |\n|        12088 |         Viswanathan ANAND |   2817 |           3,542 |\n|        12295 |        Vladimir KRAMNIK |   2817 |           3,026 |\n|        12089 |           Veselin TOPALOV |   2816 |           2,278 |\n|        50065 |   Shakhriyar MAMEDYAROV |   2814 |           2,254 |\n|        10084 |          Hikaru NAKAMURA |   2814 |           2,424 |\n|        17279 |      Alexander GRISCHUK |   2797 |           2,586 |\n|        52629 |               Ding LIREN |   2797 |             920 |\n|       107252 |              Anish GIRI |   2793 |           1,522 |\n|        49796 |         Teimour RADJABOV |   2793 |           1,746 |\n|        54535 |         Sergey KARJAKIN |   2788 |           2,399 |\n|        11719 |  Alexander MOROZEVICH |   2788 |           1,847 |\n|        12183 |         Vassily IVANCHUK |   2787 |           3,752 |\n|        19233 |      Robert James FISCHER |   2785 |           1,052 |\n|        20719 |           Anatoly KARPOV |   2780 |           3,609 |\n|        13847 |            Boris GELFAND |   2777 |           3,014 |\n|        79968 |            Peter SVIDLER |   2769 |           2,786 |\n|        49080 |  Leinier Dominguez PEREZ |   2768 |           1,342 |\n|        12109 |         Ruslan PONOMARIOV |   2768 |           1,989 |\n|        54683 |      Ian NEPOMNIACHTCHI |   2767 |           1,614 |\n|        49246 |     Pentala HARIKRISHNA |   2766 |           1,442 |\n|        49456 |            Pavel ELJANOV |   2765 |           1,409 |\n|        15874 |               Gata KAMSKY |   2763 |           1,889 |\n|        12290 |                Peter LEKO |   2763 |           2,364 |\n|       112240 |                Yu YANGYI |   2762 |             991 |\n\nThe `games` table was then populated by web scraping <small>`http://www.chessgames.com/perl/chessplayer?pid=<pid>`</small> and iterating over the top 30 player IDs. The following fields were extracted for each game:\n\n<center> Table 3: `games` table fields </center>\n\n| Field         | Description        | Data Type |\n|---------------|--------------------|-----------|\n| gid           | Game ID            | integer   |\n| white_pid     | White Player ID    | int       |\n| black_pid     | Black Player ID    | int       |\n| result        | Result             | varchar   |\n| moves         | Number of moves    | integer   |\n| year          | Year               | integer   |\n| tournament    | Tournament Name    | varchar   |\n| eco           | Encyclopaedia of Chess Openings  | varchar   |\n| opening_move  | Opening move       | varchar   |\n\nThe `get_players_games` function was implemented to scrape the game data for a given Player ID `pid` and page number `page_start`. This writes the games data of the player to a CSV file (`<pid>.csv`):\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-summary=\"Function description for the get_players_games()\"}\ndef get_player_games(pid, page_start):\n    \"\"\"\n    Web scrapes the games list for a player and writes it to a CSV \n\n    Parameters:\n    -----------\n    pid : player ID\n    page_start : starting page\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    url = 'http://www.chessgames.com/perl/chessplayer?pid=' + str(pid)\n    resp = requests.get(url, headers=headers)\n    print('pid = ', pid, '\\turl = ', url, '\\tcode = ', resp.status_code)\n    time.sleep(np.random.randint(1, 2))\n    soup = BeautifulSoup(resp.text, 'lxml')\n    div_page_count = soup.select(\n        'td[background$=\"/chessimages/table_stripes.gif\"]')\n    page_count = int(re.findall('of (\\d+)\\;', div_page_count[0].text)[0])\n\n    with open(f'{pid}.csv', 'a') as file:\n        csv_writer = csv.writer(file, delimiter=',', quotechar='\"')\n        for page in range(page_start, page_count+1):\n            page_url = 'http://www.chessgames.com/perl/chess.pl?page=' + \\\n                str(page) + '&pid=' + str(pid)\n            page_resp = requests.get(page_url, headers=headers)\n            print('\\tpage = ', page, '\\turl = ', page_url,\n                  '\\tcode = ', page_resp.status_code)\n            time.sleep(np.random.randint(1, 2))\n            page_soup = BeautifulSoup(page_resp.text, 'lxml')\n            games = page_soup.select(\n                'tr[bgcolor=\"#FFFFFF\"],tr[bgcolor=\"#EEDDCC\"]')\n\n            for game in games:\n                data = game.select('td')\n                game_url = data[0].find(\"a\")['href']\n                game_id = re.findall('(\\d+)', game_url)[0]\n                result = data[2].text.strip()\n                moves = data[3].text.strip()\n                year = data[4].text.strip()\n                tournament = data[5].text.strip()\n                eco = data[6].select('a')[0].text.strip()\n                opening_move = re.findall(\n                    '^[A-E0-9][0-9]{2} (.*)', data[6].text.strip())[0]\n\n                game_resp = requests.get(\n                    'http://www.chessgames.com' + game_url, headers=headers)\n                time.sleep(np.random.randint(1, 2))\n                game_soup = BeautifulSoup(game_resp.text, 'lxml')\n                players = game_soup.select('center')[0].select('a')\n                try:\n                    white_id = re.findall('(\\d+)', players[0]['href'])[0]\n                except:\n                    white_id = None\n                try:\n                    black_id = re.findall('(\\d+)', players[1]['href'])[0]\n                except:\n                    black_id = None\n                tup = (game_id, white_id, black_id, result,\n                       moves, year, tournament, eco, opening_move)\n\n                try:\n                    csv_writer.writerow(tup)\n                except:\n                    print('\\t\\tgameID: ', game_id, '\\tWrite to CSV failed')\n```\n:::\n\n\nAll player csv files were then inserted to the `games` table of the `chessgames.db` database using the `read_csvs` function:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-summary=\"Function description for read_csvs()\"}\ndef read_csvs(conn):\n    \"\"\"\n    Read all player csvs and save them to the games table\n    \"\"\"\n    gid_failed = []\n    cur = conn.cursor()\n    for file_name in glob.glob('./games/*.csv'):\n        with open(file_name) as file:\n            print(file_name)\n            reader = csv.reader(file, delimiter = ',')\n            for line in reader:\n                try:\n                    cur.execute('''INSERT INTO games VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);''', line)\n                    print('\\t\\tgameID: ', line[0], '\\tInsert success!')\n                except Exception as e:\n                    gid_failed.append(line[0])\n                    print('\\t\\tgameID: ', line[0], '\\tInsert Failed!\\t', e)\n    return gid_failed\n```\n:::\n\n\nNow, let us examine the games dataframe by calling it with the `read_sql` Pandas module.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-summary=\"Accessing the database.\"}\ndf_games = pd.read_sql(\"\"\"SELECT * FROM games\"\"\", conn)\ndf_games.columns = (['Game ID', 'White Player ID', 'Black Player ID',\n                     'Result', 'Number of Moves', 'Year', 'Tournament', 'ECO',\n                     'Opening Move'])\n```\n:::\n\n\nBelow you can see that **Table 4** and **Table 5** shows the summary statistics for the `players` and `games` table. **8,574 players** and **50,087** games were inserted into the chessgames database.\n\n\n\n|Statistic | Rating |\n|---------|-----------|\n|count\t|8574.000000|\n|mean\t|2361.279566|\n|std\t|184.344627|\n|min\t|1379.000000|\n|25%\t|2268.000000|\n|50%\t|2389.000000|\n|75%\t|2479.000000|\n|max\t|2882.000000|\n\n: First Table {#tbl-first}\n\n<h2>Conclusion</h2>\n\n---\n<i class='appendix'>Images: https://primer.com.ph/blog/2016/02/04/philippine-elections-the-culture-the-drama-the-battle-2/</i>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}